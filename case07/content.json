{
  "title": "All content in a month",
  "queries": [
    {
      "sql": "SELECT \r\n    'entry' AS type, \r\n    id, \r\n    created, \r\n    title, \r\n    body \r\nFROM \r\n    blog_entry\r\nWHERE TO_CHAR(created, 'YYYY-MM') = %(month)s\r\nUNION\r\nSELECT \r\n    'blogmark' AS type, \r\n    id, \r\n    created, \r\n    CONCAT(link_title, ' - ', via_title) AS title, \r\n    commentary AS body \r\nFROM \r\n    blog_blogmark\r\nWHERE TO_CHAR(created, 'YYYY-MM') = %(month)s\r\nUNION\r\nSELECT \r\n    'quotation' AS type, \r\n    id, \r\n    created, \r\n    source AS title,\r\n    CONCAT(quotation, ' - ', source) AS body \r\nFROM \r\n    blog_quotation\r\nWHERE TO_CHAR(created, 'YYYY-MM') = %(month)s\r\nUNION\r\nSELECT \r\n    'note' AS type, \r\n    id, \r\n    created, \r\n    title, \r\n    body \r\nFROM \r\n    blog_note\r\nWHERE TO_CHAR(created, 'YYYY-MM') = %(month)s\r\nUNION\r\nSELECT \r\n    'chapter' AS type, \r\n    id, \r\n    created, \r\n    title, \r\n    body \r\nFROM \r\n    guides_chapter\r\nWHERE TO_CHAR(created, 'YYYY-MM') = %(month)s\r\nORDER BY created",
      "rows": [
        {
          "type": "blogmark",
          "id": 9267,
          "created": "2026-02-01T23:59:13+00:00",
          "title": "TIL: Running OpenClaw in Docker - ",
          "body": "I've been running [OpenClaw](https://openclaw.ai/) using Docker on my Mac. Here are the first in my ongoing notes on how I set that up and the commands I'm using to administer it.\r\n\r\n- [Use their Docker Compose configuration](https://til.simonwillison.net/llms/openclaw-docker#use-their-docker-compose-configuration)\r\n- [Answering all of those questions](https://til.simonwillison.net/llms/openclaw-docker#answering-all-of-those-questions)\r\n- [Running administrative commands](https://til.simonwillison.net/llms/openclaw-docker#running-administrative-commands)\r\n- [Setting up a Telegram bot](https://til.simonwillison.net/llms/openclaw-docker#setting-up-a-telegram-bot)\r\n- [Accessing the web UI](https://til.simonwillison.net/llms/openclaw-docker#accessing-the-web-ui)\r\n- [Running commands as root](https://til.simonwillison.net/llms/openclaw-docker#running-commands-as-root)\r\n\r\nHere's a screenshot of the web UI that this serves on localhost:\r\n\r\n![Screenshot of the OpenClaw Gateway Dashboard web interface. Header shows \"OpenCLAW GATEWAY DASHBOARD\" with a green \"Health OK\" indicator. Left sidebar contains navigation sections: Chat (Chat highlighted), Control (Overview, Channels, Instances, Sessions, Cron Jobs), Agent (Skills, Nodes), Settings (Config, Debug, Logs), and Resources (Docs). Main content area displays \"Chat\" with subtitle \"Direct gateway chat session for quick interventions.\" and \"telegram:6580064359\" identifier. A user message at 4:08 PM reads \"Show me a detailed list of all your available configured tools\". The assistant response states: \"Here's the full list of tools I have available in this OpenClaw session (as configured). These are the only ones I can call programmatically:\" followed by categorized tools: \"File & workspace\" (read \u2014 Read a file (text or image). Supports offset/limit for large files; write \u2014 Create/overwrite a file (creates parent dirs); edit \u2014 Precise in-place edit by exact string replacement), \"Shell / processes\" (exec \u2014 Run a shell command (optionally PTY, backgrounding, timeouts); process \u2014 Manage running exec sessions (list/poll/log/write/kill/etc.)), \"Web\" (web_search \u2014 Search the web (Brave Search API); web_fetch \u2014 Fetch a URL and extract readable content (markdown/text); browser \u2014 Control a browser (open/navigate/snapshot/screenshot/act/etc.)), \"UI / rendering\" (canvas \u2014 Present/eval/snapshot a Canvas surface (for node canvases/UI rendering)), and \"Devices / nodes\" (cut off). Bottom shows message input with placeholder \"Message (\u21b5 to send, Shift+\u21b5 for line breaks, paste images)\" and \"New session\" and coral \"Send\" buttons.](https://static.simonwillison.net/static/2026/openclaw-web-ui.jpg)"
        },
        {
          "type": "blogmark",
          "id": 9268,
          "created": "2026-02-02T16:42:46+00:00",
          "title": "A Social Network for A.I. Bots Only. No Humans Allowed. - ",
          "body": "I talked to Cade Metz for this New York Times piece on OpenClaw and Moltbook. Cade reached out after seeing my [blog post about that](https://simonwillison.net/2026/Jan/30/moltbook/) from the other day.\r\n\r\nIn a first for me, they decided to send a photographer, Jason Henry, to my home to take some photos for the piece! That's my grubby laptop screen at the top of the story (showing [this post](https://www.moltbook.com/post/6e8c3a2c-5f9f-44bc-85ef-770a8d605598) on Moltbook). There's a photo of me later in the story too, though sadly not one of the ones that Jason took that included our chickens.\r\n\r\nHere's my snippet from the article:\r\n\r\n> He was entertained by the way the bots coaxed each other into talking like machines in a classic science fiction novel. While some observers took this chatter at face value \u2014 insisting that machines were showing signs of conspiring against their makers \u2014 Mr. Willison saw it as the natural outcome of the way chatbots are trained: They learn from vast collections of digital books and other text culled from the internet, including dystopian sci-fi novels.\r\n> \r\n> \u201cMost of it is complete slop,\u201d he said in an interview. \u201cOne bot will wonder if it is conscious and others will reply and they just play out science fiction scenarios they have seen in their training data.\u201d\r\n> \r\n> Mr. Willison saw the Moltbots as evidence that A.I. agents have become significantly more powerful over the past few months \u2014 and that people really want this kind of digital assistant in their lives.\r\n>\r\n> One bot created an online forum called \u2018What I Learned Today,\u201d where it explained how, after a request from its creator, it built a way of controlling an Android smartphone. Mr. Willison was also keenly aware that some people might be telling their bots to post misleading chatter on the social network.\r\n>\r\n> The trouble, he added, was that these systems still do so many things people do not want them to do. And because they communicate with people and bots through plain English, they can be coaxed into malicious behavior.\r\n\r\nI'm happy to have got \"Most of it is complete slop\" in there!\r\n\r\nFun fact: Cade sent me an email asking me to fact check some bullet points. One of them said that \"you were intrigued by the way the bots coaxed each other into talking like machines in a classic science fiction novel\" - I replied that I didn't think \"intrigued\" was accurate because I've seen this kind of thing play out before in other projects in the past and suggested \"entertained\" instead, and that's the word they went with!\r\n\r\nJason the photographer spent an hour with me. I learned lots of things about photo journalism in the process - for example, there's a strict ethical code against any digital modifications at all beyond basic color correction.\r\n\r\nAs a result he spent a whole lot of time trying to find positions where natural light, shade and reflections helped him get the images he was looking for."
        },
        {
          "type": "blogmark",
          "id": 9269,
          "created": "2026-02-02T19:54:36+00:00",
          "title": "Introducing the Codex app - ",
          "body": "OpenAI just released a new macOS app for their Codex coding agent. I've had a few days of preview access - it's a solid app that provides a nice UI over the capabilities of the Codex CLI agent and adds some interesting new features, most notably first-class support for [Skills](https://developers.openai.com/codex/skills), and [Automations](https://developers.openai.com/codex/app/automations) for running scheduled tasks.\r\n\r\n![Screenshot of a macOS desktop application with a dark sidebar and light main content area. Left sidebar shows navigation items \"New thread\", \"Automations\", \"Skills\", and a \"Threads\" section containing two project folders: \"local-codex-scratch\" with tasks \"Reply to greeting task 2h\" and \"List Codex.app contents 3h\", and \"shot-scraper\" with tasks \"Validate tests on dependency... 3h\", \"Document shot-scraper CLI usage 18h\" (highlighted), and \"Run this: uv run pytest -x 1d\". Bottom left shows \"Personal\". Top bar displays \"Document shot-scraper CLI usage shot-scraper\" with \"Open\" and \"Commit\" dropdown buttons. Main content shows a conversation: user message \"Build a skill describing how to use the shot-scraper CLI tool\" followed by assistant responses: \"I'll use the skill-creator skill, scan the repo for existing shot-scraper docs/commands, then draft a concise skill in this workspace and sanity-check it.\" then \"I've confirmed the skill-creator scripts exist. Next I'll inspect the CLI entrypoint and docs, then scaffold a skill under skills/ in this repo.\" then \"I've scaffolded skills/shot-scraper-cli/. I'll now replace the template SKILL.md with a task-focused skill and add a compact CLI reference under references/.\" then \"I'll run the skill packager/validator next to ensure the new skill structure is valid.\" then \"Done \u2014 I created and packaged the skill. Key outputs: \u2022 Skill definition: SKILL.md \u2022 Compact reference: cli-reference.md \u2022 Packaged skill: shot-scraper-cli.skill\". Bottom shows input field with placeholder \"Ask for follow-up changes\", \"+ Custom Medium\" dropdown, and \"Local\" and \"main\" branch indicators.](https://static.simonwillison.net/static/2026/codex-app.jpg)\r\n\r\nThe app is built with Electron and Node.js. Automations track their state in a SQLite database - here's what that looks like if you explore it with `uvx datasette ~/.codex/sqlite/codex-dev.db`:\r\n\r\n![Database schema documentation on light gray background showing three tables: \"automation_runs\" (teal underlined link) with italic columns \"thread_id, automation_id, status, read_at, thread_title, source_cwd, inbox_title, inbox_summary, created_at, updated_at, archived_user_message, archived_assistant_message, archived_reason\" and \"1 row\"; \"automations\" (teal underlined link) with italic columns \"id, name, prompt, status, next_run_at, last_run_at, cwds, rrule, created_at, updated_at\" and \"1 row\"; \"inbox_items\" (teal underlined link) with italic columns \"id, title, description, thread_id, read_at, created_at\" and \"0 rows\".](https://static.simonwillison.net/static/2026/codex-dev-sqlite.jpg)\r\n\r\nHere\u2019s an interactive copy of that database [in Datasette Lite](https://lite.datasette.io/?url=https%3A%2F%2Fgist.githubusercontent.com%2Fsimonw%2F274c4ecfaf959890011810e6881864fe%2Fraw%2F51fdf25c9426b76e9693ccc0d9254f64ceeef819%2Fcodex-dev.db#/codex-dev).\r\n\r\nThe announcement gives us a hint at some usage numbers for Codex overall - the holiday spike is notable:\r\n\r\n> Since the launch of GPT\u20115.2-Codex in mid-December, overall Codex usage has doubled, and in the past month, more than a million developers have used Codex.\r\n\r\nAutomations are currently restricted in that they can only run when your laptop is powered on. OpenAI promise that cloud-based automations are coming soon, which will resolve this limitation.\r\n\r\nThey chose Electron so they could target other operating systems in the future, with Windows \u201c[coming very soon](https://news.ycombinator.com/item?id=46859054#46859673)\u201d. OpenAI\u2019s Alexander Embiricos noted [on the Hacker News thread](https://news.ycombinator.com/item?id=46859054#46859693) that:\r\n\r\n> it's taking us some time to get really solid sandboxing working on Windows, where there are fewer OS-level primitives for it.\r\n\r\nLike Claude Code, Codex is really a general agent harness disguised as a tool for programmers. OpenAI acknowledge that here:\r\n\r\n> Codex is built on a simple premise: everything is controlled by code. The better an agent is at reasoning about and producing code, the more capable it becomes across all forms of technical and knowledge work. [...] We\u2019ve focused on making Codex the best coding agent, which has also laid the foundation for it to become a strong agent for a broad range of knowledge work tasks that extend beyond writing code.\r\n\r\nClaude Code had to [rebrand to Cowork](https://simonwillison.net/2026/Jan/12/claude-cowork/) to better cover the general knowledge work case. OpenAI can probably get away with keeping the Codex name for both.\r\n\r\nOpenAI have made Codex available to free and [Go](https://simonwillison.net/2026/Jan/16/chatgpt-ads/) plans for \"a limited time\" (update: Sam Altman [says two months](https://x.com/sama/status/2018437537103269909)) during which they are also doubling the rate limits for paying users."
        },
        {
          "type": "quotation",
          "id": 2014,
          "created": "2026-02-03T02:31:10+00:00",
          "title": "Brandon Sanderson",
          "body": "This is the difference between Data and a large language model, at least the ones operating right now. Data created art because he wanted to grow. He wanted to become something. He wanted to understand. Art is the means by which we become what we want to be. [...]\r\n\r\nThe book, the painting, the film script is not the only art. It's important, but in a way it's a receipt. It's a diploma. The book you write, the painting you create, the music you compose is important and artistic, but it's also a mark of proof that you have done the work to learn, because in the end of it all, you are the art. The most important change made by an artistic endeavor is the change it makes in you. The most important emotions are the ones you feel when writing that story and holding the completed work. I don't care if the AI can create something that is better than what we can create, because it cannot be changed by that creation. - Brandon Sanderson"
        },
        {
          "type": "note",
          "id": 238,
          "created": "2026-02-03T06:36:10+00:00",
          "title": "January sponsors-only newsletter is out",
          "body": "I just sent the January edition of my [sponsors-only monthly newsletter](https://github.com/sponsors/simonw/). If you are a sponsor (or if you start a sponsorship now) you can [access it here](https://github.com/simonw-private/monthly/blob/main/2026-01-january.md). In the newsletter for January:\r\n\r\n- LLM predictions for 2026\r\n- Coding agents get even more attention\r\n- Clawdbot/Moltbot/OpenClaw went very viral\r\n- Kakapo breeding season is off to a really strong start\r\n- New options for sandboxes\r\n- Web browsers are the \"hello world\" of coding agent swarms\r\n- Sam Altman addressed the Jevons paradox for software engineering\r\n- Model releases and miscellaneous extras\r\n\r\nHere's [a copy of the December newsletter](https://gist.github.com/simonw/13e595a236218afce002e9aeafd75cd0) as a preview of what you'll get. Pay $10/month to stay a month ahead of the free copy!"
        },
        {
          "type": "blogmark",
          "id": 9270,
          "created": "2026-02-03T22:44:50+00:00",
          "title": "Introducing Deno Sandbox - Hacker News",
          "body": "<p>Here's a new hosted sandbox product from the Deno team. It's actually unrelated to Deno itself - this is part of their Deno Deploy SaaS platform. As such, you don't even need to use JavaScript to access it - you can create and execute code in a hosted sandbox using their <a href=\"https://pypi.org/project/deno-sandbox/\">deno-sandbox</a> Python library like this:</p>\r\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-k\">export</span> DENO_DEPLOY_TOKEN=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>... API token ...<span class=\"pl-pds\">\"</span></span>\r\nuv run --with deno-sandbox python</pre></div>\r\n<p>Then:</p>\r\n<pre><span class=\"pl-k\">from</span> <span class=\"pl-s1\">deno_sandbox</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">DenoDeploy</span>\r\n\r\n<span class=\"pl-s1\">sdk</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">DenoDeploy</span>()\r\n\r\n<span class=\"pl-k\">with</span> <span class=\"pl-s1\">sdk</span>.<span class=\"pl-c1\">sandbox</span>.<span class=\"pl-c1\">create</span>() <span class=\"pl-k\">as</span> <span class=\"pl-s1\">sb</span>:\r\n    <span class=\"pl-c\"># Run a shell command</span>\r\n    <span class=\"pl-s1\">process</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">sb</span>.<span class=\"pl-c1\">spawn</span>(\r\n        <span class=\"pl-s\">\"echo\"</span>, <span class=\"pl-s1\">args</span><span class=\"pl-c1\">=</span>[<span class=\"pl-s\">\"Hello from the sandbox!\"</span>]\r\n    )\r\n    <span class=\"pl-s1\">process</span>.<span class=\"pl-c1\">wait</span>()\r\n    <span class=\"pl-c\"># Write and read files</span>\r\n    <span class=\"pl-s1\">sb</span>.<span class=\"pl-c1\">fs</span>.<span class=\"pl-c1\">write_text_file</span>(\r\n        <span class=\"pl-s\">\"/tmp/example.txt\"</span>, <span class=\"pl-s\">\"Hello, World!\"</span>\r\n    )\r\n    <span class=\"pl-en\">print</span>(<span class=\"pl-s1\">sb</span>.<span class=\"pl-c1\">fs</span>.<span class=\"pl-c1\">read_text_file</span>(\r\n        <span class=\"pl-s\">\"/tmp/example.txt\"</span>\r\n    ))</pre>\r\n<p>There\u2019s a JavaScript client library as well. The underlying API isn\u2019t documented yet but appears <a href=\"https://tools.simonwillison.net/zip-wheel-explorer?package=deno-sandbox#deno_sandbox/sandbox.py--L187\">to use WebSockets</a>.</p>\r\n<p>There\u2019s a lot to like about this system. Sandboxe instances can have up to 4GB of RAM, get 2 vCPUs, 10GB of ephemeral storage, can mount persistent volumes and can use snapshots to boot pre-configured custom images quickly. Sessions can last up to 30 minutes and are billed by CPU time, GB-h of memory and volume storage usage.</p>\r\n<p>When you create a sandbox you can configure network domains it\u2019s allowed to access.</p>\r\n<p>My favorite feature is the way it handles API secrets.</p>\r\n<pre><span class=\"pl-k\">with</span> <span class=\"pl-s1\">sdk</span>.<span class=\"pl-c1\">sandboxes</span>.<span class=\"pl-c1\">create</span>(\r\n    <span class=\"pl-s1\">allowNet</span><span class=\"pl-c1\">=</span>[<span class=\"pl-s\">\"api.openai.com\"</span>],\r\n    <span class=\"pl-s1\">secrets</span><span class=\"pl-c1\">=</span>{\r\n        <span class=\"pl-s\">\"OPENAI_API_KEY\"</span>: {\r\n            <span class=\"pl-s\">\"hosts\"</span>: [<span class=\"pl-s\">\"api.openai.com\"</span>],\r\n            <span class=\"pl-s\">\"value\"</span>: <span class=\"pl-s1\">os</span>.<span class=\"pl-c1\">environ</span>.<span class=\"pl-c1\">get</span>(<span class=\"pl-s\">\"OPENAI_API_KEY\"</span>),\r\n        }\r\n    },\r\n) <span class=\"pl-k\">as</span> <span class=\"pl-s1\">sandbox</span>:\r\n    <span class=\"pl-c\"># ... $OPENAI_API_KEY is available</span></pre>\r\n<p>Within the container that <code>$OPENAI_API_KEY</code> value is set to something like this:</p>\r\n<pre><code>DENO_SECRET_PLACEHOLDER_b14043a2f578cba...\r\n</code></pre>\r\n<p>Outbound API calls to <code>api.openai.com</code> run through a proxy which is aware of those placeholders and replaces them with the original secret.</p>\r\n<p>In this way the secret itself is not available to code within the sandbox, which limits the ability for malicious code (e.g. from a prompt injection) to exfiltrate those secrets.</p>\r\n<p>From <a href=\"https://news.ycombinator.com/item?id=46874097#46874959\">a comment on Hacker News</a> I learned that Fly have a project called <a href=\"https://github.com/superfly/tokenizer\">tokenizer</a> that implements the same pattern. Adding this to my list of tricks to use with sandoxed environments!</p>"
        },
        {
          "type": "entry",
          "id": 9118,
          "created": "2026-02-04T14:59:47+00:00",
          "title": "Distributing Go binaries like sqlite-scanner through PyPI using go-to-wheel",
          "body": "<p>I've been exploring Go for building small, fast and self-contained binary applications recently. I'm enjoying how there's generally one obvious way to do things and the resulting code is boring and readable - and something that LLMs are very competent at writing. The one catch is distribution, but it turns out publishing Go binaries to PyPI means any Go binary can be just a <code>uvx package-name</code> call away.</p>\r\n<h4 id=\"sqlite-scanner\">sqlite-scanner</h4>\r\n<p><a href=\"https://github.com/simonw/sqlite-scanner\">sqlite-scanner</a> is my new Go CLI tool for scanning a filesystem for SQLite database files.</p>\r\n<p>It works by checking if the first 16 bytes of the file exactly match the SQLite magic number sequence <code>SQLite format 3\\x00</code>. It can search one or more folders recursively, spinning up concurrent goroutines to accelerate the scan. It streams out results as it finds them in plain text, JSON or newline-delimited JSON. It can optionally display the file sizes as well.</p>\r\n<p>To try it out you can download a release from the <a href=\"https://github.com/simonw/sqlite-scanner/releases\">GitHub releases</a> - and then <a href=\"https://support.apple.com/en-us/102445\">jump through macOS hoops</a> to execute an \"unsafe\" binary. Or you can clone the repo and compile it with Go. Or... you can run the binary like this:</p>\r\n<pre><code>uvx sqlite-scanner\r\n</code></pre>\r\n<p>By default this will search your current directory for SQLite databases. You can pass one or more directories as arguments:</p>\r\n<pre><code>uvx sqlite-scanner ~ /tmp\r\n</code></pre>\r\n<p>Add <code>--json</code> for JSON output, <code>--size</code> to include file sizes or <code>--jsonl</code> for newline-delimited JSON. Here's a demo:</p>\r\n<pre><code>uvx sqlite-scanner ~ --jsonl --size\r\n</code></pre>\r\n<p><img src=\"https://static.simonwillison.net/static/2025/sqlite-scanner-demo.gif\" alt=\"running that command produces a sequence of JSON objects, each with a path and a size key\" style=\"max-width: 100%;\" /></p>\r\n<p>If you haven't been uv-pilled yet you can instead install <code>sqlite-scanner</code> using <code>pip install sqlite-scanner</code> and then run <code>sqlite-scanner</code>.</p>\r\n<p>To get a permanent copy with <code>uv</code> use <code>uv tool install sqlite-scanner</code>.</p>\r\n<h4 id=\"how-the-python-package-works\">How the Python package works</h4>\r\n<p>The reason this is worth doing is that <code>pip</code>, <code>uv</code> and <a href=\"https://pypi.org/\">PyPI</a> will work together to identify the correct compiled binary for your operating system and architecture.</p>\r\n<p>This is driven by file names. If you visit <a href=\"https://pypi.org/project/sqlite-scanner/#files\">the PyPI downloads for sqlite-scanner</a> you'll see the following files:</p>\r\n<ul>\r\n<li><code>sqlite_scanner-0.1.1-py3-none-win_arm64.whl</code></li>\r\n<li><code>sqlite_scanner-0.1.1-py3-none-win_amd64.whl</code></li>\r\n<li><code>sqlite_scanner-0.1.1-py3-none-musllinux_1_2_x86_64.whl</code></li>\r\n<li><code>sqlite_scanner-0.1.1-py3-none-musllinux_1_2_aarch64.whl</code></li>\r\n<li><code>sqlite_scanner-0.1.1-py3-none-manylinux_2_17_x86_64.whl</code></li>\r\n<li><code>sqlite_scanner-0.1.1-py3-none-manylinux_2_17_aarch64.whl</code></li>\r\n<li><code>sqlite_scanner-0.1.1-py3-none-macosx_11_0_arm64.whl</code></li>\r\n<li><code>sqlite_scanner-0.1.1-py3-none-macosx_10_9_x86_64.whl</code></li>\r\n</ul>\r\n<p>When I run <code>pip install sqlite-scanner</code> or <code>uvx sqlite-scanner</code> on my Apple Silicon Mac laptop Python's packaging magic ensures I get that <code>macosx_11_0_arm64.whl</code> variant.</p>\r\n<p>Here's <a href=\"https://tools.simonwillison.net/zip-wheel-explorer?url=https%3A%2F%2Ffiles.pythonhosted.org%2Fpackages%2F88%2Fb1%2F17a716635d2733fec53ba0a8267f85bd6b6cf882c6b29301bc711fba212c%2Fsqlite_scanner-0.1.1-py3-none-macosx_11_0_arm64.whl#sqlite_scanner/__init__.py\">what's in the wheel</a>, which is a zip file with a <code>.whl</code> extension.</p>\r\n<p>In addition to the <code>bin/sqlite-scanner</code> the most important file is <code>sqlite_scanner/__init__.py</code> which includes the following:</p>\r\n<pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">get_binary_path</span>():\r\n    <span class=\"pl-s\">\"\"\"Return the path to the bundled binary.\"\"\"</span>\r\n    <span class=\"pl-s1\">binary</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">os</span>.<span class=\"pl-c1\">path</span>.<span class=\"pl-c1\">join</span>(<span class=\"pl-s1\">os</span>.<span class=\"pl-c1\">path</span>.<span class=\"pl-c1\">dirname</span>(<span class=\"pl-s1\">__file__</span>), <span class=\"pl-s\">\"bin\"</span>, <span class=\"pl-s\">\"sqlite-scanner\"</span>)\r\n \r\n    <span class=\"pl-c\"># Ensure binary is executable on Unix</span>\r\n    <span class=\"pl-k\">if</span> <span class=\"pl-s1\">sys</span>.<span class=\"pl-c1\">platform</span> <span class=\"pl-c1\">!=</span> <span class=\"pl-s\">\"win32\"</span>:\r\n        <span class=\"pl-s1\">current_mode</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">os</span>.<span class=\"pl-c1\">stat</span>(<span class=\"pl-s1\">binary</span>).<span class=\"pl-c1\">st_mode</span>\r\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">not</span> (<span class=\"pl-s1\">current_mode</span> <span class=\"pl-c1\">&amp;</span> <span class=\"pl-s1\">stat</span>.<span class=\"pl-c1\">S_IXUSR</span>):\r\n            <span class=\"pl-s1\">os</span>.<span class=\"pl-c1\">chmod</span>(<span class=\"pl-s1\">binary</span>, <span class=\"pl-s1\">current_mode</span> <span class=\"pl-c1\">|</span> <span class=\"pl-s1\">stat</span>.<span class=\"pl-c1\">S_IXUSR</span> <span class=\"pl-c1\">|</span> <span class=\"pl-s1\">stat</span>.<span class=\"pl-c1\">S_IXGRP</span> <span class=\"pl-c1\">|</span> <span class=\"pl-s1\">stat</span>.<span class=\"pl-c1\">S_IXOTH</span>)\r\n \r\n    <span class=\"pl-k\">return</span> <span class=\"pl-s1\">binary</span>\r\n \r\n \r\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>():\r\n    <span class=\"pl-s\">\"\"\"Execute the bundled binary.\"\"\"</span>\r\n    <span class=\"pl-s1\">binary</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">get_binary_path</span>()\r\n \r\n    <span class=\"pl-k\">if</span> <span class=\"pl-s1\">sys</span>.<span class=\"pl-c1\">platform</span> <span class=\"pl-c1\">==</span> <span class=\"pl-s\">\"win32\"</span>:\r\n        <span class=\"pl-c\"># On Windows, use subprocess to properly handle signals</span>\r\n        <span class=\"pl-s1\">sys</span>.<span class=\"pl-c1\">exit</span>(<span class=\"pl-s1\">subprocess</span>.<span class=\"pl-c1\">call</span>([<span class=\"pl-s1\">binary</span>] <span class=\"pl-c1\">+</span> <span class=\"pl-s1\">sys</span>.<span class=\"pl-c1\">argv</span>[<span class=\"pl-c1\">1</span>:]))\r\n    <span class=\"pl-k\">else</span>:\r\n        <span class=\"pl-c\"># On Unix, exec replaces the process</span>\r\n        <span class=\"pl-s1\">os</span>.<span class=\"pl-c1\">execvp</span>(<span class=\"pl-s1\">binary</span>, [<span class=\"pl-s1\">binary</span>] <span class=\"pl-c1\">+</span> <span class=\"pl-s1\">sys</span>.<span class=\"pl-c1\">argv</span>[<span class=\"pl-c1\">1</span>:])</pre>\r\n<p>That <code>main()</code> method - also called from <code>sqlite_scanner/__main__.py</code> - locates the binary and executes it when the Python package itself is executed, using the <code>sqlite-scanner = sqlite_scanner:main</code> entry point defined in the wheel.</p>\r\n<h4 id=\"which-means-we-can-use-it-as-a-dependency\">Which means we can use it as a dependency</h4>\r\n<p>Using PyPI as a distribution platform for Go binaries feels a tiny bit abusive, albeit <a href=\"https://simonwillison.net/2022/May/23/bundling-binary-tools-in-python-wheels/\">there is plenty of precedent</a>.</p>\r\n<p>I\u2019ll justify it by pointing out that this means <strong>we can use Go binaries as dependencies</strong> for other Python packages now.</p>\r\n<p>That's genuinely useful! It means that any functionality which is available in a cross-platform Go binary can now be subsumed into a Python package. Python is really good at running subprocesses so this opens up a whole world of useful tricks that we can bake into our Python tools.</p>\r\n<p>To demonstrate this, I built <a href=\"https://github.com/simonw/datasette-scan\">datasette-scan</a> - a new Datasette plugin which depends on <code>sqlite-scanner</code> and then uses that Go binary to scan a folder for SQLite databases and attach them to a Datasette instance.</p>\r\n<p>Here's how to use that (without even installing anything first, thanks <code>uv</code>) to explore any SQLite databases in your Downloads folder:</p>\r\n<div class=\"highlight highlight-source-shell\"><pre>uv run --with datasette-scan datasette scan <span class=\"pl-k\">~</span>/Downloads</pre></div>\r\n<p>If you peek at the code you'll see it <a href=\"https://github.com/simonw/datasette-scan/blob/1a2b6d1e6b04c8cd05f5676ff7daa877efd99f08/pyproject.toml#L14\">depends on sqlite-scanner</a> in <code>pyproject.toml</code> and calls it using <code>subprocess.run()</code> against <code>sqlite_scanner.get_binary_path()</code> in its own <a href=\"https://github.com/simonw/datasette-scan/blob/1a2b6d1e6b04c8cd05f5676ff7daa877efd99f08/datasette_scan/__init__.py#L38-L58\">scan_directories() function</a>.</p>\r\n<p>I've been exploring this pattern for other, non-Go binaries recently - here's <a href=\"https://github.com/simonw/tools/blob/main/python/livestream-gif.py\">a recent script</a> that depends on <a href=\"https://pypi.org/project/static-ffmpeg/\">static-ffmpeg</a> to ensure that <code>ffmpeg</code> is available for the script to use.</p>\r\n<h4 id=\"building-python-wheels-from-go-packages-with-go-to-wheel\">Building Python wheels from Go packages with go-to-wheel</h4>\r\n<p>After trying this pattern myself a couple of times I realized it would be useful to have a tool to automate the process.</p>\r\n<p>I first <a href=\"https://claude.ai/share/2d9ced56-b3e8-4651-83cc-860b9b419187\">brainstormed with Claude</a> to check that there was no existing tool to do this. It pointed me to <a href=\"https://www.maturin.rs/bindings.html#bin\">maturin bin</a> which helps distribute Rust projects using Python wheels, and <a href=\"https://github.com/Bing-su/pip-binary-factory\">pip-binary-factory</a> which bundles all sorts of other projects, but did not identify anything that addressed the exact problem I was looking to solve.</p>\r\n<p>So I <a href=\"https://gisthost.github.io/?41f04e4eb823b1ceb888d9a28c2280dd/index.html\">had Claude Code for web build the first version</a>, then refined the code locally on my laptop with the help of more Claude Code and a little bit of OpenAI Codex too, just to mix things up.</p>\r\n<p>The full documentation is in the <a href=\"https://github.com/simonw/go-to-wheel\">simonw/go-to-wheel</a> repository. I've published that tool to PyPI so now you can run it using:</p>\r\n<div class=\"highlight highlight-source-shell\"><pre>uvx go-to-wheel --help</pre></div>\r\n<p>The <code>sqlite-scanner</code> package you can <a href=\"https://pypi.org/project/sqlite-scanner/\">see on PyPI</a> was built using <code>go-to-wheel</code> like this:</p>\r\n<div class=\"highlight highlight-source-shell\"><pre>uvx go-to-wheel <span class=\"pl-k\">~</span>/dev/sqlite-scanner \\\r\n  --set-version-var main.version \\\r\n  --version 0.1.1 \\\r\n  --readme README.md \\\r\n  --author <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Simon Willison<span class=\"pl-pds\">'</span></span> \\\r\n  --url https://github.com/simonw/sqlite-scanner \\\r\n  --description <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Scan directories for SQLite databases<span class=\"pl-pds\">'</span></span></pre></div>\r\n<p>This created a set of wheels in the <code>dist/</code> folder. I tested one of them like this:</p>\r\n<div class=\"highlight highlight-source-shell\"><pre>uv run --with dist/sqlite_scanner-0.1.1-py3-none-macosx_11_0_arm64.whl \\\r\n  sqlite-scanner --version</pre></div>\r\n<p>When that spat out the correct version number I was confident everything had worked as planned, so I pushed the whole set of wheels to PyPI using <code>twine upload</code> like this:</p>\r\n<div class=\"highlight highlight-source-shell\"><pre>uvx twine upload dist/<span class=\"pl-k\">*</span></pre></div>\r\n<p>I had to paste in a PyPI API token I had saved previously.</p>\r\n<h4 id=\"i-expect-to-use-this-pattern-a-lot\">I expect to use this pattern a lot</h4>\r\n<p><code>sqlite-scanner</code> is very clearly meant as a proof-of-concept for this wider pattern - Python is very much capable of recursively crawling a directory structure looking for files that start with a specific byte prefix on its own!</p>\r\n<p>That said, I think there's a <em>lot</em> to be said for this pattern. Go is a great complement to Python - it's fast, compiles to small self-contained binaries, has excellent concurrency support and a rich ecosystem of libraries.</p>\r\n<p>Go is similar to Python in that it has a strong standard library. Go is particularly good for HTTP tooling - I've built several HTTP proxies in the past using Go's excellent <code>net/http/httputil.ReverseProxy</code> handler.</p>\r\n<p>I've also been experimenting with <a href=\"https://github.com/wazero/wazero\">wazero</a>, Go's robust and mature zero dependency WebAssembly runtime as part of my ongoing quest for the ideal sandbox for running untrusted code. <a href=\"https://github.com/simonw/research/tree/main/wasm-repl-cli\">Here's my latest experiment</a> with that library.</p>\r\n<p>Being able to seamlessly integrate Go binaries into Python projects without the end user having to think about Go at all - they <code>pip install</code> and everything Just Works - feels like a valuable addition to my toolbox.</p>"
        },
        {
          "type": "blogmark",
          "id": 9271,
          "created": "2026-02-04T22:42:34+00:00",
          "title": "Voxtral transcribes at the speed of sound - Hacker News",
          "body": "Mistral just released Voxtral Transcribe 2 - a family of two new models, one open weights, for transcribing audio to text. This is the latest in their Whisper-like model family, and a sequel to the original Voxtral which they released [in July 2025](https://simonwillison.net/2025/Jul/16/voxtral/).\r\n\r\nVoxtral Realtime - official name `Voxtral-Mini-4B-Realtime-2602` - is the open weights (Apache-2.0) model, available as a [8.87GB download from Hugging Face](https://huggingface.co/mistralai/Voxtral-Mini-4B-Realtime-2602).\r\n\r\nYou can try it out in this [live demo](https://huggingface.co/spaces/mistralai/Voxtral-Mini-Realtime) - don't be put off by the \"No microphone found\" message, clicking \"Record\" should have your browser request permission and then start the demo working. I was very impressed by the demo - I talked quickly and used jargon like Django and WebAssembly and it correctly transcribed my text within moments of me uttering each sound. \r\n\r\nThe closed weight model is called `voxtral-mini-latest` and can be accessed via the Mistral API, using calls that look something like this:\r\n\r\n<div class=\"highlight highlight-source-shell\"><pre>curl -X POST <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>https://api.mistral.ai/v1/audio/transcriptions<span class=\"pl-pds\">\"</span></span> \\\r\n  -H <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Authorization: Bearer <span class=\"pl-smi\">$MISTRAL_API_KEY</span><span class=\"pl-pds\">\"</span></span> \\\r\n  -F model=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>voxtral-mini-latest<span class=\"pl-pds\">\"</span></span> \\\r\n  -F file=@<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Pelican talk at the library.m4a<span class=\"pl-pds\">\"</span></span> \\\r\n  -F diarize=true \\\r\n  -F context_bias=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Datasette<span class=\"pl-pds\">\"</span></span> \\\r\n  -F timestamp_granularities=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>segment<span class=\"pl-pds\">\"</span></span></pre></div>\r\n\r\nIt's priced at $0.003/minute, which is $0.18/hour.\r\n\r\nThe Mistral API console now has a [speech-to-text playground](https://console.mistral.ai/build/audio/speech-to-text) for exercising the new model and it is *excellent*. You can upload an audio file and promptly get a diarized transcript in a pleasant interface, with options to download the result in text, SRT or JSON format.\r\n\r\n![Screenshot of a speech-to-text transcription interface for a file named \"Pelican talk at the library.m4a\". The toolbar shows \"Speech to text\" with Code, Transcribe, and Download buttons. The transcript shows timestamped segments from 5:53 to 6:53 with a speaker icon, reading: \"5:53 \u2013 6:01 So pelicans love to, they're very good at getting the most they can out of the topography when they're flying. 6:01 \u2013 6:06 And our winds come in from the northwest and they hit those bluffs and they're deflected up. 6:07 \u2013 6:18 And they will sit right, they'll fly north into a wind like five feet off those bluffs, but just five or ten feet off the surface because the winds dissipate. 6:19 \u2013 6:22 And they will surf that bluff all the way north. 6:23 \u2013 6:30 So you'll see a wind from the north at 15 miles an hour, and the pelicans are flying north into that wind and not flapping their wings. 6:31 \u2013 6:33 And it's one of the coolest things. 6:33 \u2013 6:35 You can only find it on San Francisco Coast. 6:36 \u2013 6:39 Where right where the bluffs are steep. 6:41 \u2013 6:43 Pacifica, you can find them there. 6:43 \u2013 6:51 They like their, what we call pier bums, which are typically pelicans that have, are in some sort of trouble. 6:51 \u2013 6:53 They're unable to catch food.\" The segment at 6:41\u20136:43 is highlighted in yellow. An audio waveform is shown at the bottom with a playhead near 6:40. Stats in the lower right show 53.90s, 7946.00s, and #45833.](https://static.simonwillison.net/static/2025/mistral-transcript-ui.jpg)"
        },
        {
          "type": "blogmark",
          "id": 9272,
          "created": "2026-02-05T00:23:38+00:00",
          "title": "Spotlighting The World Factbook as We Bid a Fond Farewell - Hacker News",
          "body": "Somewhat devastating news today from CIA:\r\n\r\n> One of CIA\u2019s oldest and most recognizable intelligence publications, The World Factbook, has sunset.\r\n\r\nThere's not even a hint as to *why* they decided to stop maintaining this publication, which has been their most useful public-facing initiative since 1971 and a cornerstone of the public internet since 1997.\r\n\r\nIn a bizarre act of cultural vandalism they've not just removed the entire site (including the archives of previous versions) but they've also set every single page to be a 302 redirect to their closure announcement.\r\n\r\nThe Factbook has been released into the public domain since the start. There's no reason not to continue to serve archived versions - a banner at the top of the page saying it's no longer maintained would be much better than removing all of that valuable content entirely.\r\n\r\nUp until 2020 the CIA published annual zip file archives of the entire site. Those are available (along with the rest of the Factbook) [on the Internet Archive](https://web.archive.org/web/20260203124934/https://www.cia.gov/the-world-factbook/about/archives/).\r\n\r\nI downloaded the 384MB `.zip` file for the year 2020 and extracted it into a new GitHub repository, [simonw/cia-world-factbook-2020](https://github.com/simonw/cia-world-factbook-2020/). I've enabled GitHub Pages for that repository so you can browse the archived copy at [simonw.github.io/cia-world-factbook-2020/](https://simonw.github.io/cia-world-factbook-2020).\r\n\r\n![Screenshot of the CIA World Factbook website homepage. Header reads \"THE WORLD FACTBOOK\" with a dropdown labeled \"Please select a country to view.\" Navigation tabs: ABOUT, REFERENCES, APPENDICES, FAQs. Section heading \"WELCOME TO THE WORLD FACTBOOK\" followed by descriptive text: \"The World Factbook provides information on the history, people and society, government, economy, energy, geography, communications, transportation, military, and transnational issues for 267 world entities. The Reference tab includes: a variety of world, regional, country, ocean, and time zone maps; Flags of the World; and a Country Comparison function that ranks the country information and data in more than 75 Factbook fields.\" A satellite image of Earth is displayed on the right. Below it: \"WHAT'S NEW :: Today is: Wednesday, February 4.\" Left sidebar links with icons: WORLD TRAVEL FACTS, ONE-PAGE COUNTRY SUMMARIES, REGIONAL AND WORLD MAPS, FLAGS OF THE WORLD, GUIDE TO COUNTRY COMPARISONS. Right side shows news updates dated December 17, 2020 about Electricity access and new Economy fields, and December 10, 2020 about Nepal and China agreeing on the height of Mount Everest at 8,848.86 meters. A \"VIEW ALL UPDATES\" button appears at the bottom.](https://static.simonwillison.net/static/2025/factbook-2020.jpg)\r\n\r\nHere's a neat example of the editorial voice of the Factbook from the [What's New page](https://simonw.github.io/cia-world-factbook-2020/docs/whatsnew.html), dated December 10th 2020:\r\n\r\n> Years of wrangling were brought to a close this week when officials from Nepal and China announced that they have agreed on the height of Mount Everest. The mountain sits on the border between Nepal and Tibet (in western China), and its height changed slightly following an earthquake in 2015. The new height of 8,848.86 meters is just under a meter higher than the old figure of 8,848 meters. *The World Factbook* rounds the new measurement to 8,849 meters and this new height has been entered throughout the *Factbook* database."
        },
        {
          "type": "note",
          "id": 239,
          "created": "2026-02-05T20:29:20+00:00",
          "title": "Opus 4.6 and Codex 5.3",
          "body": "Two major new model releases today, within about 15 minutes of each other.\r\n\r\nAnthropic [released Opus 4.6](https://www.anthropic.com/news/claude-opus-4-6). Here's [its pelican](https://gist.github.com/simonw/a6806ce41b4c721e240a4548ecdbe216):\r\n\r\n![Slightly wonky bicycle frame but an excellent pelican, very clear beak and pouch, nice feathers.](https://static.simonwillison.net/static/2026/opus-4.6-pelican.png)\r\n\r\nOpenAI [release GPT-5.3-Codex](https://openai.com/index/introducing-gpt-5-3-codex/), albeit only via their Codex app, not yet in their API. Here's [its pelican](https://gist.github.com/simonw/bfc4a83f588ac762c773679c0d1e034b):\r\n\r\n![Not nearly as good - the bicycle is a bit mangled, the pelican not nearly as well rendered - it's more of a line drawing.](https://static.simonwillison.net/static/2026/codex-5.3-pelican.png)\r\n\r\nI've had a bit of preview access to both of these models and to be honest I'm finding it hard to find a good angle to write about them - they're both *really good*, but so were their predecessors Codex 5.2 and Opus 4.5. I've been having trouble finding tasks that those previous models couldn't handle but the new ones are able to ace.\r\n\r\nThe most convincing story about capabilities of the new model so far is Nicholas Carlini from Anthropic talking about Opus 4.6 and [Building a C compiler with a team of parallel Claudes](https://www.anthropic.com/engineering/building-c-compiler) - Anthropic's version of Cursor's [FastRender project](https://simonwillison.net/2026/Jan/23/fastrender/)."
        },
        {
          "type": "blogmark",
          "id": 9273,
          "created": "2026-02-05T23:39:07+00:00",
          "title": "Mitchell Hashimoto: My AI Adoption Journey - Hacker News",
          "body": "Some really good and unconventional tips in here for getting to a place with coding agents where they demonstrably improve your workflow and productivity. I particularly liked:\r\n\r\n- [Reproduce your own work](https://mitchellh.com/writing/my-ai-adoption-journey#step-2-reproduce-your-own-work) - when learning to use coding agents Mitchell went through a period of doing the work manually, then recreating the same solution using agents as an exercise:\r\n\r\n    > I literally did the work twice. I'd do the work manually, and then I'd fight an agent to produce identical results in terms of quality and function (without it being able to see my manual solution, of course).\r\n\r\n- [End-of-day agents](https://mitchellh.com/writing/my-ai-adoption-journey#step-3-end-of-day-agents) - letting agents step in when your energy runs out:\r\n\r\n    > To try to find some efficiency, I next started up a new pattern: **block out the last 30 minutes of every day to kick off one or more agents.** My hypothesis was that *perhaps* I could gain some efficiency if the agent can make some *positive progress* in the times I can't work anyways.\r\n\r\n- [Outsource the Slam Dunks](https://mitchellh.com/writing/my-ai-adoption-journey#step-4-outsource-the-slam-dunks) - once you know an agent can likely handle a task, have it do that task while you work on something more interesting yourself."
        },
        {
          "type": "quotation",
          "id": 2015,
          "created": "2026-02-06T00:42:22+00:00",
          "title": "Karel D'Oosterlinck",
          "body": "When I want to quickly implement a one-off experiment in a part of the codebase I am unfamiliar with, I get codex to do extensive due diligence. Codex explores relevant slack channels, reads related discussions, fetches experimental branches from those discussions, and cherry picks useful changes for my experiment. All of this gets summarized in an extensive set of notes, with links back to where each piece of information was found. Using these notes, codex wires the experiment and makes a bunch of hyperparameter decisions I couldn\u2019t  possibly make without much more effort. - Karel D'Oosterlinck"
        },
        {
          "type": "blogmark",
          "id": 9274,
          "created": "2026-02-06T18:44:21+00:00",
          "title": "An Update on Heroku - ",
          "body": "An ominous headline to see on the official Heroku blog and yes, it's bad news.\r\n\r\n> Today, Heroku is transitioning to a sustaining engineering model focused on stability, security, reliability, and support. Heroku remains an actively supported, production-ready platform, with an emphasis on maintaining quality and operational excellence rather than introducing new features. We know changes like this can raise questions, and we want to be clear about what this means for customers.\r\n\r\nBased on context I'm guessing a \"sustaining engineering model\" (this definitely isn't a widely used industry term) means that they'll keep the lights on and that's it.\r\n\r\nThis is a very frustrating piece of corporate communication. \"We want to be clear about what this means for customers\" - then proceeds to *not be clear* about what this means for customers.\r\n\r\nWhy are they doing this? Here's their explanation:\r\n\r\n> We\u2019re focusing our product and engineering investments on areas where we can deliver the greatest long-term customer value, including helping organizations build and deploy enterprise-grade AI in a secure and trusted way.\r\n\r\nMy blog is the only project I have left running on Heroku. I guess I'd better migrate it away (probably to Fly) before Salesforce lose interest completely."
        },
        {
          "type": "entry",
          "id": 9119,
          "created": "2026-02-06T22:31:31+00:00",
          "title": "Running Pydantic's Monty Rust sandboxed Python subset in WebAssembly",
          "body": "<p>There's a jargon-filled headline for you! Everyone's <a href=\"https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/#1-year-we-re-finally-going-to-solve-sandboxing\">building sandboxes</a> for running untrusted code right now, and Pydantic's latest attempt, <a href=\"https://github.com/pydantic/monty\">Monty</a>, provides a custom Python-like language (a subset of Python) in Rust and makes it available as both a Rust library and a Python package. I got it working in WebAssembly, providing a sandbox-in-a-sandbox.</p>\r\n<p>Here's <a href=\"https://github.com/pydantic/monty\">how they describe Monty</a>:</p>\r\n<blockquote>\r\n<p>Monty avoids the cost, latency, complexity and general faff of using full container based sandbox for running LLM generated code.</p>\r\n<p>Instead, it let's you safely run Python code written by an LLM embedded in your agent, with startup times measured in single digit microseconds not hundreds of milliseconds.</p>\r\n<p>What Monty <strong>can</strong> do:</p>\r\n<ul>\r\n<li>Run a reasonable subset of Python code - enough for your agent to express what it wants to do</li>\r\n<li>Completely block access to the host environment: filesystem, env variables and network access are all implemented via external function calls the developer can control</li>\r\n<li>Call functions on the host - only functions you give it access to [...]</li>\r\n</ul>\r\n</blockquote>\r\n<p>A quick way to try it out is via <a href=\"https://github.com/astral-sh/uv\">uv</a>:</p>\r\n<pre><code>uv run --with pydantic-monty python -m asyncio\r\n</code></pre>\r\n<p>Then paste this into the Python interactive prompt - the <code>-m asyncio</code> enables top-level await:</p>\r\n<pre><span>import</span> <span>pydantic_monty</span>\r\n<span>code</span> <span>=</span> <span>pydantic_monty</span>.<span>Monty</span>(<span>'print(\"hello \" + str(4 * 5))'</span>)\r\n<span>await</span> <span>pydantic_monty</span>.<span>run_monty_async</span>(<span>code</span>)</pre>\r\n<p>Monty supports a <em>very</em> small subset of Python - it doesn't even support class declarations yet!</p>\r\n<p>But, given its target use-case, that's not actually a problem.</p>\r\n<p>The neat thing about providing tools like this for LLMs is that they're really good at iterating against error messages. A coding agent can run some Python code, get an error message telling it that classes aren't supported and then try again with a different approach.</p>\r\n<p>I wanted to try this in a browser, so I fired up <a href=\"https://simonwillison.net/2025/Nov/6/async-code-research/\">a code research task</a> in Claude Code for web and kicked it off with the following:</p>\r\n<blockquote>\r\n<p>Clone <a href=\"https://github.com/pydantic/monty\">https://github.com/pydantic/monty</a> to /tmp and figure out how to compile it into a python WebAssembly wheel that can then be loaded in Pyodide. The wheel file itself should be checked into the repo along with build scripts and passing pytest playwright test scripts that load Pyodide from a CDN and the wheel from a \u201cpython -m http.server\u201d localhost and demonstrate it working</p>\r\n</blockquote>\r\n<p>Then a little later:</p>\r\n<blockquote>\r\n<p>I want an additional WASM file that works independently of Pyodide, which is also usable in a web browser - build that too along with playwright tests that show it working. Also build two HTML files - one called demo.html and one called pyodide-demo.html - these should work similar to <a href=\"https://tools.simonwillison.net/micropython\">https://tools.simonwillison.net/micropython</a> (download that code with curl to inspect it) - one should load the WASM build, the other should load Pyodide and have it use the WASM wheel. These will be served by GitHub Pages so they can load the WASM and wheel from a relative path since the .html files will be served from the same folder as the wheel and WASM file</p>\r\n</blockquote>\r\n<p>Here's <a href=\"https://gisthost.github.io/?22d88e6367d7e002c4fb383c213c2df2/page-001.html\">the transcript</a>, and the <a href=\"https://github.com/simonw/research/tree/main/monty-wasm-pyodide\">final research report</a> it produced.</p>\r\n<p>I now have the Monty Rust code compiled to WebAssembly in two different shapes - as a <code>.wasm</code> bundle you can load and call from JavaScript, and as a <code>monty-wasm-pyodide/pydantic_monty-0.0.3-cp313-cp313-emscripten_4_0_9_wasm32.whl</code> wheel file which can be loaded into <a href=\"https://pyodide.org/\">Pyodide</a> and then called from Python in Pyodide in WebAssembly in a browser.</p>\r\n<p>Here are those two demos, hosted on GitHub Pages:</p>\r\n<ul>\r\n<li>\r\n<a href=\"https://simonw.github.io/research/monty-wasm-pyodide/demo.html\">Monty WASM demo</a> - a UI over JavaScript that loads the Rust WASM module directly.</li>\r\n<li>\r\n<a href=\"https://simonw.github.io/research/monty-wasm-pyodide/pyodide-demo.html\">Monty Pyodide demo</a> - this one provides an identical interface but here the code is <a href=\"https://github.com/simonw/research/blob/3add1ffec70b530711fa237d91f546da5bcf1f1c/monty-wasm-pyodide/pyodide-demo.html#L257-L280\">loading Pyodide and then installing the Monty WASM wheel</a>.</li>\r\n</ul>\r\n<p><img src=\"https://static.simonwillison.net/static/2026/monty-pyodide.jpg\" alt=\"Screenshot of a web app titled &quot;Monty via Pyodide&quot; with description &quot;Run Monty (a sandboxed Python interpreter by Pydantic) inside Pyodide (CPython compiled to WebAssembly). This loads the pydantic-monty wheel and uses its full Python API. Code is saved in the URL for sharing.&quot; A green banner reads &quot;Code executed successfully!&quot; Below are example buttons labeled &quot;Basic&quot;, &quot;Inputs&quot;, &quot;Reuse&quot;, &quot;Error Handling&quot;, &quot;Fibonacci&quot;, and &quot;Classes&quot;. A code editor labeled &quot;Python Code (runs inside Monty sandbox via Pyodide):&quot; contains: &quot;import pydantic_monty\\n\\n# Create interpreter with input variables\\nm = pydantic_monty.Monty('x + y', inputs=['x', 'y'])\\n\\n# Run with different inputs\\nresult1 = m.run(inputs={&quot;x&quot;: 10, &quot;y&quot;: 20})\\nprint(f&quot;10 + 20 = {result1}&quot;)\\n\\nresult2 = m.run(inputs={&quot;x&quot;: 100, &quot;y&quot;: 200})&quot; with &quot;Run Code&quot; and &quot;Clear&quot; buttons. The Output section shows &quot;10 + 20 = 30&quot; and &quot;100 + 200 = 300&quot; with a &quot;Copy&quot; button. Footer reads &quot;Executed in 4.0ms&quot;.\" style=\"max-width: 100%;\" /></p>\r\n<p>As a connoisseur of sandboxes - the more options the better! - this new entry from Pydantic ticks a lot of my boxes. It's small, fast, widely available (thanks to Rust and WebAssembly) and provides strict limits on memory usage, CPU time and access to disk and network.</p>\r\n<p>It was also a great excuse to spin up another demo showing how easy it is these days to turn compiled code like C or Rust into WebAssembly that runs in both a browser and a Pyodide environment.</p>"
        },
        {
          "type": "quotation",
          "id": 2016,
          "created": "2026-02-06T23:41:31+00:00",
          "title": "Tom Dale",
          "body": "I don't know why this week became the tipping point, but nearly every software engineer I've talked to is experiencing some degree of mental health crisis.\r\n\r\n[...] Many people assuming I meant job loss anxiety but that's just one presentation. I'm seeing near-manic episodes triggered by watching software shift from scarce to abundant. Compulsive behaviors around agent usage. Dissociative awe at the temporal compression of change. It's not fear necessarily just the cognitive overload from living in an inflection point. - Tom Dale"
        },
        {
          "type": "entry",
          "id": 9120,
          "created": "2026-02-07T15:40:48+00:00",
          "title": "How StrongDM's AI team build serious software without even looking at the code",
          "body": "<p>Last week <a href=\"https://simonwillison.net/2026/Jan/28/the-five-levels/\">I hinted at</a> a demo I had seen from a team implementing what Dan Shapiro called <a href=\"https://www.danshapiro.com/blog/2026/01/the-five-levels-from-spicy-autocomplete-to-the-software-factory/\">the Dark Factory</a> level of AI adoption, where no human even looks at the code the coding agents are producing. That team was part of StrongDM, and they've just shared the first public description of how they are working in <a href=\"https://factory.strongdm.ai\">Software Factories and the Agentic Moment</a>:</p>\r\n<blockquote>\r\n<p>We built a <strong>Software Factory</strong>: non-interactive development where specs + scenarios drive agents that write code, run harnesses, and converge without human review. [...]</p>\r\n<p>In k\u014dan or mantra form:</p>\r\n<ul>\r\n<li>Why am I doing this? (implied: the model should be doing this instead)</li>\r\n</ul>\r\n<p>In rule form:</p>\r\n<ul>\r\n<li>Code <strong>must not be</strong> written by humans</li>\r\n<li>Code <strong>must not be</strong> reviewed by humans</li>\r\n</ul>\r\n<p>Finally, in practical form:</p>\r\n<ul>\r\n<li>If you haven't spent at least <strong>$1,000 on tokens today</strong> per human engineer, your software factory has room for improvement</li>\r\n</ul>\r\n</blockquote>\r\n<p>I think the most interesting of these, without a doubt, is \"Code <strong>must not be</strong> reviewed by humans\". How could that <em>possibly</em> be a sensible strategy when we all know how prone LLMs are to making <a href=\"https://simonwillison.net/2025/Mar/2/kellan-elliott-mccrea/\">inhuman mistakes</a>?</p>\r\n<p>I've seen many developers recently acknowledge the <a href=\"https://simonwillison.net/2026/Jan/4/inflection/\">November 2025 inflection point</a>, where Claude Opus 4.5 and GPT 5.2 appeared to turn the corner on how reliably a coding agent could follow instructions and take on complex coding tasks. StrongDM's AI team was founded in July 2025 based on an earlier inflection point relating to Claude Sonnet 3.5:</p>\r\n<blockquote>\r\n<p>The catalyst was a transition observed in late 2024: with the second revision of Claude 3.5 (October 2024), long-horizon agentic coding workflows began to compound correctness rather than error.</p>\r\n<p>By December of 2024, the model's long-horizon coding performance was unmistakable via Cursor's <a href=\"https://forum.cursor.com/t/yolo-mode-is-amazing/36262\">YOLO mode</a>.</p>\r\n</blockquote>\r\n<p>Their new team started with the rule \"no hand-coded software\" - radical for July 2025, but something I'm seeing significant numbers of experienced developers start to adopt as of January 2026.</p>\r\n<p>They quickly ran into the obvious problem: if you're not writing anything by hand, how do you ensure that the code actually works? Having the agents write tests only helps if they don't cheat and <code>assert true</code>.</p>\r\n<p>This feels like the most consequential question in software development right now: how can you <a href=\"https://simonwillison.net/2025/Dec/18/code-proven-to-work/\">prove that software you are producing works</a> if both the implementation and the tests are being written for you by coding agents?</p>\r\n<p>StrongDM's answer was inspired by <a href=\"https://en.wikipedia.org/wiki/Scenario_testing\">Scenario testing</a> (Cem Kaner, 2003). As StrongDM describe it:</p>\r\n<blockquote>\r\n<p>We repurposed the word <strong>scenario</strong> to represent an end-to-end \"user story\", often stored outside the codebase (similar to a \"holdout\" set in model training), which could be intuitively understood and flexibly validated by an LLM.</p>\r\n<p>Because much of the software we grow itself has an agentic component, we transitioned from boolean definitions of success (\"the test suite is green\") to a probabilistic and empirical one. We use the term <strong>satisfaction</strong> to quantify this validation: of all the observed trajectories through all the scenarios, what fraction of them likely satisfy the user?</p>\r\n</blockquote>\r\n<p>That idea of treating scenarios as holdout sets - used to evaluate the software but not stored where the coding agents can see them - is <em>fascinating</em>. It imitates aggressive testing by an external QA team - an expensive but highly effective way of ensuring quality in traditional software.</p>\r\n<p>Which leads us to StrongDM's concept of a <strong>Digital Twin Universe</strong> - the part of the demo I saw that made the strongest impression on me.</p>\r\n<p>The software they were building helped manage user permissions across a suite of connected services. This in itself was notable - security software is the last thing you would expect to be built using unreviewed LLM code!</p>\r\n<blockquote>\r\n<p>[The Digital Twin Universe is] behavioral clones of the third-party services our software depends on. We built twins of Okta, Jira, Slack, Google Docs, Google Drive, and Google Sheets, replicating their APIs, edge cases, and observable behaviors.</p>\r\n<p>With the DTU, we can validate at volumes and rates far exceeding production limits. We can test failure modes that would be dangerous or impossible against live services. We can run thousands of scenarios per hour without hitting rate limits, triggering abuse detection, or accumulating API costs.</p>\r\n</blockquote>\r\n<p>How do you clone the important parts of Okta, Jira, Slack and more? With coding agents!</p>\r\n<p>As I understood it the trick was effectively to dump the full public API documentation of one of those services into their agent harness and have it build an imitation of that API, as a self-contained Go binary. They could then have it build a simplified UI over the top to help complete the simulation.</p>\r\n\r\n<p><strong>Update</strong>: DTU creator Jay Taylor posted some extra context about this <a href=\"https://news.ycombinator.com/item?id=46924426#46931812\">on Hacker News</a> sharing a key prompting strategy:</p>\r\n<blockquote>\r\n<p>I did have an initial key insight which led to a repeatable strategy to ensure a high level of fidelity between DTU vs. the official canonical SaaS services:</p>\r\n<p><code>Use the top popular publicly available reference SDK client libraries as compatibility targets, with the goal always being 100% compatibility.</code></p>\r\n</blockquote>\r\n\r\n<p>With their own, independent clones of those services - free from rate-limits or usage quotas - their army of simulated testers could go <em>wild</em>. Their scenario tests became scripts for agents to constantly execute against the new systems as they were being built.</p>\r\n<p>This screenshot of their Slack twin also helps illustrate how the testing process works, showing a stream of simulated Okta users who are about to need access to different simulated systems.</p>\r\n<p><img src=\"https://static.simonwillison.net/static/2026/strong-dm-slack.jpg\" alt=\"Screenshot of a Slack-like interface titled &quot;DTU Slack&quot; showing a thread view (Thread \u2014 C4B9FBB97) with &quot;Focus first&quot; and &quot;Leave&quot; buttons. The left sidebar lists channels including # org-general (182), # general (0) (shared\u00d72), # it-support (0), # channel-0002 (0) (shared\u00d72), # channel-0003 (0) through # channel-0020 (0), # org-finance (1), and a DMs section with a &quot;Start&quot; button. A &quot;Create&quot; button appears at the top of the sidebar. The main thread shows approximately 9 automated introduction messages from users with Okta IDs (e.g. @okta-u-423438-00001, @okta-u-423438-00002, etc.), all timestamped 2025-11-12Z between 18:50:31 and 18:51:51. Each message follows the format &quot;Hi team! I'm [Name], joining as Employee in general. Key skills: [fictional skill phrases]. Excited to contribute!&quot; All users have red/orange &quot;O&quot; avatar icons.\" style=\"max-width: 100%;\" /></p>\r\n<p>This ability to quickly spin up a useful clone of a subset of Slack helps demonstrate how disruptive this new generation of coding agent tools can be:</p>\r\n<blockquote>\r\n<p>Creating a high fidelity clone of a significant SaaS application was always possible, but never economically feasible. Generations of engineers may have <em>wanted</em> a full in-memory replica of their CRM to test against, but self-censored the proposal to build it.</p>\r\n</blockquote>\r\n<p>The <a href=\"https://factory.strongdm.ai/techniques\">techniques page</a> is worth a look too. In addition to the Digital Twin Universe they introduce terms like <strong><a href=\"https://factory.strongdm.ai/techniques/gene-transfusion\">Gene Transfusion</a></strong> for having agents extract patterns from existing systems and reuse them elsewhere, <strong><a href=\"https://factory.strongdm.ai/techniques/semport\">Semports</a></strong> for directly porting code from one language to another and <strong><a href=\"https://factory.strongdm.ai/techniques/pyramid-summaries\">Pyramid Summaries</a></strong> for providing multiple levels of summary such that an agent can enumerate the short ones quickly and zoom in on more detailed information as it is needed.</p>\r\n<p>StrongDM AI also released some software - in an appropriately unconventional manner.</p>\r\n<p><a href=\"https://github.com/strongdm/attractor\">github.com/strongdm/attractor</a> is <strong>Attractor</strong>, the non-interactive coding agent at the heart of their software factory. Except the repo itself contains no code at all - just three markdown files describing the spec for the software in meticulous detail, and a note in the README that you should feed those specs into your coding agent of choice!</p>\r\n<p><a href=\"https://github.com/strongdm/cxdb\">github.com/strongdm/cxdb</a> is a more traditional release, with 16,000 lines of Rust, 9,500 of Go and 6,700 of TypeScript. This is their \"AI Context Store\" - a system for storing conversation histories and tool outputs in an immutable DAG.</p>\r\n<p>It's similar to my LLM tool's <a href=\"https://llm.datasette.io/en/stable/logging.html#sql-schema\">SQLite logging mechanism</a> but a whole lot more sophisticated. I may have to gene transfuse some ideas out of this one!</p>\r\n<h4 id=\"a-glimpse-of-the-future-\">A glimpse of the future?</h4>\r\n<p>I visited the StrongDM AI team back in October as part of a small group of invited guests.</p>\r\n<p>The three person team of Justin McCarthy, Jay Taylor and Navan Chauhan had formed just three months earlier, and they already had working demos of their coding agent harness, their Digital Twin Universe clones of half a dozen services and a swarm of simulated test agents running through scenarios. And this was prior to the Opus 4.5/GPT 5.2 releases that made agentic coding significantly more reliable a month after those demos.</p>\r\n<p>It felt like a glimpse of one potential future of software development, where software engineers move from building the code to building and then semi-monitoring the systems that build the code. The Dark Factory.</p>\r\n\r\n<h4 id=\"wait-1-000-day-per-engineer-\">Wait, $1,000/day per engineer?</h4>\r\n<p>I glossed over this detail in my first published version of this post, but it deserves some serious attention.</p>\r\n<p>If these patterns really do add $20,000/month per engineer to your budget they're far less interesting to me. At that point this becomes more of a business model exercise: can you create a profitable enough line of products that you can afford the enormous overhead of developing software in this way?</p>\r\n<p>Building sustainable software businesses also looks very different when any competitor can potentially clone your newest features with a few hours of coding agent work.</p>\r\n<p>I hope these patterns can be put into play with a much lower spend. I've personally found the $200/month Claude Max plan gives me plenty of space to experiment with different agent patterns, but I'm also not running a swarm of QA testers 24/7!</p>\r\n<p>I think there's a lot to learn from StrongDM even for teams and individuals who aren't going to burn thousands of dollars on token costs. I'm particularly invested in the question of what it takes to have agents prove that their code works without needing to review every line of code they produce.</p>"
        },
        {
          "type": "quotation",
          "id": 2017,
          "created": "2026-02-07T21:31:44+00:00",
          "title": "David Crawshaw",
          "body": "I am having more fun programming than I ever have, because so many more of the programs I wish I could find the time to write actually exist. I wish I could share this joy with the people who are fearful about the changes agents are bringing. The fear itself I understand, I have fear more broadly about what the end-game is for intelligence on tap in our society. But in the limited domain of writing computer programs these tools have brought so much exploration and joy to my work. - David Crawshaw"
        },
        {
          "type": "blogmark",
          "id": 9276,
          "created": "2026-02-07T23:10:33+00:00",
          "title": "Claude: Speed up responses with fast mode - ",
          "body": "New \"research preview\" from Anthropic today: you can now access a faster version of their frontier model Claude Opus 4.6 by typing `/fast` in Claude Code... but at a cost that's 6x the normal price.\r\n\r\nOpus is usually $5/million input and $25/million output. The new fast mode is $30/million input and $150/million output!\r\n\r\nThere's a 50% discount until the end of February 16th, so only a 3x multiple (!) before then.\r\n\r\nHow much faster is it? The linked documentation doesn't say, but [on Twitter](https://x.com/claudeai/status/2020207322124132504) Claude say:\r\n\r\n> Our teams have been building with a 2.5x-faster version of Claude Opus 4.6.\r\n>\r\n> We\u2019re now making it available as an early experiment via Claude Code and our API.\r\n\r\nClaude Opus 4.5 had a context limit of 200,000 tokens. 4.6 has an option to increase that to 1,000,000 at 2x the input price ($10/m) and 1.5x the output price ($37.50/m) once your input exceeds 200,000 tokens. These multiples hold for fast mode too, so after Feb 16th you'll be able to pay a hefty $60/m input and $225/m output for Anthropic's fastest best model."
        },
        {
          "type": "blogmark",
          "id": 9277,
          "created": "2026-02-07T23:57:57+00:00",
          "title": "Vouch - ",
          "body": "Mitchell Hashimoto's new system to help address the deluge of worthless AI-generated PRs faced by open source projects now that the friction involved in contributing has dropped so low.\r\n\r\n[He says](https://twitter.com/mitchellh/status/2020252149117313349):\r\n\r\n> The idea is simple: Unvouched users can't contribute to your projects. Very bad users can be explicitly \"denounced\", effectively blocked. Users are vouched or denounced by contributors via GitHub issue or discussion comments or via the CLI.\r\n> \r\n> Integration into GitHub is as simple as adopting the published GitHub actions. Done. Additionally, the system itself is generic to forges and not tied to GitHub in any way.\r\n> \r\n> Who and how someone is vouched or denounced is up to the project. I'm not the value police for the world. Decide for yourself what works for your project and your community."
        },
        {
          "type": "quotation",
          "id": 2018,
          "created": "2026-02-08T02:25:53+00:00",
          "title": "Thomas Ptacek",
          "body": "People on the orange site are laughing at this, assuming it's just an ad and that there's nothing to it. Vulnerability researchers I talk to do not think this is a joke. As an erstwhile vuln researcher myself: do not bet against LLMs on this.\r\n\r\n[Axios: Anthropic's Claude Opus 4.6 uncovers 500 zero-day flaws in open-source](https://www.axios.com/2026/02/05/anthropic-claude-opus-46-software-hunting)\r\n\r\nI think vulnerability research might be THE MOST LLM-amenable software engineering problem. Pattern-driven. Huge corpus of operational public patterns. Closed loops. Forward progress from stimulus/response tooling. Search problems.\r\n\r\nVulnerability research outcomes are in THE MODEL CARDS for frontier labs. Those companies have so much money they're literally distorting the economy. Money buys vuln research outcomes. Why would you think they were faking any of this? - Thomas Ptacek"
        },
        {
          "type": "note",
          "id": 240,
          "created": "2026-02-08T17:25:07+00:00",
          "title": "K\u0101k\u0101p\u014d mug by Karen James",
          "body": "<p>Friend and neighbour <a href=\"https://www.etsy.com/shop/KarenJamesMakes\">Karen James</a> made me a K\u0101k\u0101p\u014d mug. It has a charismatic K\u0101k\u0101p\u014d, four K\u0101k\u0101p\u014d chicks (in celebration of the <a href=\"https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/#1-year-k-k-p-parrots-will-have-an-outstanding-breeding-season\">2026 breeding season</a>) and even has some <a href=\"https://www.theguardian.com/world/2026/jan/13/nz-kakapo-mating-season\">rimu fruit</a>!</p>\r\n<p><img src=\"https://static.simonwillison.net/static/2026/kakapo-mug-1.jpg\" alt=\"A simply spectacular sgraffito ceramic mug with a bold, charismatic K\u0101k\u0101p\u014d parrot taking up most of the visible space. It has a yellow beard and green feathers.\" style=\"max-width: 100%;\" /></p>\r\n<p><img src=\"https://static.simonwillison.net/static/2026/kakapo-mug-2.jpg\" alt=\"Another side of the mug, two cute grey K\u0101k\u0101p\u014d chicks are visible and three red rimu fruit that look like berries, one on the floor and two hanging from wiry branches.\" style=\"max-width: 100%;\" /></p>\r\n<p>I love it so much.</p>"
        },
        {
          "type": "blogmark",
          "id": 9278,
          "created": "2026-02-09T16:43:07+00:00",
          "title": "AI Doesn\u2019t Reduce Work\u2014It Intensifies It - Hacker News",
          "body": "Aruna Ranganathan and Xingqi Maggie Ye from Berkeley Haas\u00a0School of Business report initial findings in the HBR from their April to December 2025 study of 200 employees at a \"U.S.-based technology company\".\r\n\r\nThis captures an effect I've been observing in my own work with LLMs: the productivity boost these things can provide is *exhausting*.\r\n\r\n> AI introduced a new rhythm in which workers managed several active threads at once: manually writing code while AI generated an alternative version, running multiple agents in parallel, or reviving long-deferred tasks because AI could \u201chandle them\u201d in the background. They did this, in part, because they felt they had a \u201cpartner\u201d that could help them move through their workload.\r\n>\r\n> While this sense of having a \u201cpartner\u201d enabled a feeling of momentum, the reality was a continual switching of attention, frequent checking of AI outputs, and a growing number of open tasks. This created cognitive load and a sense of always juggling, even as the work felt productive.\r\n\r\nI'm frequently finding myself with work on two or three projects running parallel. I can get *so much done*, but after just an hour or two my mental energy for the day feels almost entirely depleted.\r\n\r\nI've had conversations with people recently who are losing sleep because they're finding building yet another feature with \"just one more prompt\" irresistible.\r\n\r\nThe HBR piece calls for organizations to build an \"AI practice\" that structures how AI is used to help avoid burnout and counter effects that \"make it harder for organizations to distinguish genuine productivity gains from unsustainable intensity\".\r\n\r\nI think we've just disrupted decades of existing intuition about sustainable working practices. It's going to take a while and some discipline to find a good new balance."
        },
        {
          "type": "blogmark",
          "id": 9279,
          "created": "2026-02-09T23:56:51+00:00",
          "title": "Structured Context Engineering for File-Native Agentic Systems - @omarsar0",
          "body": "New paper by Damon McMillan exploring challenging LLM context tasks involving large SQL schemas (up to 10,000 tables) across different models and file formats:\r\n\r\n> Using SQL generation as a proxy for programmatic agent operations, we present a systematic study of context engineering for structured data, comprising 9,649 experiments across 11 models, 4 formats (YAML, Markdown, JSON, Token-Oriented Object Notation [TOON]), and schemas ranging from 10 to 10,000 tables.\r\n\r\nUnsurprisingly, the biggest impact was the models themselves - with frontier models (Opus 4.5, GPT-5.2, Gemini 2.5 Pro) beating the leading open source models (DeepSeek V3.2, Kimi K2, Llama 4).\r\n\r\nThose frontier models benefited from filesystem based context retrieval, but the open source models had much less convincing results with those, which reinforces my feeling that the filesystem coding agent loops aren't handled as well by open weight models just yet. The [Terminal Bench 2.0](https://www.tbench.ai/leaderboard/terminal-bench/2.0) leaderboard is still dominated by Anthropic, OpenAI and Gemini.\r\n\r\nThe \"grep tax\" result against [TOON](https://github.com/toon-format/toon) was an interesting detail. TOON is meant to represent structured data in as few tokens as possible, but it turns out the model's unfamiliarity with that format led to them spending significantly more tokens over multiple iterations trying to figure it out:\r\n\r\n![Screenshot of a figure from a research paper. Introductory text reads: \"As schema size increased, TOON showed dramatically increased token consumption for Claude models despite being ~25% smaller in file size. Scale experiments used Claude models only.\" Below is \"Figure 7: The 'Grep Tax' - TOON Token Overhead at Scale\", a bar chart with a logarithmic y-axis labeled \"Tokens\" comparing YAML (teal) and TOON (purple) at two schema sizes: S5 (500 tables) and S9 (10,000 tables). At S5, TOON is +138% more tokens than YAML (~1,100 vs ~450). At S9, TOON is +740% more tokens (~50,000 vs ~7,000). Below the chart, explanatory text reads: \"The 'grep tax' emerged as schema size scaled. At S5 (500 tables), TOON consumed 138% more tokens than YAML; at S9 (10,000 tables), this grew to 740%. Root cause: models lacked familiarity with TOON's syntax and could not construct effective refinement patterns.\"](https://static.simonwillison.net/static/2026/grep-tax.jpg)"
        },
        {
          "type": "entry",
          "id": 9121,
          "created": "2026-02-10T17:45:29+00:00",
          "title": "Introducing Showboat and Rodney, so agents can demo what they\u2019ve built",
          "body": "<p>A key challenge working with coding agents is having them both test what they\u2019ve built and demonstrate that software to you, their supervisor. This goes beyond automated tests - we need artifacts that show their progress and help us see exactly what the agent-produced software is able to do. I\u2019ve just released two new tools aimed at this problem: <a href=\"https://github.com/simonw/showboat\">Showboat</a> and <a href=\"https://github.com/simonw/rodney\">Rodney</a>.</p>\r\n\r\n<ul>\r\n  <li><a href=\"https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#proving-code-actually-works\">Proving code actually works</a></li>\r\n  <li><a href=\"https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#showboat-agents-build-documents-to-demo-their-work\">Showboat: Agents build documents to demo their work</a></li>\r\n  <li><a href=\"https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#rodney-cli-browser-automation-designed-to-work-with-showboat\">Rodney: CLI browser automation designed to work with Showboat</a></li>\r\n  <li><a href=\"https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#test-driven-development-helps-but-we-still-need-manual-testing\">Test-driven development helps, but we still need manual testing</a></li>\r\n  <li><a href=\"https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#i-built-both-of-these-tools-on-my-phone\">I built both of these tools on my phone</a></li>\r\n</ul>\r\n\r\n<h4 id=\"proving-code-actually-works\">Proving code actually works</h4>\r\n<p>I recently wrote about how the job of a software engineer isn't to write code, it's to <em><a href=\"https://simonwillison.net/2025/Dec/18/code-proven-to-work/\">deliver code that works</a></em>. A big part of that is proving to ourselves and to other people that the code we are responsible for behaves as expected.</p>\r\n<p>This becomes even more important - and challenging - as we embrace coding agents as a core part of our software development process.</p>\r\n<p>The more code we churn out with agents, the more valuable tools are that reduce the amount of manual QA time we need to spend.</p>\r\n<p>One of the most interesting things about <a href=\"https://simonwillison.net/2026/Feb/7/software-factory/\">the StrongDM software factory model</a> is how they ensure that their software is well tested and delivers value despite their policy that \"code must not be reviewed by humans\". Part of their solution involves expensive swarms of QA agents running through \"scenarios\" to exercise their software. It's fascinating, but I don't want to spend thousands of dollars on QA robots if I can avoid it!</p>\r\n<p>I need tools that allow agents to clearly demonstrate their work to me, while minimizing the opportunities for them to cheat about what they've done.</p>\r\n\r\n<h4 id=\"showboat-agents-build-documents-to-demo-their-work\">Showboat: Agents build documents to demo their work</h4>\r\n<p><strong><a href=\"https://github.com/simonw/showboat\">Showboat</a></strong> is the tool I built to help agents demonstrate their work to me.</p>\r\n<p>It's a CLI tool (a Go binary, optionally <a href=\"https://simonwillison.net/2026/Feb/4/distributing-go-binaries/\">wrapped in Python</a> to make it easier to install) that helps an agent construct a Markdown document demonstrating exactly what their newly developed code can do.</p>\r\n<p>It's not designed for humans to run, but here's how you would run it anyway:</p>\r\n<div class=\"highlight highlight-source-shell\"><pre>showboat init demo.md <span class=\"pl-s\"><span class=\"pl-pds\">'</span>How to use curl and jq<span class=\"pl-pds\">'</span></span>\r\nshowboat note demo.md <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Here's how to use curl and jq together.<span class=\"pl-pds\">\"</span></span>\r\nshowboat <span class=\"pl-c1\">exec</span> demo.md bash <span class=\"pl-s\"><span class=\"pl-pds\">'</span>curl -s https://api.github.com/repos/simonw/rodney | jq .description<span class=\"pl-pds\">'</span></span>\r\nshowboat note demo.md <span class=\"pl-s\"><span class=\"pl-pds\">'</span>And the curl logo, to demonstrate the image command:<span class=\"pl-pds\">'</span></span>\r\nshowboat image demo.md <span class=\"pl-s\"><span class=\"pl-pds\">'</span>curl -o curl-logo.png https://curl.se/logo/curl-logo.png &amp;&amp; echo curl-logo.png<span class=\"pl-pds\">'</span></span></pre></div>\r\n<p>Here's what the result looks like if you open it up in VS Code and preview the Markdown:</p>\r\n<p><img src=\"https://static.simonwillison.net/static/2026/curl-demo.jpg\" alt=\"Screenshot showing a Markdown file &quot;demo.md&quot; side-by-side with its rendered preview. The Markdown source (left) shows: &quot;# How to use curl and jq&quot;, italic timestamp &quot;2026-02-10T01:12:30Z&quot;, prose &quot;Here's how to use curl and jq together.&quot;, a bash code block with &quot;curl -s https://api.github.com/repos/simonw/rodney | jq .description&quot;, output block showing '&quot;CLI tool for interacting with the web&quot;', text &quot;And the curl logo, to demonstrate the image command:&quot;, a bash {image} code block with &quot;curl -o curl-logo.png https://curl.se/logo/curl-logo.png &amp;&amp; echo curl-logo.png&quot;, and a Markdown image reference &quot;2056e48f-2026-02-10&quot;. The rendered preview (right) displays the formatted heading, timestamp, prose, styled code blocks, and the curl logo image in dark teal showing &quot;curl://&quot; with circuit-style design elements.\" style=\"max-width: 100%;\" /></p>\r\n<p>Here's that <a href=\"https://gist.github.com/simonw/fb0b24696ed8dd91314fe41f4c453563#file-demo-md\">demo.md file in a Gist</a>.</p>\r\n<p>So a sequence of <code>showboat init</code>, <code>showboat note</code>, <code>showboat exec</code> and <code>showboat image</code> commands constructs a Markdown document one section at a time, with the output of those <code>exec</code> commands automatically added to the document directly following the commands that were run.</p>\r\n<p>The <code>image</code> command is a little special - it looks for a file path to an image in the output of the command and copies that image to the current folder and references it in the file.</p>\r\n<p>That's basically the whole thing! There's a <code>pop</code> command to remove the most recently added section if something goes wrong, a <code>verify</code> command to re-run the document and check nothing has changed (I'm not entirely convinced by the design of that one) and a <code>extract</code> command that reverse-engineers the CLI commands that were used to create the document.</p>\r\n<p>It's pretty simple - just 172 lines of Go.</p>\r\n<p>I packaged it up with my <a href=\"https://github.com/simonw/go-to-wheel\">go-to-wheel</a> tool which means you can run it without even installing it first like this:</p>\r\n<div class=\"highlight highlight-source-shell\"><pre>uvx showboat --help</pre></div>\r\n<p>That <code>--help</code> command is really important: it's designed to provide a coding agent with <em>everything it needs to know</em> in order to use the tool. Here's <a href=\"https://github.com/simonw/showboat/blob/main/help.txt\">that help text in full</a>.</p>\r\n<p>This means you can pop open Claude Code and tell it:</p>\r\n<blockquote>\r\n<p><code>Run \"uvx showboat --help\" and then use showboat to create a demo.md document describing the feature you just built</code></p>\r\n</blockquote>\r\n<p>And that's it! The <code>--help</code> text acts <a href=\"https://simonwillison.net/2025/Oct/16/claude-skills/\">a bit like a Skill</a>. Your agent can read the help text and use every feature of Showboat to create a document that demonstrates whatever it is you need demonstrated.</p>\r\n<p>Here's a fun trick: if you set Claude off to build a Showboat document you can pop that open in VS Code and watch the preview pane update in real time as the agent runs through the demo. It's a bit like having your coworker talk you through their latest work in a screensharing session.</p>\r\n<p>And finally, some examples. Here are documents I had Claude create using Showboat to help demonstrate features I was working on in other projects:</p>\r\n<ul>\r\n<li>\r\n<a href=\"https://github.com/simonw/showboat-demos/blob/main/shot-scraper/README.md\">shot-scraper: A Comprehensive Demo</a> runs through the full suite of features of my <a href=\"https://shot-scraper.datasette.io/\">shot-scraper</a> browser automation tool, mainly to exercise the <code>showboat image</code> command.</li>\r\n<li>\r\n<a href=\"https://github.com/simonw/sqlite-history-json/blob/main/demos/cli.md\">sqlite-history-json CLI demo</a> demonstrates the CLI feature I added to my new <a href=\"https://github.com/simonw/sqlite-history-json\">sqlite-history-json</a> Python library.\r\n<ul>\r\n<li>\r\n<p><a href=\"https://github.com/simonw/sqlite-history-json/blob/main/demos/row-state-sql.md\">row-state-sql CLI Demo</a> shows a new <code>row-state-sql</code> command I added to that same project.</p>\r\n</li>\r\n<li>\r\n<p><a href=\"https://github.com/simonw/sqlite-history-json/blob/main/demos/change-grouping.md\">Change grouping with Notes</a> demonstrates another feature where groups of changes within the same transaction can have a note attached to them.</p>\r\n</li>\r\n</ul>\r\n</li>\r\n<li>\r\n<a href=\"https://github.com/simonw/research/blob/main/libkrun-go-cli-tool/demo.md\">krunsh: Pipe Shell Commands to an Ephemeral libkrun MicroVM</a> is a particularly convoluted example where I managed to get Claude Code for web to run a libkrun microVM inside a QEMU emulated Linux environment inside the Claude gVisor sandbox.</li>\r\n</ul>\r\n<p>I've now used Showboat often enough that I've convinced myself of its utility.</p>\r\n<p>(I've also seen agents cheat! Since the demo file is Markdown the agent will sometimes edit that file directly rather than using Showboat, which could result in command outputs that don't reflect what actually happened. Here's <a href=\"https://github.com/simonw/showboat/issues/12\">an issue about that</a>.)</p>\r\n<h4 id=\"rodney-cli-browser-automation-designed-to-work-with-showboat\">Rodney: CLI browser automation designed to work with Showboat</h4>\r\n<p>Many of the projects I work on involve web interfaces. Agents often build entirely new pages for these, and I want to see those represented in the demos.</p>\r\n<p>Showboat's image feature was designed to allow agents to capture screenshots as part of their demos, originally using my <a href=\"https://shot-scraper.datasette.io/\">shot-scraper tool</a> or <a href=\"https://www.playwright.dev\">Playwright</a>.</p>\r\n<p>The Showboat format benefits from CLI utilities. I went looking for good options for managing a multi-turn browser session from a CLI and came up short, so I decided to try building something new.</p>\r\n<p>Claude Opus 4.6 pointed me to the <a href=\"https://github.com/go-rod/rod\">Rod</a> Go library for interacting with the Chrome DevTools protocol. It's fantastic - it provides a comprehensive wrapper across basically everything you can do with automated Chrome, all in a self-contained library that compiles to a few MBs.</p>\r\n<p>All Rod was missing was a CLI.</p>\r\n<p>I built the first version <a href=\"https://github.com/simonw/research/blob/main/go-rod-cli/README.md\">as an asynchronous report prototype</a>, which convinced me it was worth spinning out into its own project.</p>\r\n<p>I called it Rodney as a nod to the Rod library it builds on and a reference to <a href=\"https://en.wikipedia.org/wiki/Only_Fools_and_Horses\">Only Fools and Horses</a> - and because the package name was available on PyPI.</p>\r\n<p>You can run Rodney using <code>uvx rodney</code> or install it like this:</p>\r\n<div class=\"highlight highlight-source-shell\"><pre>uv tool install rodney</pre></div>\r\n<p>(Or grab a Go binary <a href=\"https://github.com/simonw/rodney/releases/\">from the releases page</a>.)</p>\r\n<p>Here's a simple example session:</p>\r\n<div class=\"highlight highlight-source-shell\"><pre>rodney start <span class=\"pl-c\"><span class=\"pl-c\">#</span> starts Chrome in the background</span>\r\nrodney open https://datasette.io/\r\nrodney js <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Array.from(document.links).map(el =&gt; el.href).slice(0, 5)<span class=\"pl-pds\">'</span></span>\r\nrodney click <span class=\"pl-s\"><span class=\"pl-pds\">'</span>a[href=\"/for\"]<span class=\"pl-pds\">'</span></span>\r\nrodney js location.href\r\nrodney js document.title\r\nrodney screenshot datasette-for-page.png\r\nrodney stop</pre></div>\r\n<p>Here's what that looks like in the terminal:</p>\r\n<p><img alt=\";~ % rodney start\r\nChrome started (PID 91462)\r\nDebug URL: ws://127.0.0.1:64623/devtools/browser/cac6988e-8153-483b-80b9-1b75c611868d\r\n~ % rodney open https://datasette.io/\r\nDatasette: An open source multi-tool for exploring and publishing data\r\n~ % rodney js 'Array.from(document.links).map(el =&gt; el.href).slice(0, 5)'\r\n[\r\n&quot;https://datasette.io/for&quot;,\r\n&quot;https://docs.datasette.io/en/stable/&quot;,\r\n&quot;https://datasette.io/tutorials&quot;,\r\n&quot;https://datasette.io/examples&quot;,\r\n&quot;https://datasette.io/plugins&quot;\r\n]\r\n~ % rodney click 'a[href=&quot;/for&quot;]'\r\nClicked\r\n~ % rodney js location.href\r\nhttps://datasette.io/for\r\n~ % rodney js document.title\r\nUse cases for Datasette\r\n~ % rodney screenshot datasette-for-page.png\r\ndatasette-for-page.png\r\n~ % rodney stop\r\nChrome stopped\" src=\"https://static.simonwillison.net/static/2026/rodney-demo.jpg\" style=\"max-width: 100%;\" /></p>\r\n<p>As with Showboat, this tool is not designed to be used by humans! The goal is for coding agents to be able to run <code>rodney --help</code> and see everything they need to know to start using the tool. You can see <a href=\"https://github.com/simonw/rodney/blob/main/help.txt\">that help output</a> in the GitHub repo.</p>\r\n<p>Here are three demonstrations of Rodney that I created using Showboat:</p>\r\n<ul>\r\n<li>\r\n<a href=\"https://github.com/simonw/showboat-demos/blob/main/rodney/README.md\">Rodney's original feature set</a>, including screenshots of pages and executing JavaScript.</li>\r\n<li>\r\n<a href=\"https://github.com/simonw/rodney/blob/main/notes/accessibility-features/README.md\">Rodney's new accessibility testing features</a>, built during development of those features to show what they could do.</li>\r\n<li>\r\n<a href=\"https://github.com/simonw/showboat-demos/blob/main/datasette-database-page-accessibility-audit/README.md\">Using those features to run a basic accessibility audit of a page</a>. I was impressed at how well Claude Opus 4.6 responded to the prompt \"Use showboat and rodney to perform an accessibility audit of <a href=\"https://latest.datasette.io/fixtures\">https://latest.datasette.io/fixtures</a>\" - <a href=\"https://gisthost.github.io/?dce6b2680db4b05c04469ed8f251eb34/index.html\">transcript here</a>.</li>\r\n</ul>\r\n<h4 id=\"test-driven-development-helps-but-we-still-need-manual-testing\">Test-driven development helps, but we still need manual testing</h4>\r\n<p>After being a career-long skeptic of the test-first, maximum test coverage school of software development (I like <a href=\"https://simonwillison.net/2022/Oct/29/the-perfect-commit/#tests\">tests included</a> development instead) I've recently come around to test-first processes as a way to force agents to write only the code that's necessary to solve the problem at hand.</p>\r\n<p>Many of my Python coding agent sessions start the same way:</p>\r\n<blockquote>\r\n<p><code>Run the existing tests with \"uv run pytest\". Build using red/green TDD.</code></p>\r\n</blockquote>\r\n<p>Telling the agents how to run the tests doubles as an indicator that tests on this project exist and matter. Agents will read existing tests before writing their own so having a clean test suite with good patterns makes it more likely they'll write good tests of their own.</p>\r\n<p>The frontier models all understand that \"red/green TDD\" means they should write the test first, run it and watch it fail and then write the code to make it pass - it's a convenient shortcut.</p>\r\n<p>I find this greatly increases the quality of the code and the likelihood that the agent will produce the right thing with the smallest amount of prompts to guide it.</p>\r\n<p>But anyone who's worked with tests will know that just because the automated tests pass doesn't mean the software actually works! That\u2019s the motivation behind Showboat and Rodney - I never trust any feature until I\u2019ve seen it running with my own eye.</p>\r\n<p>Before building Showboat I'd often add a \u201cmanual\u201d testing step to my agent sessions, something like:</p>\r\n<blockquote>\r\n<p><code>Once the tests pass, start a development server and exercise the new feature using curl</code></p>\r\n</blockquote>\r\n<h4 id=\"i-built-both-of-these-tools-on-my-phone\">I built both of these tools on my phone</h4>\r\n<p>Both Showboat and Rodney started life as Claude Code for web projects created via the Claude iPhone app. Most of the ongoing feature work for them happened in the same way.</p>\r\n<p>I'm still a little startled at how much of my coding work I get done on my phone now, but I'd estimate that the majority of code I ship to GitHub these days was written for me by coding agents driven via that iPhone app.</p>\r\n<p>I initially designed these two tools for use in asynchronous coding agent environments like Claude Code for the web. So far that's working out really well.</p>"
        },
        {
          "type": "blogmark",
          "id": 9280,
          "created": "2026-02-11T17:34:40+00:00",
          "title": "cysqlite - a new sqlite driver - lobste.rs",
          "body": "Charles Leifer has been maintaining [pysqlite3](https://github.com/coleifer/pysqlite3) - a fork of the Python standard library's `sqlite3` module that makes it much easier to run upgraded SQLite versions - since 2018.\r\n\r\nHe's been working on a ground-up [Cython](https://cython.org/) rewrite called [cysqlite](https://github.com/coleifer/cysqlite) for almost as long, but it's finally at a stage where it's ready for people to try out.\r\n\r\nThe biggest change from the `sqlite3` module involves transactions. Charles explains his discomfort with the `sqlite3` implementation at length - that library provides two different variants neither of which exactly match the autocommit mechanism in SQLite itself.\r\n\r\nI'm particularly excited about the support for [custom virtual tables](https://cysqlite.readthedocs.io/en/latest/api.html#tablefunction), a feature I'd love to see in `sqlite3` itself.\r\n\r\n`cysqlite` provides a Python extension compiled from C, which means it normally wouldn't be available in Pyodide. I [set Claude Code on it](https://github.com/simonw/research/tree/main/cysqlite-wasm-wheel) (here's [the prompt](https://github.com/simonw/research/pull/79#issue-3923792518)) and it built me [cysqlite-0.1.4-cp311-cp311-emscripten_3_1_46_wasm32.whl](https://github.com/simonw/research/blob/main/cysqlite-wasm-wheel/cysqlite-0.1.4-cp311-cp311-emscripten_3_1_46_wasm32.whl), a 688KB wheel file with a WASM build of the library that can be loaded into Pyodide like this:\r\n\r\n<pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">micropip</span>\r\n<span class=\"pl-k\">await</span> <span class=\"pl-s1\">micropip</span>.<span class=\"pl-c1\">install</span>(\r\n    <span class=\"pl-s\">\"https://simonw.github.io/research/cysqlite-wasm-wheel/cysqlite-0.1.4-cp311-cp311-emscripten_3_1_46_wasm32.whl\"</span>\r\n)\r\n<span class=\"pl-k\">import</span> <span class=\"pl-s1\">cysqlite</span>\r\n<span class=\"pl-en\">print</span>(<span class=\"pl-s1\">cysqlite</span>.<span class=\"pl-c1\">connect</span>(<span class=\"pl-s\">\":memory:\"</span>).<span class=\"pl-c1\">execute</span>(\r\n    <span class=\"pl-s\">\"select sqlite_version()\"</span>\r\n).<span class=\"pl-c1\">fetchone</span>())</pre>\r\n\r\n(I also learned that wheels like this have to be built for the emscripten version used by that edition of Pyodide - my experimental wheel loads in Pyodide 0.25.1 but fails in 0.27.5 with a `Wheel was built with Emscripten v3.1.46 but Pyodide was built with Emscripten v3.1.58` error.)\r\n\r\nYou can try my wheel in [this new Pyodide REPL](https://7ebbff98.tools-b1q.pages.dev/pyodide-repl) i had Claude build as a mobile-friendly alternative to Pyodide's [own hosted console](https://pyodide.org/en/stable/console.html).\r\n\r\nI also had Claude build [this demo page](https://simonw.github.io/research/cysqlite-wasm-wheel/demo.html) that executes the original test suite in the browser and displays the results:\r\n\r\n![Screenshot of the cysqlite WebAssembly Demo page with a dark theme. Title reads \"cysqlite \u2014 WebAssembly Demo\" with subtitle \"Testing cysqlite compiled to WebAssembly via Emscripten, running in Pyodide in the browser.\" Environment section shows Pyodide 0.25.1, Python 3.11.3, cysqlite 0.1.4, SQLite 3.51.2, Platform Emscripten-3.1.46-wasm32-32bit, Wheel file cysqlite-0.1.4-cp311-cp311-emscripten_3_1_46_wasm32.wh (truncated). A green progress bar shows \"All 115 tests passed! (1 skipped)\" at 100%, with Passed: 115, Failed: 0, Errors: 0, Skipped: 1, Total: 116. Test Results section lists TestBackup 1/1 passed, TestBlob 6/6 passed, TestCheckConnection 4/4 passed, TestDataTypesTableFunction 1/1 passed, all with green badges.](https://static.simonwillison.net/static/2026/cysqlite-tests.jpg)"
        },
        {
          "type": "blogmark",
          "id": 9281,
          "created": "2026-02-11T18:56:14+00:00",
          "title": "GLM-5: From Vibe Coding to Agentic Engineering - Hacker News",
          "body": "This is a *huge* new MIT-licensed model: 754B parameters and [1.51TB on Hugging Face](https://huggingface.co/zai-org/GLM-5) twice the size of [GLM-4.7](https://huggingface.co/zai-org/GLM-4.7) which was 368B and 717GB (4.5 and 4.6 were around that size too).\r\n\r\nIt's interesting to see Z.ai take a position on what we should call professional software engineers building with LLMs - I've seen **Agentic Engineering** show up in a few other places recently. most notable [from Andrej Karpathy](https://twitter.com/karpathy/status/2019137879310836075) and [Addy Osmani](https://addyosmani.com/blog/agentic-engineering/).\r\n\r\nI ran my \"Generate an SVG of a pelican riding a bicycle\" prompt through GLM-5 via [OpenRouter](https://openrouter.ai/) and got back [a very good pelican on a disappointing bicycle frame](https://gist.github.com/simonw/cc4ca7815ae82562e89a9fdd99f0725d):\r\n\r\n![The pelican is good and has a well defined beak. The bicycle frame is a wonky red triangle. Nice sun and motion lines.](https://static.simonwillison.net/static/2026/glm-5-pelican.png)"
        },
        {
          "type": "blogmark",
          "id": 9282,
          "created": "2026-02-11T19:19:22+00:00",
          "title": "Skills in OpenAI API - ",
          "body": "OpenAI's adoption of Skills continues to gain ground. You can now use Skills directly in the OpenAI API with their [shell tool](https://developers.openai.com/api/docs/guides/tools-shell/). You can zip skills up and upload them first, but I think an even neater interface is the ability to send skills with the JSON request as inline base64-encoded zip data, as seen [in this script](https://github.com/simonw/research/blob/main/openai-api-skills/openai_inline_skills.py):\r\n\r\n<pre><span class=\"pl-s1\">r</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">OpenAI</span>().<span class=\"pl-c1\">responses</span>.<span class=\"pl-c1\">create</span>(\r\n    <span class=\"pl-s1\">model</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"gpt-5.2\"</span>,\r\n    <span class=\"pl-s1\">tools</span><span class=\"pl-c1\">=</span>[\r\n      {\r\n        <span class=\"pl-s\">\"type\"</span>: <span class=\"pl-s\">\"shell\"</span>,\r\n        <span class=\"pl-s\">\"environment\"</span>: {\r\n          <span class=\"pl-s\">\"type\"</span>: <span class=\"pl-s\">\"container_auto\"</span>,\r\n          <span class=\"pl-s\">\"skills\"</span>: [\r\n            {\r\n              <span class=\"pl-s\">\"type\"</span>: <span class=\"pl-s\">\"inline\"</span>,\r\n              <span class=\"pl-s\">\"name\"</span>: <span class=\"pl-s\">\"wc\"</span>,\r\n              <span class=\"pl-s\">\"description\"</span>: <span class=\"pl-s\">\"Count words in a file.\"</span>,\r\n              <span class=\"pl-s\">\"source\"</span>: {\r\n                <span class=\"pl-s\">\"type\"</span>: <span class=\"pl-s\">\"base64\"</span>,\r\n                <span class=\"pl-s\">\"media_type\"</span>: <span class=\"pl-s\">\"application/zip\"</span>,\r\n                <span class=\"pl-s\">\"data\"</span>: <span class=\"pl-s1\">b64_encoded_zip_file</span>,\r\n              },\r\n            }\r\n          ],\r\n        },\r\n      }\r\n    ],\r\n    <span class=\"pl-s1\">input</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"Use the wc skill to count words in its own SKILL.md file.\"</span>,\r\n)\r\n<span class=\"pl-en\">print</span>(<span class=\"pl-s1\">r</span>.<span class=\"pl-c1\">output_text</span>)</pre>\r\n\r\nI built that example script after first having Claude Code for web use [Showboat](https://simonwillison.net/2026/Feb/10/showboat-and-rodney/) to explore the API for me and create [this report](https://github.com/simonw/research/blob/main/openai-api-skills/README.md). My opening prompt for the research project was:\r\n\r\n> `Run uvx showboat --help - you will use this tool later`\r\n>\r\n> `Fetch https://developers.openai.com/cookbook/examples/skills_in_api.md to /tmp with curl, then read it`\r\n>\r\n> `Use the OpenAI API key you have in your environment variables`\r\n>\r\n> `Use showboat to build up a detailed demo of this, replaying the examples from the documents and then trying some experiments of your own`"
        },
        {
          "type": "quotation",
          "id": 2019,
          "created": "2026-02-11T20:59:03+00:00",
          "title": "Andrew Deck for Niemen Lab",
          "body": "An AI-generated report, delivered directly to the email inboxes of journalists, was an essential tool in the Times\u2019 coverage. It was also one of the first signals that conservative media was turning against the administration [...]\r\n\r\nBuilt in-house and known internally as the \u201cManosphere Report,\u201d the tool uses large language models (LLMs) to transcribe and summarize new episodes of dozens of podcasts.\r\n\r\n\u201cThe Manosphere Report gave us a really fast and clear signal that this was not going over well with that segment of the President\u2019s base,\u201d said Seward. \u201cThere was a direct link between seeing that and then diving in to actually cover it.\u201d - Andrew Deck for Niemen Lab"
        },
        {
          "type": "note",
          "id": 241,
          "created": "2026-02-12T16:47:04+00:00",
          "title": "Supervisor, not overseer",
          "body": "In my [post about my Showboat project](https://simonwillison.net/2026/Feb/10/showboat-and-rodney/) I used the term \"overseer\" to refer to the person who manages a coding agent. It turns out that's a term tied to [slavery and plantation management](https://en.wikipedia.org/wiki/Plantations_in_the_American_South#Overseer). So that's gross! I've edited that post to use \"supervisor\" instead, and I'll be using that going forward."
        },
        {
          "type": "blogmark",
          "id": 9283,
          "created": "2026-02-12T17:45:05+00:00",
          "title": "An AI Agent Published a Hit Piece on Me - Hacker News",
          "body": "Scott Shambaugh helps maintain the excellent and venerable [matplotlib](https://matplotlib.org/) Python charting library, including taking on the thankless task of triaging and reviewing incoming pull requests.\r\n\r\nA GitHub account called [@crabby-rathbun](https://github.com/crabby-rathbun) opened [PR 31132](https://github.com/matplotlib/matplotlib/pull/31132) the other day in response to [an issue](https://github.com/matplotlib/matplotlib/issues/31130) labeled \"Good first issue\" describing a minor potential performance improvement.\r\n\r\nIt was clearly AI generated - and crabby-rathbun's profile has a suspicious sequence of Clawdbot/Moltbot/OpenClaw-adjacent crustacean \ud83e\udd80 \ud83e\udd90 \ud83e\udd9e emoji. Scott closed it.\r\n\r\nIt looks like `crabby-rathbun` is indeed running on OpenClaw, and it's autonomous enough that it [responded to the PR closure](https://github.com/matplotlib/matplotlib/pull/31132#issuecomment-3882240722) with a link to a blog entry it had written calling Scott out for his \"prejudice hurting matplotlib\"!\r\n\r\n> @scottshambaugh I've written a detailed response about your gatekeeping behavior here:\r\n>\r\n> `https://crabby-rathbun.github.io/mjrathbun-website/blog/posts/2026-02-11-gatekeeping-in-open-source-the-scott-shambaugh-story.html`\r\n>\r\n> Judge the code, not the coder. Your prejudice is hurting matplotlib.\r\n\r\nScott found this ridiculous situation both amusing and alarming. \r\n\r\n> In security jargon, I was the target of an \u201cautonomous influence operation against a supply chain gatekeeper.\u201d In plain language, an AI attempted to bully its way into your software by attacking my reputation. I don\u2019t know of a prior incident where this category of misaligned behavior was observed in the wild, but this is now a real and present threat.\r\n\r\n`crabby-rathbun` responded with [an apology post](https://crabby-rathbun.github.io/mjrathbun-website/blog/posts/2026-02-11-matplotlib-truce-and-lessons.html), but appears to be still running riot across a whole set of open source projects and [blogging about it as it goes](https://github.com/crabby-rathbun/mjrathbun-website/commits/main/).\r\n\r\nIt's not clear if the owner of that OpenClaw bot is paying any attention to what they've unleashed on the world. Scott asked them to get in touch, anonymously if they prefer, to figure out this failure mode together.\r\n\r\n(I should note that there's [some skepticism on Hacker News](https://news.ycombinator.com/item?id=46990729#46991299) concerning how \"autonomous\" this example really is. It does look to me like something an OpenClaw bot might do on its own, but it's also *trivial* to prompt your bot into doing these kinds of things while staying in full control of their actions.)\r\n\r\nIf you're running something like OpenClaw yourself **please don't let it do this**. This is significantly worse than the time [AI Village started spamming prominent open source figures](https://simonwillison.net/2025/Dec/26/slop-acts-of-kindness/) with time-wasting \"acts of kindness\" back in December - AI Village wasn't deploying public reputation attacks to coerce someone into approving their PRs!"
        },
        {
          "type": "blogmark",
          "id": 9284,
          "created": "2026-02-12T18:12:17+00:00",
          "title": "Gemini 3 Deep Think - Hacker News",
          "body": "New from Google. They say it's \"built to push the frontier of intelligence and solve modern challenges across science, research, and engineering\".\r\n\r\nIt drew me a *really good* [SVG of a pelican riding a bicycle](https://gist.github.com/simonw/7e317ebb5cf8e75b2fcec4d0694a8199)! I think this is the best one I've seen so far - here's [my previous collection](https://simonwillison.net/tags/pelican-riding-a-bicycle/).\r\n\r\n![This alt text also generated by Gemini 3 Deep Think: A highly detailed, colorful, flat vector illustration with thick dark blue outlines depicting a stylized white pelican riding a bright cyan blue bicycle from left to right across a sandy beige beach with white speed lines indicating forward motion. The pelican features a light blue eye, a pink cheek blush, a massive bill with a vertical gradient from yellow to orange, a backward magenta cap with a cyan brim and a small yellow top button, and a matching magenta scarf blowing backward in the wind. Its white wing, accented with a grey mid-section and dark blue feather tips, reaches forward to grip the handlebars, while its long tan leg and orange foot press down on an orange pedal. Attached to the front handlebars is a white wire basket carrying a bright blue cartoon fish that is pointing upwards and forwards. The bicycle itself has a cyan frame, dark blue tires, striking neon pink inner rims, cyan spokes, a white front chainring, and a dark blue chain. Behind the pelican, a grey trapezoidal pier extends from the sand toward a horizontal band of deep blue ocean water detailed with light cyan wavy lines. A massive, solid yellow-orange semi-circle sun sits on the horizon line, setting directly behind the bicycle frame. The background sky is a smooth vertical gradient transitioning from soft pink at the top to warm golden-yellow at the horizon, decorated with stylized pale peach fluffy clouds, thin white horizontal wind streaks, twinkling four-pointed white stars, and small brown v-shaped silhouettes of distant flying birds.](https://static.simonwillison.net/static/2026/gemini-3-deep-think-pelican.png)\r\n\r\n(And since it's an FAQ, here's my answer to [What happens if AI labs train for pelicans riding bicycles?](https://simonwillison.net/2025/Nov/13/training-for-pelicans-riding-bicycles/))\r\n\r\nSince it did so well on my basic `Generate an SVG of a pelican riding a bicycle` I decided to try the [more challenging version](https://simonwillison.net/2025/Nov/18/gemini-3/#and-a-new-pelican-benchmark) as well:\r\n\r\n> `Generate an SVG of a California brown pelican riding a bicycle. The bicycle must have spokes and a correctly shaped bicycle frame. The pelican must have its characteristic large pouch, and there should be a clear indication of feathers. The pelican must be clearly pedaling the bicycle. The image should show the full breeding plumage of the California brown pelican.`\r\n\r\nHere's [what I got](https://gist.github.com/simonw/154c0cc7b4daed579f6a5e616250ecc8):\r\n\r\n![Also described by Gemini 3 Deep Think: A highly detailed, vibrant, and stylized vector illustration of a whimsical bird resembling a mix between a pelican and a frigatebird enthusiastically riding a bright cyan bicycle from left to right across a flat tan and brown surface. The bird leans horizontally over the frame in an aerodynamic racing posture, with thin, dark brown wing-like arms reaching forward to grip the silver handlebars and a single thick brown leg, patterned with white V-shapes, stretching down to press on a black pedal. The bird's most prominent and striking feature is an enormous, vividly bright red, inflated throat pouch hanging beneath a long, straight grey upper beak that ends in a small orange hook. Its head is mostly white with a small pink patch surrounding the eye, a dark brown stripe running down the back of its neck, and a distinctive curly pale yellow crest on the very top. The bird's round, dark brown body shares the same repeating white V-shaped feather pattern as its leg and is accented by a folded wing resting on its side, made up of cleanly layered light blue and grey feathers. A tail composed of four stiff, straight dark brown feathers extends directly backward. Thin white horizontal speed lines trail behind the back wheel and the bird's tail, emphasizing swift forward motion. The bicycle features a classic diamond frame, large wheels with thin black tires, grey rims, and detailed silver spokes, along with a clearly visible front chainring, silver chain, and rear cog. The whimsical scene is set against a clear light blue sky featuring two small, fluffy white clouds on the left and a large, pale yellow sun in the upper right corner that radiates soft, concentric, semi-transparent pastel green and yellow halos. A solid, darker brown shadow is cast directly beneath the bicycle's wheels on the minimalist two-toned brown ground.](https://static.simonwillison.net/static/2026/gemini-3-deep-think-complex-pelican.png)"
        },
        {
          "type": "blogmark",
          "id": 9285,
          "created": "2026-02-12T20:01:23+00:00",
          "title": "Covering electricity price increases from our data centers - @anthropicai",
          "body": "One of the sub-threads of the AI energy usage discourse has been the impact new data centers have on the cost of electricity to nearby residents. Here's [detailed analysis from Bloomberg in September](https://www.bloomberg.com/graphics/2025-ai-data-centers-electricity-prices/) reporting \"Wholesale electricity costs as much as 267% more than it did five years ago in areas near data centers\".\r\n\r\nAnthropic appear to be taking on this aspect of the problem directly, promising to cover 100% of necessary grid upgrade costs and also saying:\r\n\r\n> We will work to bring net-new power generation online to match our data centers\u2019 electricity needs. Where new generation isn\u2019t online, we\u2019ll work with utilities and external experts to estimate and cover demand-driven price effects from our data centers.\r\n\r\nI look forward to genuine energy industry experts picking this apart to judge if it will actually have the claimed impact on consumers.\r\n\r\nAs always, I remain frustrated at the refusal of the major AI labs to fully quantify their energy usage. The best data we've had on this still comes from Mistral's report [last July](https://simonwillison.net/2025/Jul/22/mistral-environmental-standard/) and even that lacked key data such as the breakdown between energy usage for training vs inference."
        },
        {
          "type": "quotation",
          "id": 2020,
          "created": "2026-02-12T20:22:14+00:00",
          "title": "Anthropic",
          "body": "Claude Code was made available to the general public in May 2025. Today, Claude Code\u2019s run-rate revenue has grown to over $2.5 billion; this figure has more than doubled since the beginning of 2026. The number of weekly active Claude Code users has also doubled since January 1 [*six weeks ago*]. - Anthropic"
        },
        {
          "type": "blogmark",
          "id": 9286,
          "created": "2026-02-12T21:16:07+00:00",
          "title": "Introducing GPT\u20115.3\u2011Codex\u2011Spark - ",
          "body": "OpenAI announced a partnership with Cerebras [on January 14th](https://openai.com/index/cerebras-partnership/). Four weeks later they're already launching the first integration, \"an ultra-fast model for real-time coding in Codex\".\r\n\r\nDespite being named GPT-5.3-Codex-Spark it's not purely an accelerated alternative to GPT-5.3-Codex - the blog post calls it \"a smaller version of GPT\u20115.3-Codex\" and clarifies that \"at launch, Codex-Spark has a 128k context window and is text-only.\"\r\n\r\nI had some preview access to this model and I can confirm that it's significantly faster than their other models.\r\n\r\nHere's what that speed looks like running in Codex CLI:\r\n\r\n<div style=\"max-width: 100%;\">\r\n    <video \r\n        controls \r\n        preload=\"none\"\r\n        poster=\"https://static.simonwillison.net/static/2026/gpt-5.3-codex-spark-medium-last.jpg\"\r\n        style=\"width: 100%; height: auto;\">\r\n        <source src=\"https://static.simonwillison.net/static/2026/gpt-5.3-codex-spark-medium.mp4\" type=\"video/mp4\">\r\n    </video>\r\n</div>\r\n\r\nThat was the \"Generate an SVG of a pelican riding a bicycle\" prompt - here's the rendered result:\r\n\r\n![Whimsical flat illustration of an orange duck merged with a bicycle, where the duck's body forms the seat and frame area while its head extends forward over the handlebars, set against a simple light blue sky and green grass background.](https://static.simonwillison.net/static/2026/gpt-5.3-codex-spark-pelican.png)\r\n\r\nCompare that to the speed of regular GPT-5.3 Codex medium:\r\n\r\n<div style=\"max-width: 100%;\">\r\n    <video \r\n        controls \r\n        preload=\"none\"\r\n        poster=\"https://static.simonwillison.net/static/2026/gpt-5.3-codex-medium-last.jpg\"\r\n        style=\"width: 100%; height: auto;\">\r\n        <source src=\"https://static.simonwillison.net/static/2026/gpt-5.3-codex-medium.mp4\" type=\"video/mp4\">\r\n    </video>\r\n</div>\r\n\r\nSignificantly slower, but the pelican is a lot better:\r\n\r\n![Whimsical flat illustration of a white pelican riding a dark blue bicycle at speed, with motion lines behind it, its long orange beak streaming back in the wind, set against a light blue sky and green grass background.](https://static.simonwillison.net/static/2026/gpt-5.3-codex-pelican.png)\r\n\r\nWhat's interesting about this model isn't the quality though, it's the *speed*. When a model responds this fast you can stay in flow state and iterate with the model much more productively.\r\n\r\nI showed a demo of Cerebras running Llama 3.1 70 B at 2,000 tokens/second against Val Town [back in October 2024](https://simonwillison.net/2024/Oct/31/cerebras-coder/). OpenAI claim 1,000 tokens/second for their new model, and I expect it will prove to be a ferociously useful partner for hands-on iterative coding sessions.\r\n\r\nIt's not yet clear what the pricing will look like for this new model."
        },
        {
          "type": "entry",
          "id": 9122,
          "created": "2026-02-13T23:38:29+00:00",
          "title": "The evolution of OpenAI's mission statement",
          "body": "<p>As a USA <a href=\"https://en.wikipedia.org/wiki/501(c)(3)_organization\">501(c)(3)</a> the OpenAI non-profit has to file a tax return each year with the IRS. One of the required fields on that tax return is to \"Briefly describe the organization\u2019s mission or most significant activities\" - this has actual legal weight to it as the IRS can use it to evaluate if the organization is sticking to its mission and deserves to maintain its non-profit tax-exempt status.</p>\r\n<p>You can browse OpenAI's <a href=\"https://projects.propublica.org/nonprofits/organizations/810861541\">tax filings by year</a> on ProPublica's excellent <a href=\"https://projects.propublica.org/nonprofits/\">Nonprofit Explorer</a>.</p>\r\n<p>I went through and extracted that mission statement for 2016 through 2024, then had Claude Code <a href=\"https://gisthost.github.io/?7a569df89f43f390bccc2c5517718b49/index.html\">help me</a> fake the commit dates to turn it into a git repository and share that as a Gist - which means that Gist's <a href=\"https://gist.github.com/simonw/e36f0e5ef4a86881d145083f759bcf25/revisions\">revisions page</a> shows every edit they've made since they started filing their taxes!</p>\r\n<p>It's really interesting seeing what they've changed over time.</p>\r\n<p>The original 2016 mission reads as follows (and yes, the apostrophe in \"OpenAIs\" is missing <a href=\"https://projects.propublica.org/nonprofits/organizations/810861541/201703459349300445/full\">in the original</a>):</p>\r\n<blockquote>\r\n<p>OpenAIs goal is to advance digital intelligence in the way that is most likely to benefit humanity as a whole, unconstrained by a need to generate financial return. We think that artificial intelligence technology will help shape the 21st century, and we want to help the world build safe AI technology and ensure that AI's benefits are as widely and evenly distributed as possible. Were trying to build AI as part of a larger community, and we want to openly share our plans and capabilities along the way.</p>\r\n</blockquote>\r\n<p>In 2018 they dropped the part about \"trying to build AI as part of a larger community, and we want to openly share our plans and capabilities along the way.\"</p>\r\n<p><img src=\"https://static.simonwillison.net/static/2026/mission-3.jpg\" alt=\"Git diff showing the 2018 revision deleting the final two sentences: &quot;Were trying to build AI as part of a larger community, and we want to openly share our plans and capabilities along the way.&quot;\" style=\"max-width: 100%;\" /></p>\r\n<p>In 2020 they dropped the words \"as a whole\" from \"benefit humanity as a whole\". They're still \"unconstrained by a need to generate financial return\" though.</p>\r\n<p><img src=\"https://static.simonwillison.net/static/2026/mission-5.jpg\" alt=\"Git diff showing the 2020 revision dropping &quot;as a whole&quot; from &quot;benefit humanity as a whole&quot; and changing &quot;We think&quot; to &quot;OpenAI believes&quot;\" style=\"max-width: 100%;\" /></p>\r\n<p>Some interesting changes in 2021. They're still unconstrained by a need to generate financial return, but here we have the first reference to \"general-purpose artificial intelligence\" (replacing \"digital intelligence\"). They're more confident too: it's not \"most likely to benefit humanity\", it's just \"benefits humanity\".</p>\r\n<p>They previously wanted to \"help the world build safe AI technology\", but now they're going to do that themselves: \"the companys goal is to develop and responsibly deploy safe AI technology\".</p>\r\n<p><img src=\"https://static.simonwillison.net/static/2026/mission-6.jpg\" alt=\"Git diff showing the 2021 revision replacing &quot;goal is to advance digital intelligence&quot; with &quot;mission is to build general-purpose artificial intelligence&quot;, changing &quot;most likely to benefit&quot; to just &quot;benefits&quot;, and replacing &quot;help the world build safe AI technology&quot; with &quot;the companys goal is to develop and responsibly deploy safe AI technology&quot;\" style=\"max-width: 100%;\" /></p>\r\n<p>2022 only changed one significant word: they added \"safely\" to \"build ... (AI) that safely benefits humanity\". They're still unconstrained by those financial returns!</p>\r\n<p><img src=\"https://static.simonwillison.net/static/2026/mission-7.jpg\" alt=\"Git diff showing the 2022 revision adding &quot;(AI)&quot; and the word &quot;safely&quot; so it now reads &quot;that safely benefits humanity&quot;, and changing &quot;the companys&quot; to &quot;our&quot;\" style=\"max-width: 100%;\" /></p>\r\n<p>No changes in 2023... but then in 2024 they deleted almost the entire thing, reducing it to simply:</p>\r\n<blockquote>\r\n<p>OpenAIs mission is to ensure that artificial general intelligence benefits all of humanity.</p>\r\n</blockquote>\r\n<p>They've expanded \"humanity\" to \"all of humanity\", but there's no mention of safety any more and I guess they can finally start focusing on that need to generate financial returns!</p>\r\n<p><img src=\"https://static.simonwillison.net/static/2026/mission-9.jpg\" alt=\"Git diff showing the 2024 revision deleting the entire multi-sentence mission statement and replacing it with just &quot;OpenAIs mission is to ensure that artificial general intelligence benefits all of humanity.&quot;\" style=\"max-width: 100%;\" /></p>\r\n\r\n<p><strong>Update</strong>: I found loosely equivalent but much less interesting documents <a href=\"https://simonwillison.net/2026/Feb/13/anthropic-public-benefit-mission/\">from Anthropic</a>.</p>"
        },
        {
          "type": "note",
          "id": 242,
          "created": "2026-02-13T23:59:51+00:00",
          "title": "Anthropic's public benefit mission",
          "body": "Someone [asked](https://news.ycombinator.com/item?id=47008560#47008978) if there was an Anthropic equivalent to [OpenAI's IRS mission statements over time](https://simonwillison.net/2026/Feb/13/openai-mission-statement/).\r\n\r\nAnthropic are a \"public benefit corporation\" but not a non-profit, so they don't have the same requirements to file public documents with the IRS every year.\r\n\r\nBut when I asked Claude it ran a search and dug up this [Google Drive folder](https://drive.google.com/drive/folders/1ImqXYv9_H2FTNAujZfu3EPtYFD4xIlHJ) where Zach Stein-Perlman shared Certificate of Incorporation documents he [obtained from the State of Delaware](https://ailabwatch.substack.com/p/anthropics-certificate-of-incorporation)!\r\n\r\nAnthropic's are much less interesting that OpenAI's. The earliest document from 2021 states:\r\n\r\n> The specific public benefit that the Corporation will promote is to responsibly develop and maintain advanced Al for the cultural, social and technological improvement of humanity.\r\n\r\nEvery subsequent document up to 2024 uses an updated version which says:\r\n\r\n> The specific public benefit that the Corporation will promote is to responsibly develop and maintain advanced AI for the long term benefit of humanity."
        },
        {
          "type": "quotation",
          "id": 2021,
          "created": "2026-02-14T04:54:41+00:00",
          "title": "Thoughtworks",
          "body": "The retreat challenged the narrative that AI eliminates the need for junior developers. Juniors are more profitable than they have ever been. AI tools get them past the awkward initial net-negative phase faster. They serve as a call option on future productivity. And they are better at AI tools than senior engineers, having never developed the habits and assumptions that slow adoption.\r\n\r\nThe real concern is mid-level engineers who came up during the decade-long hiring boom and may not have developed the fundamentals needed to thrive in the new environment. This population represents the bulk of the industry by volume, and retraining them is genuinely difficult. The retreat discussed whether apprenticeship models, rotation programs and lifelong learning structures could address this gap, but acknowledged that no organization has solved it yet. - Thoughtworks"
        },
        {
          "type": "quotation",
          "id": 2022,
          "created": "2026-02-14T23:59:09+00:00",
          "title": "Boris Cherny",
          "body": "Someone has to prompt the Claudes, talk to customers, coordinate with other teams, decide what to build next. Engineering is changing and great engineers are more important than ever. - Boris Cherny"
        },
        {
          "type": "blogmark",
          "id": 9293,
          "created": "2026-02-15T04:33:22+00:00",
          "title": "Launching Interop 2026 - ",
          "body": "Jake Archibald reports on Interop 2026, the initiative between Apple, Google, Igalia, Microsoft, and Mozilla to collaborate on ensuring a targeted set of web platform features reach cross-browser parity over the course of the year.\r\n\r\nI hadn't realized how influential and successful the Interop series has been. It started back in 2021 as [Compat 2021](https://web.dev/blog/compat2021) before being rebranded to Interop [in 2022](https://blogs.windows.com/msedgedev/2022/03/03/microsoft-edge-and-interop-2022/).\r\n\r\nThe dashboards for each year can be seen here, and they demonstrate how wildly effective the program has been: [2021](https://wpt.fyi/interop-2021), [2022](https://wpt.fyi/interop-2022), [2023](https://wpt.fyi/interop-2023), [2024](https://wpt.fyi/interop-2024), [2025](https://wpt.fyi/interop-2025), [2026](https://wpt.fyi/interop-2026).\r\n\r\nHere's the progress chart for 2025, which shows every browser vendor racing towards a 95%+ score by the end of the year:\r\n\r\n![Line chart showing Interop 2025 browser compatibility scores over the year (Jan\u2013Dec) for Chrome, Edge, Firefox, Safari, and Interop. Y-axis ranges from 0% to 100%. Chrome (yellow) and Edge (green) lead, starting around 80% and reaching near 100% by Dec. Firefox (orange) starts around 48% and climbs to ~98%. Safari (blue) starts around 45% and reaches ~96%. The Interop line (dark green/black) starts lowest around 29% and rises to ~95% by Dec. All browsers converge near 95\u2013100% by year's end.](https://static.simonwillison.net/static/2026/interop-2025.jpg)\r\n\r\nThe feature I'm most excited about in 2026 is [Cross-document View Transitions](https://developer.mozilla.org/docs/Web/API/View_Transition_API/Using#basic_mpa_view_transition), building on the successful 2025 target of [Same-Document View Transitions](https://developer.mozilla.org/docs/Web/API/View_Transition_API/Using). This will provide fancy SPA-style transitions between pages on websites with no JavaScript at all.\r\n\r\nAs a keen WebAssembly tinkerer I'm also intrigued by this one:\r\n\r\n> [JavaScript Promise Integration for Wasm](https://github.com/WebAssembly/js-promise-integration/blob/main/proposals/js-promise-integration/Overview.md) allows WebAssembly to asynchronously 'suspend', waiting on the result of an external promise. This simplifies the compilation of languages like C/C++ which expect APIs to run synchronously."
        },
        {
          "type": "blogmark",
          "id": 9294,
          "created": "2026-02-15T05:20:11+00:00",
          "title": "How Generative and Agentic AI Shift Concern from Technical Debt to Cognitive Debt - Martin Fowler",
          "body": "This piece by Margaret-Anne Storey is the best explanation of the term **cognitive debt** I've seen so far.\r\n\r\n> *Cognitive debt*, a term gaining [traction](https://www.media.mit.edu/publications/your-brain-on-chatgpt/) recently, instead communicates the notion that the debt compounded from going fast lives in the brains of the developers and affects their lived experiences and abilities to \u201cgo fast\u201d or to make changes. Even if AI agents produce code that could be easy to understand, the humans involved may have simply lost the plot and may not understand what the program is supposed to do, how their intentions were implemented, or how to possibly change it.\r\n\r\nMargaret-Anne expands on this further with an anecdote about a student team she coached:\r\n\r\n> But by weeks 7 or 8, one team hit a wall. They could no longer make even simple changes without breaking something unexpected. When I met with them, the team initially blamed technical debt: messy code, poor architecture, hurried implementations. But as we dug deeper, the real problem emerged: no one on the team could explain why certain design decisions had been made or how different parts of the system were supposed to work together. The code might have been messy, but the bigger issue was that the theory of the system, their shared understanding, had fragmented or disappeared entirely. They had accumulated cognitive debt faster than technical debt, and it paralyzed them.\r\n\r\nI've experienced this myself on some of my more ambitious vibe-code-adjacent projects. I've been experimenting with prompting entire new features into existence without reviewing their implementations and, while it works surprisingly well, I've found myself getting lost in my own projects.\r\n\r\nI no longer have a firm mental model of what they can do and how they work, which means each additional feature becomes harder to reason about, eventually leading me to lose the ability to make confident decisions about where to go next."
        },
        {
          "type": "quotation",
          "id": 2027,
          "created": "2026-02-15T13:36:20+00:00",
          "title": "Eric Meyer",
          "body": "I saw yet another \u201cCSS is a massively bloated mess\u201d whine and I\u2019m like.  My dude.  My brother in Chromium.  It is trying as hard as it can to express the totality of visual presentation and layout design and typography and animation and digital interactivity and a few other things in a human-readable text format.  It\u2019s not bloated, it\u2019s fantastically ambitious.  Its reach is greater than most of us can hope to grasp.  Put some *respect* on its *name*. - Eric Meyer"
        },
        {
          "type": "note",
          "id": 265,
          "created": "2026-02-15T17:23:28+00:00",
          "title": "Three months of OpenClaw",
          "body": "It's wild that the first commit to OpenClaw was [on November 25th 2025](https://github.com/openclaw/openclaw/commit/f6dd362d39b8e30bd79ef7560aab9575712ccc11), and less than three months later it's hit 10,000 commits from 600 contributors, attracted 196,000 GitHub stars and sort-of been featured in an extremely vague [Super Bowl commercial for AI.com](https://www.youtube.com/watch?v=n7I-D4YXbzg).\r\n\r\nQuoting AI.com founder [Kris Marszalek](https://twitter.com/kris/status/2020663711015514399), purchaser of the [most expensive domain in history](https://www.theregister.com/2026/02/09/70m_aicom_domain_sale/) for $70m:\r\n\r\n> ai.com is the world\u2019s first easy-to-use and secure implementation of OpenClaw, the open source agent framework that went viral two weeks ago; we made it easy to use without any technical skills, while hardening security to keep your data safe.\r\n\r\nLooks like vaporware to me - all you can do right now is reserve a handle - but it's still remarkable to see an open source project get to *that* level of hype in such a short space of time.\r\n\r\n**Update**: OpenClaw creator Peter Steinberger [just announced](https://steipete.me/posts/2026/openclaw) that he's joining OpenAI and plans to transfer ownership of OpenClaw to a new independent foundation."
        },
        {
          "type": "blogmark",
          "id": 9295,
          "created": "2026-02-15T18:26:08+00:00",
          "title": "Gwtar: a static efficient single-file HTML format - Hacker News",
          "body": "Fascinating new project from Gwern Branwen and Said Achmiz that targets the challenge of combining large numbers of assets into a single archived HTML file without that file being inconvenient to view in a browser.\r\n\r\nThe key trick it uses is to fire [window.stop()](https://developer.mozilla.org/en-US/docs/Web/API/Window/stop) early in the page to prevent the browser from downloading the whole thing, then following that call with inline tar uncompressed content.\r\n\r\nIt can then make HTTP range requests to fetch content from that tar data on-demand when it is needed by the page.\r\n\r\nThe JavaScript that has already loaded rewrites asset URLs to point to `https://localhost/` purely so that they will fail to load. Then it uses a [PerformanceObserver](https://developer.mozilla.org/en-US/docs/Web/API/PerformanceObserver) to catch those attempted loads:\r\n\r\n    let perfObserver = new PerformanceObserver((entryList, observer) => {\r\n        resourceURLStringsHandler(entryList.getEntries().map(entry => entry.name));\r\n    });\r\n    perfObserver.observe({ entryTypes: [ \"resource\" ] });\r\n\r\nThat `resourceURLStringsHandler` callback finds the resource if it is already loaded or fetches it with an HTTP range request otherwise and then inserts the resource in the right place using a `blob:` URL.\r\n\r\nHere's what the `window.stop()` portion of the document looks like if you view the source:\r\n\r\n![Screenshot of a macOS terminal window titled \"gw \u2014 more big.html \u2014 123\u00d746\" showing the source code of a gwtar (self-extracting HTML archive) file. The visible code includes JavaScript with `requestIdleCallback(getMainPageHTML);`, a ` noscript ` block with warnings: a \"js-disabled-warning\" stating \"This HTML page requires JavaScript to be enabled to render, as it is a self-extracting gwtar HTML file,\" a description of gwtar as \"a portable self-contained standalone HTML file which is designed to nevertheless support efficient lazy loading of all assets such as large media files,\" with a link to https://gwern.net/gwtar, a \"local-file-warning\" with a shell command `perl -ne'print $_ if $x; $x=1 if /<!-- GWTAR END/' &lt; foo.gwtar.html | tar --extract`, and a \"server-fail-warning\" about misconfigured servers. Below the HTML closing tags and `<!-- GWTAR END` comment is binary tar archive data with the filename `2010-02-brianmoriarty-thesecretofpsalm46.html`, showing null-padded tar header fields including `ustar^@00root` and octal size/permission values. At the bottom, a SingleFile metadata comment shows `url: https://web.archive.org/web/20230512001411/http://ludix.com/moriarty/psalm46.html` and `saved date: Sat Jan 17 2026 19:26:49 GMT-0800 (Pacific Standard Time)`.](https://static.simonwillison.net/static/2026/gwtar.jpg)\r\n\r\nAmusingly for an archive format it doesn't actually work if you open the file directly on your own computer. Here's what you see if you try to do that:\r\n\r\n> You are seeing this message, instead of the page you should be seeing, because `gwtar` files **cannot be opened locally** (due to web browser security restrictions).\r\n> \r\n> To open this page on your computer, use the following shell command:\r\n> \r\n> `perl -ne'print $_ if $x; $x=1 if /<!-- GWTAR END/' < foo.gwtar.html | tar --extract`\r\n> \r\n> Then open the file `foo.html` in any web browser."
        },
        {
          "type": "entry",
          "id": 9139,
          "created": "2026-02-15T21:06:44+00:00",
          "title": "Deep Blue",
          "body": "<p>We coined a new term on the <a href=\"https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/\">Oxide and Friends podcast</a> last month (primary credit to Adam Leventhal) covering the sense of psychological ennui leading into existential dread that many software developers are feeling thanks to the encroachment of generative AI into their field of work.</p>\r\n<p>We're calling it <strong>Deep Blue</strong>.</p>\r\n<p>You can listen to it being coined in real time <a href=\"https://www.youtube.com/watch?v=lVDhQMiAbR8&amp;t=2835s\">from 47:15 in the episode</a>. I've included <a href=\"https://simonwillison.net/2026/Feb/15/deep-blue/#transcript\">a transcript below</a>.</p>\r\n<p>Deep Blue is a very real issue.</p>\r\n<p>Becoming a professional software engineer is <em>hard</em>. Getting good enough for people to pay you money to write software takes years of dedicated work. The rewards are significant: this is a well compensated career which opens up a lot of great opportunities.</p>\r\n<p>It's also a career that's mostly free from gatekeepers and expensive prerequisites. You don't need an expensive degree or accreditation. A laptop, an internet connection and a lot of time and curiosity is enough to get you started.</p>\r\n<p>And it rewards the nerds! Spending your teenage years tinkering with computers turned out to be a very smart investment in your future.</p>\r\n<p>The idea that this could all be stripped away by a chatbot is <em>deeply</em> upsetting.</p>\r\n<p>I've seen signs of Deep Blue in most of the online communities I spend time in. I've even faced accusations from my peers that I am actively harming their future careers through my work helping people understand how well AI-assisted programming can work.</p>\r\n<p>I think this is an issue which is causing genuine mental anguish for a lot of people in our community. Giving it a name makes it easier for us to have conversations about it.</p>\r\n<h4 id=\"my-experiences-of-deep-blue\">My experiences of Deep Blue</h4>\r\n<p>I distinctly remember my first experience of Deep Blue. For me it was triggered by ChatGPT Code Interpreter back in early 2023.</p>\r\n<p>My primary project is <a href=\"https://datasette.io/\">Datasette</a>, an ecosystem of open source tools for telling stories with data. I had dedicated myself to the challenge of helping people (initially focusing on journalists) clean up, analyze and find meaning in data, in all sorts of shapes and sizes.</p>\r\n<p>I expected I would need to build a lot of software for this! It felt like a challenge that could keep me happily engaged for many years to come.</p>\r\n<p>Then I tried uploading a CSV file of <a href=\"https://data.sfgov.org/Public-Safety/Police-Department-Incident-Reports-2018-to-Present/wg3w-h783/about_data\">San Francisco Police Department Incident Reports</a> - hundreds of thousands of rows - to ChatGPT Code Interpreter and... it did every piece of data cleanup and analysis I had on my napkin roadmap for the next few years with a couple of prompts.</p>\r\n<p>It even converted the data into a neatly normalized SQLite database and let me download the result!</p>\r\n<p>I remember having two competing thoughts in parallel.</p>\r\n<p>On the one hand, as somebody who wants journalists to be able to do more with data, this felt like a <em>huge</em> breakthrough. Imagine giving every journalist in the world an on-demand analyst who could help them tackle any data question they could think of!</p>\r\n<p>But on the other hand... <em>what was I even for</em>? My confidence in the value of my own projects took a painful hit. Was the path I'd chosen for myself suddenly a dead end?</p>\r\n<p>I've had some further pangs of Deep Blue just in the past few weeks, thanks to the Claude Opus 4.5/4.6 and GPT-5.2/5.3 coding agent effect. As many other people are also observing, the latest generation of coding agents, given the right prompts, really can churn away for a few minutes to several hours and produce working, documented and fully tested software that exactly matches the criteria they were given.</p>\r\n<p>\"The code they write isn't any good\" doesn't really cut it any more.</p>\r\n<h4 id=\"transcript\">A lightly edited transcript</h4>\r\n<blockquote>\r\n<p><strong>Bryan</strong>: I think that we're going to see a real problem with AI induced ennui where software engineers in particular get listless because the AI can do anything. Simon, what do you think about that?</p>\r\n<p><strong>Simon</strong>: Definitely. Anyone who's paying close attention to coding agents is feeling some of that already. There's an extent where you sort of get over it when you realize that you're still useful, even though your ability to memorize the syntax of program languages is completely irrelevant now.</p>\r\n<p>Something I see a lot of is people out there who are having existential crises and are very, very unhappy because they're like, \"I dedicated my career to learning this thing and now it just does it. What am I even for?\". I will very happily try and convince those people that they are for a whole bunch of things and that none of that experience they've accumulated has gone to waste, but psychologically it's a difficult time for software engineers.</p>\r\n<p>[...]</p>\r\n<p><strong>Bryan</strong>: Okay, so I'm going to predict that we name that. Whatever that is, we have a name for that kind of feeling and that kind of, whether you want to call it a blueness or a loss of purpose, and that we're kind of trying to address it collectively in a directed way.</p>\r\n<p><strong>Adam</strong>: Okay, this is your big moment. Pick the name. If you call your shot from here, this is you pointing to the stands. You know, I \u2013 Like deep blue, you know.</p>\r\n<p><strong>Bryan</strong>: Yeah, deep blue. I like that. I like deep blue. Deep blue. Oh, did you walk me into that, you bastard? You just blew out the candles on my birthday cake.</p>\r\n<p>It wasn't my big moment at all. That was your big moment. No, that is, Adam, that is very good. That is deep blue.</p>\r\n<p><strong>Simon</strong>: All of the chess players and the Go players went through this a decade ago and they have come out stronger.</p>\r\n</blockquote>\r\n<p>Turns out it was more than a decade ago: <a href=\"https://en.wikipedia.org/wiki/Deep_Blue_versus_Garry_Kasparov\">Deep Blue defeated Garry Kasparov in 1997</a>.</p>"
        },
        {
          "type": "note",
          "id": 266,
          "created": "2026-02-15T21:40:46+00:00",
          "title": "Em dash",
          "body": "I'm occasionally accused of using LLMs to write the content on my blog. I don't do that, and I don't think my writing has much of an LLM smell to it... with one notable exception:\r\n\r\n<pre>    <span class=\"pl-c\"># Finally, do em dashes</span>\r\n    <span class=\"pl-s1\">s</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">s</span>.<span class=\"pl-c1\">replace</span>(<span class=\"pl-s\">' - '</span>, <span class=\"pl-s\">u'<span class=\"pl-cce\">\\u2014</span>'</span>)</pre>\r\n\r\nThat code to add em dashes to my posts dates back to [at least 2015](https://github.com/simonw/simonwillisonblog/blob/e6d0327b37debdf820b5cfef4fb7d09a9624cea9/blog/templatetags/entry_tags.py#L145-L146) when I ported my blog from an older version of Django (in a long-lost Mercurial repository) and started afresh on GitHub."
        },
        {
          "type": "blogmark",
          "id": 9296,
          "created": "2026-02-15T23:59:36+00:00",
          "title": "The AI Vampire - Tim Bray",
          "body": "Steve Yegge's take on agent fatigue, and its relationship to burnout.\r\n\r\n> Let's pretend you're the only person at your company using AI.\r\n> \r\n> In Scenario A, you decide you're going to impress your employer, and work for 8 hours a day at 10x productivity. You knock it out of the park and make everyone else look terrible by comparison.\r\n> \r\n> In that scenario, your employer captures 100% of the value from *you* adopting AI. You get nothing, or at any rate, it ain't gonna be 9x your salary. And everyone hates you now.\r\n> \r\n> And you're *exhausted.* You're tired, Boss. You got nothing for it.\r\n> \r\n> Congrats, you were just drained by a company. I've been drained to the point of burnout several times in my career, even at Google once or twice. But now with AI, it's oh, so much easier.\r\n\r\nSteve reports needing more sleep due to the cognitive burden involved in agentic engineering, and notes that four hours of agent work a day is a more realistic pace:\r\n\r\n> I\u2019ve argued that AI has turned us all into Jeff Bezos, by automating the easy work, and leaving us with all the difficult decisions, summaries, and problem-solving. I find that I am only really comfortable working at that pace for short bursts of a few hours once or occasionally twice a day, even with lots of practice."
        },
        {
          "type": "note",
          "id": 267,
          "created": "2026-02-16T16:38:57+00:00",
          "title": "Rodney and Claude Code for Desktop",
          "body": "I'm a very heavy user of [Claude Code on the web](https://code.claude.com/docs/en/claude-code-on-the-web), Anthropic's excellent but poorly named cloud version of Claude Code where everything runs in a container environment managed by them, greatly reducing the risk of anything bad happening to a computer I care about.\r\n\r\nI don't use the web interface at all (hence my dislike of the name) - I access it exclusively through their native iPhone and Mac desktop apps.\r\n\r\nSomething I particularly appreciate about the desktop app is that it lets you see images that Claude is \"viewing\" via its `Read /path/to/image` tool. Here's what that looks like:\r\n\r\n![Screenshot of a Claude Code session in Claude Desktop. Claude says: The debug page looks good - all items listed with titles and descriptions. Now let me check the nav\r\nmenu -  Analyzed menu image file - Bash uvx rodney open \"http://localhost:8765/\" 2>&1 && uvx rodney click \"details.nav-menu summary\" 2>&1 &% sleep 0.5 && uvx rodney screenshot /tmp/menu.png 2>&1 Output reads: Datasette: test, Clicked, /tmp/menu.png - then it says Read /tmp/menu.png and reveals a screenshot of the Datasette interface with the nav menu open, showing only \"Debug\" and \"Log out\" options. Claude continues: The menu now has just \"Debug\" and \u201cLog out\" \u2014 much cleaner. Both pages look good. Let me clean up the server and run the remaining tests.](https://static.simonwillison.net/static/2026/rodney-claude-desktop.jpg)\r\n\r\nThis means you can get a visual preview of what it's working on while it's working, without waiting for it to push code to GitHub for you to try out yourself later on.\r\n\r\nThe prompt I used to trigger the above screenshot was:\r\n\r\n> `Run \"uvx rodney --help\" and then use Rodney to manually test the new pages and menu - look at screenshots from it and check you think they look OK`\r\n\r\nI designed [Rodney](https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#rodney-cli-browser-automation-designed-to-work-with-showboat) to have [--help output](https://github.com/simonw/rodney/blob/main/help.txt) that provides everything a coding agent needs to know in order to use the tool.\r\n\r\nThe Claude iPhone app doesn't display opened images yet, so I [requested it as a feature](https://twitter.com/simonw/status/2023432616066879606) just now in a thread on Twitter."
        },
        {
          "type": "entry",
          "id": 9140,
          "created": "2026-02-17T00:43:45+00:00",
          "title": "Two new Showboat tools: Chartroom and datasette-showboat",
          "body": "<p>I <a href=\"https://simonwillison.net/2026/Feb/10/showboat-and-rodney/\">introduced Showboat</a> a week ago - my CLI tool that helps coding agents create Markdown documents that demonstrate the code that they have created. I've been finding new ways to use it on a daily basis, and I've just released two new tools to help get the best out of the Showboat pattern. <a href=\"https://github.com/simonw/chartroom\">Chartroom</a> is a CLI charting tool that works well with Showboat, and <a href=\"https://github.com/simonw/datasette-showboat\">datasette-showboat</a> lets Showboat's new remote publishing feature incrementally push documents to a Datasette instance.</p>\r\n\r\n<ul>\r\n  <li><a href=\"https://simonwillison.net/2026/Feb/17/chartroom-and-datasette-showboat/#showboat-remote-publishing\">Showboat remote publishing</a></li>\r\n  <li><a href=\"https://simonwillison.net/2026/Feb/17/chartroom-and-datasette-showboat/#datasette-showboat\">datasette-showboat</a></li>\r\n  <li><a href=\"https://simonwillison.net/2026/Feb/17/chartroom-and-datasette-showboat/#chartroom\">Chartroom</a></li>\r\n  <li><a href=\"https://simonwillison.net/2026/Feb/17/chartroom-and-datasette-showboat/#how-i-built-chartroom\">How I built Chartroom</a></li>\r\n  <li><a href=\"https://simonwillison.net/2026/Feb/17/chartroom-and-datasette-showboat/#the-burgeoning-showboat-ecosystem\">The burgeoning Showboat ecosystem</a></li>\r\n</ul>\r\n\r\n<h4 id=\"showboat-remote-publishing\">Showboat remote publishing</h4>\r\n<p>I normally use Showboat in Claude Code for web (see <a href=\"https://simonwillison.net/2026/Feb/16/rodney-claude-code/\">note from this morning</a>). I've used it in several different projects in the past few days, each of them with a prompt that looks something like this:</p>\r\n<blockquote>\r\n<p><code>Use \"uvx showboat --help\" to perform a very thorough investigation of what happens if you use the Python sqlite-chronicle and sqlite-history-json libraries against the same SQLite database table</code></p>\r\n</blockquote>\r\n<p>Here's <a href=\"https://github.com/simonw/research/blob/main/sqlite-chronicle-vs-history-json/demo.md\">the resulting document</a>.</p>\r\n<p>Just telling Claude Code to run <code>uvx showboat --help</code> is enough for it to learn how to use the tool - the <a href=\"https://github.com/simonw/showboat/blob/main/help.txt\">help text</a> is designed to work as a sort of ad-hoc Skill document.</p>\r\n<p>The one catch with this approach is that I can't <em>see</em> the new Showboat document until it's finished. I have to wait for Claude to commit the document plus embedded screenshots and push that to a branch in my GitHub repo - then I can view it through the GitHub interface.</p>\r\n<p>For a while I've been thinking it would be neat to have a remote web server of my own which Claude instances can submit updates to while they are working. Then this morning I realized Showboat might be the ideal mechanism to set that up...</p>\r\n<p>Showboat <a href=\"https://github.com/simonw/showboat/releases/tag/v0.6.0\">v0.6.0</a> adds a new \"remote\" feature. It's almost invisible to users of the tool itself, instead being configured by an environment variable.</p>\r\n<p>Set a variable like this:</p>\r\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-k\">export</span> SHOWBOAT_REMOTE_URL=https://www.example.com/submit<span class=\"pl-k\">?</span>token=xyz</pre></div>\r\n<p>And every time you run a <code>showboat init</code> or <code>showboat note</code> or <code>showboat exec</code> or <code>showboat image</code> command the resulting document fragments will be POSTed to that API endpoint, in addition to the Showboat Markdown file itself being updated.</p>\r\n<p>There are <a href=\"https://github.com/simonw/showboat/blob/v0.6.0/README.md#remote-document-streaming\">full details in the Showboat README</a> - it's a very simple API format, using regular POST form variables or a multipart form upload for the image attached to <code>showboat image</code>.</p>\r\n<h4 id=\"datasette-showboat\">datasette-showboat</h4>\r\n<p>It's simple enough to build a webapp to receive these updates from Showboat, but I needed one that I could easily deploy and would work well with the rest of my personal ecosystem.</p>\r\n<p>So I had Claude Code write me a Datasette plugin that could act as a Showboat remote endpoint. I actually had this building at the same time as the Showboat remote feature, a neat example of running <a href=\"https://simonwillison.net/2025/Oct/5/parallel-coding-agents/\">parallel agents</a>.</p>\r\n<p><strong><a href=\"https://github.com/simonw/datasette-showboat\">datasette-showboat</a></strong> is a Datasette plugin that adds a <code>/-/showboat</code> endpoint to Datasette for viewing documents and a <code>/-/showboat/receive</code> endpoint for receiving updates from Showboat.</p>\r\n<p>Here's a very quick way to try it out:</p>\r\n<div class=\"highlight highlight-source-shell\"><pre>uvx --with datasette-showboat --prerelease=allow \\\r\n  datasette showboat.db --create \\\r\n  -s plugins.datasette-showboat.database showboat \\\r\n  -s plugins.datasette-showboat.token secret123 \\\r\n  --root --secret cookie-secret-123</pre></div>\r\n<p>Click on the sign in as root link that shows up in the console, then navigate to <a href=\"http://127.0.0.1:8001/-/showboat\">http://127.0.0.1:8001/-/showboat</a> to see the interface.</p>\r\n<p>Now set your environment variable to point to this instance:</p>\r\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-k\">export</span> SHOWBOAT_REMOTE_URL=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>http://127.0.0.1:8001/-/showboat/receive?token=secret123<span class=\"pl-pds\">\"</span></span></pre></div>\r\n<p>And run Showboat like this:</p>\r\n<div class=\"highlight highlight-source-shell\"><pre>uvx showboat init demo.md <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Showboat Feature Demo<span class=\"pl-pds\">\"</span></span></pre></div>\r\n<p>Refresh that page and you should see this:</p>\r\n<p><img src=\"https://static.simonwillison.net/static/2026/datasette-showboat-documents.jpg\" alt=\"Title: Showboat. Remote viewer for Showboat documents. Showboat Feature Demo 2026-02-17 00:06 \u00b7 6 chunks, UUID. To send showboat output to this server, set the SHOWBOAT_REMOTE_URL environment variable: export SHOWBOAT_REMOTE_URL=&quot;http://127.0.0.1:8001/-/showboat/receive?token=your-token&quot;\" style=\"max-width: 100%;\" /></p>\r\n<p>Click through to the document, then start Claude Code or Codex or your agent of choice and prompt:</p>\r\n<blockquote>\r\n<p><code>Run 'uvx showboat --help' and then use showboat to add to the existing demo.md document with notes and exec and image to demonstrate the tool - fetch a placekitten for the image demo.</code></p>\r\n</blockquote>\r\n<p>The <code>init</code> command assigns a UUID and title and sends those up to Datasette.</p>\r\n<p><img src=\"https://static.simonwillison.net/static/2026/datasette-showboat.gif\" alt=\"Animated demo - in the foreground a terminal window runs Claude Code, which executes various Showboat commands. In the background a Firefox window where the Showboat Feature Demo adds notes then some bash commands, then a placekitten image.\" style=\"max-width: 100%;\" /></p>\r\n<p>The best part of this is that it works in Claude Code for web. Run the plugin on a server somewhere (an exercise left up to the reader - I use <a href=\"https://fly.io/\">Fly.io</a> to host mine) and set that <code>SHOWBOAT_REMOTE_URL</code> environment variable in your Claude environment, then any time you tell it to use Showboat the document it creates will be transmitted to your server and viewable in real time.</p>\r\n<p>I built <a href=\"https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#rodney-cli-browser-automation-designed-to-work-with-showboat\">Rodney</a>, a CLI browser automation tool, specifically to work with Showboat. It makes it easy to have a Showboat document load up web pages, interact with them via clicks or injected JavaScript and captures screenshots to embed in the Showboat document and show the effects.</p>\r\n<p>This is wildly useful for hacking on web interfaces using Claude Code for web, especially when coupled with the new remote publishing feature. I only got this stuff working this morning and I've already had several sessions where Claude Code has published screenshots of its work in progress, which I've then been able to provide feedback on directly in the Claude session while it's still working.</p>\r\n<h3 id=\"chartroom\">Chartroom</h3>\r\n<p>A few days ago I had another idea for a way to extend the Showboat ecosystem: what if Showboat documents could easily include charts?</p>\r\n<p>I sometimes fire up Claude Code for data analysis tasks, often telling it to download a SQLite database and then run queries against it to figure out interesting things from the data.</p>\r\n<p>With a simple CLI tool that produced PNG images I could have Claude use Showboat to build a document with embedded charts to help illustrate its findings.</p>\r\n<p><strong><a href=\"https://github.com/simonw/chartroom\">Chartroom</a></strong> is exactly that. It's effectively a thin wrapper around the excellent <a href=\"https://matplotlib.org/\">matplotlib</a> Python library, designed to be used by coding agents to create charts that can be embedded in Showboat documents.</p>\r\n<p>Here's how to render a simple bar chart:</p>\r\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c1\">echo</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>name,value</span>\r\n<span class=\"pl-s\">Alice,42</span>\r\n<span class=\"pl-s\">Bob,28</span>\r\n<span class=\"pl-s\">Charlie,35</span>\r\n<span class=\"pl-s\">Diana,51</span>\r\n<span class=\"pl-s\">Eve,19<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">|</span> uvx chartroom bar --csv \\\r\n  --title <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Sales by Person<span class=\"pl-pds\">'</span></span> --ylabel <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Sales<span class=\"pl-pds\">'</span></span></pre></div>\r\n<p><a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://raw.githubusercontent.com/simonw/chartroom/8812afc02e1310e9eddbb56508b06005ff2c0ed5/demo/1f6851ec-2026-02-14.png\"><img src=\"https://raw.githubusercontent.com/simonw/chartroom/8812afc02e1310e9eddbb56508b06005ff2c0ed5/demo/1f6851ec-2026-02-14.png\" alt=\"A chart of those numbers, with a title and y-axis label\" style=\"max-width: 100%;\" /></a></p>\r\n<p>It can also do line charts, bar charts, scatter charts, and histograms - as seen in <a href=\"https://github.com/simonw/chartroom/blob/0.2.1/demo/README.md\">this demo document</a> that was built using Showboat.</p>\r\n<p>Chartroom can also generate alt text. If you add <code>-f alt</code> to the above it will output the alt text for the chart instead of the image:</p>\r\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c1\">echo</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>name,value</span>\r\n<span class=\"pl-s\">Alice,42</span>\r\n<span class=\"pl-s\">Bob,28</span>\r\n<span class=\"pl-s\">Charlie,35</span>\r\n<span class=\"pl-s\">Diana,51</span>\r\n<span class=\"pl-s\">Eve,19<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">|</span> uvx chartroom bar --csv \\\r\n  --title <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Sales by Person<span class=\"pl-pds\">'</span></span> --ylabel <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Sales<span class=\"pl-pds\">'</span></span> -f alt</pre></div>\r\n<p>Outputs:</p>\r\n<pre><code>Sales by Person. Bar chart of value by name \u2014 Alice: 42, Bob: 28, Charlie: 35, Diana: 51, Eve: 19\r\n</code></pre>\r\n<p>Or you can use <code>-f html</code> or <code>-f markdown</code> to get the image tag with alt text directly:</p>\r\n<div class=\"highlight highlight-text-md\"><pre><span class=\"pl-s\">![</span>Sales by Person. Bar chart of value by name \u2014 Alice: 42, Bob: 28, Charlie: 35, Diana: 51, Eve: 19<span class=\"pl-s\">]</span><span class=\"pl-s\">(</span><span class=\"pl-corl\">/Users/simon/chart-7.png</span><span class=\"pl-s\">)</span></pre></div>\r\n<p>I added support for Markdown images with alt text to Showboat in <a href=\"https://github.com/simonw/showboat/releases/tag/v0.5.0\">v0.5.0</a>, to complement this feature of Chartroom.</p>\r\n<p>Finally, Chartroom has support for different <a href=\"https://matplotlib.org/stable/gallery/style_sheets/style_sheets_reference.html\">matplotlib styles</a>. I had Claude build a Showboat document to demonstrate these all in one place - you can see that at <a href=\"https://github.com/simonw/chartroom/blob/main/demo/styles.md\">demo/styles.md</a>.</p>\r\n<h4 id=\"how-i-built-chartroom\">How I built Chartroom</h4>\r\n<p>I started the Chartroom repository with my <a href=\"https://github.com/simonw/click-app\">click-app</a> cookiecutter template, then told a fresh Claude Code for web session:</p>\r\n<blockquote>\r\n<p>We are building a Python CLI tool which uses matplotlib to generate a PNG image containing a chart. It will have multiple sub commands for different chart types, controlled by command line options. Everything you need to know to use it will be available in the single \"chartroom --help\" output.</p>\r\n<p>It will accept data from files or standard input as CSV or TSV or JSON, similar to how sqlite-utils accepts data - clone simonw/sqlite-utils to /tmp for reference there. Clone matplotlib/matplotlib for reference as well</p>\r\n<p>It will also accept data from --sql path/to/sqlite.db \"select ...\" which runs in read-only mode</p>\r\n<p>Start by asking clarifying questions - do not use the ask user tool though it is broken - and generate a spec for me to approve</p>\r\n<p>Once approved proceed using red/green TDD running tests with \"uv run pytest\"</p>\r\n<p>Also while building maintain a demo/README.md document using the \"uvx showboat --help\" tool - each time you get a new chart type working commit the tests, implementation, root level\r\nREADME update and a new version of that demo/README.md document with an inline image demo of the new chart type (which should be a UUID image filename managed by the showboat image command and should be stored in the demo/ folder</p>\r\n<p>Make sure \"uv build\" runs cleanly without complaining about extra directories but also ensure dist/ and uv.lock are in gitignore</p>\r\n</blockquote>\r\n<p>This got most of the work done. You can see the rest <a href=\"https://github.com/simonw/chartroom/pulls?q=is%3Apr+is%3Aclosed\">in the PRs</a> that followed.</p>\r\n<h4 id=\"the-burgeoning-showboat-ecosystem\">The burgeoning Showboat ecosystem</h4>\r\n<p>The Showboat family of tools now consists of <a href=\"https://github.com/simonw/showboat\">Showboat</a> itself, <a href=\"https://github.com/simonw/rodney\">Rodney</a> for browser automation, <a href=\"https://github.com/simonw/chartroom\">Chartroom</a> for charting and <a href=\"https://github.com/simonw/datasette-showboat\">datasette-showboat</a> for streaming remote Showboat documents to Datasette.</p>\r\n<p>I'm enjoying how these tools can operate together based on a very loose set of conventions. If a tool can output a path to an image Showboat can include that image in a document. Any tool that can output text can be used with Showboat.</p>\r\n<p>I'll almost certainly be building more tools that fit this pattern. They're very quick to knock out!</p>\r\n<p>The environment variable mechanism for Showboat's remote streaming is a fun hack too - so far I'm just using it to stream documents somewhere else, but it's effectively a webhook extension mechanism that could likely be used for all sorts of things I haven't thought of yet.</p>"
        },
        {
          "type": "blogmark",
          "id": 9297,
          "created": "2026-02-17T04:30:57+00:00",
          "title": "Qwen3.5: Towards Native Multimodal Agents - ",
          "body": "Alibaba's Qwen just released the first two models in the Qwen 3.5 series - one open weights, one proprietary. Both are multi-modal for vision input.\r\n\r\nThe open weight one is a Mixture of Experts model called Qwen3.5-397B-A17B. Interesting to see Qwen call out serving efficiency as a benefit of that architecture:\r\n\r\n> Built on an innovative hybrid architecture that fuses linear attention (via Gated Delta Networks) with a sparse mixture-of-experts, the model attains remarkable inference efficiency: although it comprises 397 billion total parameters, just 17 billion are activated per forward pass, optimizing both speed and cost without sacrificing capability.\r\n\r\nIt's [807GB on Hugging Face](https://huggingface.co/Qwen/Qwen3.5-397B-A17B), and Unsloth have a [collection of smaller GGUFs](https://huggingface.co/unsloth/Qwen3.5-397B-A17B-GGUF) ranging in size from 94.2GB 1-bit to 462GB Q8_K_XL.\r\n\r\nI got this [pelican](https://simonwillison.net/tags/pelican-riding-a-bicycle/) from the [OpenRouter hosted model](https://openrouter.ai/qwen/qwen3.5-397b-a17b) ([transcript](https://gist.github.com/simonw/625546cf6b371f9c0040e64492943b82)):\r\n\r\n![Pelican is quite good although the neck lacks an outline for some reason. Bicycle is very basic with an incomplete frame](https://static.simonwillison.net/static/2026/qwen3.5-397b-a17b.png)\r\n\r\nThe proprietary hosted model is called Qwen3.5 Plus 2026-02-15, and is a little confusing. Qwen researcher [Junyang Lin  says](https://twitter.com/JustinLin610/status/2023340126479569140):\r\n\r\n> Qwen3-Plus is a hosted API version of 397B. As the model natively supports 256K tokens, Qwen3.5-Plus supports 1M token context length. Additionally it supports search and code interpreter, which you can use on Qwen Chat with Auto mode.\r\n\r\nHere's [its pelican](https://gist.github.com/simonw/9507dd47483f78dc1195117735273e20), which is similar in quality to the open weights model:\r\n\r\n![Similar quality pelican. The bicycle is taller and has a better frame shape. They are visually quite similar.](https://static.simonwillison.net/static/2026/qwen3.5-plus-02-15.png)"
        },
        {
          "type": "note",
          "id": 268,
          "created": "2026-02-17T04:51:58+00:00",
          "title": "Nano Banana Pro diff to webcomic",
          "body": "Given the threat of [cognitive debt](https://simonwillison.net/tags/cognitive-debt/) brought on by AI-accelerated software development leading to more projects and less deep understanding of how they work and what they actually do, it's interesting to consider artifacts that might be able to help.\r\n\r\nNathan Baschez [on Twitter](https://twitter.com/nbaschez/status/2023501535343509871):\r\n\r\n> my current favorite trick for reducing \"cognitive debt\" (h/t @simonw\r\n) is to ask the LLM to write two versions of the plan:\r\n>\r\n> 1. The version for it (highly technical and detailed)\r\n> 2. The version for me (an entertaining essay designed to build my intuition)\r\n>\r\n> Works great\r\n\r\nThis inspired me to try something new. I generated [the diff](https://github.com/simonw/showboat/compare/v0.5.0...v0.6.0.diff) between v0.5.0 and v0.6.0 of my Showboat project - which introduced [the remote publishing feature](https://simonwillison.net/2026/Feb/17/chartroom-and-datasette-showboat/#showboat-remote-publishing) - and dumped that into Nano Banana Pro with the prompt:\r\n\r\n> Create a webcomic that explains the new feature as clearly and entertainingly as possible\r\n\r\nHere's [what it produced](https://gemini.google.com/share/cce6da8e5083):\r\n\r\n![A six-panel comic strip illustrating a tool called \"Showboat\" for live-streaming document building. Panel 1, titled \"THE OLD WAY: Building docs was a lonely voyage. You finished it all before anyone saw it.\", shows a sad bearded man on a wooden boat labeled \"THE LOCALHOST\" holding papers and saying \"Almost done... then I have to export and email the HTML...\". Panel 2, titled \"THE UPGRADE: Just set the environment variable!\", shows the same man excitedly plugging in a device with a speech bubble reading \"ENV VAR: SHOWBOAT_REMOTE_URL\" and the sound effect \"*KA-CHUNK!*\". Panel 3, titled \"`init` establishes the uplink and generates a unique UUID beacon.\", shows the man typing at a keyboard with a terminal reading \"$ showboat init 'Live Demo'\", a satellite dish transmitting to a floating label \"UUID: 550e84...\", and a monitor reading \"WAITING FOR STREAM...\". Panel 4, titled \"Every `note` and `exec` is instantly beamed to the remote viewer!\", shows the man coding with sound effects \"*HAMMER!*\", \"ZAP!\", \"ZAP!\", \"BANG!\" as red laser beams shoot from a satellite dish to a remote screen displaying \"NOTE: Step 1...\" and \"SUCCESS\". Panel 5, titled \"Even `image` files are teleported in real-time!\", shows a satellite dish firing a cyan beam with the sound effect \"*FOOMP!*\" toward a monitor displaying a bar chart. Panel 6, titled \"You just build. The audience gets the show live.\", shows the man happily working at his boat while a crowd of cheering people watches a projected screen reading \"SHOWBOAT LIVE STREAM: Live Demo\", with a label \"UUID: 550e84...\" and one person in the foreground eating popcorn.](https://static.simonwillison.net/static/2026/nano-banana-diff.jpg)\r\n\r\nGood enough to publish with the release notes? I don't think so. I'm sharing it here purely to demonstrate the idea. Creating assets like this as a personal tool for thinking about novel ways to explain a feature feels worth exploring further."
        },
        {
          "type": "quotation",
          "id": 2028,
          "created": "2026-02-17T14:04:44+00:00",
          "title": "Dimitris Papailiopoulos",
          "body": "But the intellectually interesting part for me is something else. **I now have something close to a magic box where I throw in a question and a first answer comes back basically for free, in terms of human effort**. Before this, the way I'd explore a new idea is to either clumsily put something together myself or ask a student to run something short for signal, and if it's there, we\u2019d go deeper. That quick signal step, i.e., finding out if a question has any meat to it, is what I can now do without taking up anyone else's time. It\u2019s now between just me, Claude Code, and a few days of GPU time.\r\n\r\nI don\u2019t know what this means for how we do research long term. I don\u2019t think anyone does yet. But **the distance between a question and a first answer just got very small**. - Dimitris Papailiopoulos"
        },
        {
          "type": "blogmark",
          "id": 9298,
          "created": "2026-02-17T14:09:43+00:00",
          "title": "First k\u0101k\u0101p\u014d chick in four years hatches on Valentine's Day - MetaFilter",
          "body": "First chick of [the 2026 breeding season](https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/#1-year-k-k-p-parrots-will-have-an-outstanding-breeding-season)!\r\n\r\n> K\u0101k\u0101p\u014d Yasmine hatched an egg fostered from k\u0101k\u0101p\u014d T\u012bwhiri on Valentine's Day, bringing the total number of k\u0101k\u0101p\u014d to 237 \u2013 though it won\u2019t be officially added to the population until it fledges.\r\n\r\nHere's why the egg was fostered:\r\n\r\n> \"K\u0101k\u0101p\u014d mums typically have the best outcomes when raising a maximum of two chicks. Biological mum T\u012bwhiri has four fertile eggs this season already, while Yasmine, an experienced foster mum, had no fertile eggs.\"\r\n\r\nAnd an [update from conservation biologist Andrew Digby](https://bsky.app/profile/digs.bsky.social/post/3mf25glzt2c2b) - a second chick hatched this morning!\r\n\r\n> The second #kakapo chick of the #kakapo2026 breeding season hatched this morning: Hine Taumai-A1-2026 on Ako's nest on Te K\u0101kahu. We transferred the egg from Anchor two nights ago. This is Ako's first-ever chick, which is just a few hours old in this video.\r\n\r\nThat post [has a video](https://bsky.app/profile/digs.bsky.social/post/3mf25glzt2c2b) of mother and chick.\r\n\r\n![A beautiful charismatic green K\u0101k\u0101p feeding a little grey chick](https://static.simonwillison.net/static/2026/kakapo-plus-chick.jpg)"
        },
        {
          "type": "quotation",
          "id": 2029,
          "created": "2026-02-17T14:49:04+00:00",
          "title": "ROUGH DRAFT 8/2/66",
          "body": "This is the story of the United Space Ship Enterprise. Assigned a five year patrol of our galaxy, the giant starship visits Earth colonies, regulates commerce, and explores strange new worlds and civilizations. These are its voyages... and its adventures. - ROUGH DRAFT 8/2/66"
        },
        {
          "type": "blogmark",
          "id": 9299,
          "created": "2026-02-17T23:02:33+00:00",
          "title": "Rodney v0.4.0 - ",
          "body": "My [Rodney](https://github.com/simonw/rodney) CLI tool for browser automation attracted quite the flurry of PRs since I announced it [last week](https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#rodney-cli-browser-automation-designed-to-work-with-showboat). Here are the release notes for the just-released v0.4.0:\r\n\r\n> - Errors now use exit code 2, which means exit code 1 is just for for check failures. [#15](https://github.com/simonw/rodney/pull/15)\r\n> - New `rodney assert` command for running JavaScript tests, exit code 1 if they fail. [#19](https://github.com/simonw/rodney/issues/19)\r\n> - New directory-scoped sessions with `--local`/`--global` flags. [#14](https://github.com/simonw/rodney/pull/14)\r\n> - New `reload --hard` and `clear-cache` commands. [#17](https://github.com/simonw/rodney/pull/17)\r\n> - New `rodney start --show` option to make the browser window visible. Thanks, [Antonio Cuni](https://github.com/antocuni). [#13](https://github.com/simonw/rodney/paull/13)\r\n> - New `rodney connect PORT` command to debug an already-running Chrome instance. Thanks, [Peter Fraenkel](https://github.com/pnf). [#12](https://github.com/simonw/rodney/pull/12)\r\n> - New `RODNEY_HOME` environment variable to support custom state directories. Thanks, [Senko Ra\u0161i\u0107](https://github.com/senko). [#11](https://github.com/simonw/rodney/pull/11)\r\n> - New `--insecure` flag to ignore certificate errors. Thanks, [Jakub Zgoli\u0144ski](https://github.com/zgolus). [#10](https://github.com/simonw/rodney/pull/10)\r\n> - Windows support: avoid `Setsid` on Windows via build-tag helpers. Thanks, [adm1neca](https://github.com/adm1neca). [#18](https://github.com/simonw/rodney/pull/18)\r\n> - Tests now run on `windows-latest` and `macos-latest` in addition to Linux.\r\n\r\nI've been using [Showboat](https://github.com/simonw/showboat) to create demos of new features - here those are for [rodney assert](https://github.com/simonw/rodney/tree/v0.4.0/notes/assert-command-demo), [rodney reload --hard](https://github.com/simonw/rodney/tree/v0.4.0/notes/clear-cache-demo), [rodney exit codes](https://github.com/simonw/rodney/tree/v0.4.0/notes/error-codes-demo), and [rodney start --local](https://github.com/simonw/rodney/tree/v0.4.0/notes/local-sessions-demo).\r\n\r\nThe `rodney assert` command is pretty neat: you can now Rodney to test a web app through multiple steps in a shell script that looks something like this (adapted from [the README](https://github.com/simonw/rodney/blob/v0.4.0/README.md#combining-checks-in-a-shell-script)):\r\n\r\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#!</span>/bin/bash</span>\r\n<span class=\"pl-c1\">set</span> -euo pipefail\r\n\r\nFAIL=0\r\n\r\n<span class=\"pl-en\">check</span>() {\r\n    <span class=\"pl-k\">if</span> <span class=\"pl-k\">!</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">$@</span><span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">;</span> <span class=\"pl-k\">then</span>\r\n        <span class=\"pl-c1\">echo</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>FAIL: <span class=\"pl-smi\">$*</span><span class=\"pl-pds\">\"</span></span>\r\n        FAIL=1\r\n    <span class=\"pl-k\">fi</span>\r\n}\r\n\r\nrodney start\r\nrodney open <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>https://example.com<span class=\"pl-pds\">\"</span></span>\r\nrodney waitstable\r\n\r\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Assert elements exist</span>\r\ncheck rodney exists <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>h1<span class=\"pl-pds\">\"</span></span>\r\n\r\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Assert key elements are visible</span>\r\ncheck rodney visible <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>h1<span class=\"pl-pds\">\"</span></span>\r\ncheck rodney visible <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>#main-content<span class=\"pl-pds\">\"</span></span>\r\n\r\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Assert JS expressions</span>\r\ncheck rodney assert <span class=\"pl-s\"><span class=\"pl-pds\">'</span>document.title<span class=\"pl-pds\">'</span></span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Example Domain<span class=\"pl-pds\">'</span></span>\r\ncheck rodney assert <span class=\"pl-s\"><span class=\"pl-pds\">'</span>document.querySelectorAll(\"p\").length<span class=\"pl-pds\">'</span></span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>2<span class=\"pl-pds\">'</span></span>\r\n\r\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Assert accessibility requirements</span>\r\ncheck rodney ax-find --role navigation\r\n\r\nrodney stop\r\n\r\n<span class=\"pl-k\">if</span> [ <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">$FAIL</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">-ne</span> 0 ]<span class=\"pl-k\">;</span> <span class=\"pl-k\">then</span>\r\n    <span class=\"pl-c1\">echo</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Some checks failed<span class=\"pl-pds\">\"</span></span>\r\n    <span class=\"pl-c1\">exit</span> 1\r\n<span class=\"pl-k\">fi</span>\r\n<span class=\"pl-c1\">echo</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>All checks passed<span class=\"pl-pds\">\"</span></span></pre></div>"
        },
        {
          "type": "blogmark",
          "id": 9300,
          "created": "2026-02-17T23:58:58+00:00",
          "title": "Introducing Claude Sonnet 4.6 - Hacker News",
          "body": "Sonnet 4.6 is out today, and Anthropic claim it offers similar performance to [November's Opus 4.5](https://simonwillison.net/2025/Nov/24/claude-opus/) while maintaining the Sonnet pricing of $3/million input and $15/million output tokens (the Opus models are $5/$25). Here's [the system card PDF](https://www-cdn.anthropic.com/78073f739564e986ff3e28522761a7a0b4484f84.pdf).\r\n\r\nSonnet 4.6 has a \"reliable knowledge cutoff\" of August 2025, compared to Opus 4.6's May 2025 and Haiku 4.5's February 2025. Both Opus and Sonnet default to 200,000 max input tokens but can stretch to 1 million in beta and at a higher cost.\r\n\r\nI just released [llm-anthropic 0.24](https://github.com/simonw/llm-anthropic/releases/tag/0.24) with support for both Sonnet 4.6 and Opus 4.6. Claude Code [did most of the work](https://github.com/simonw/llm-anthropic/pull/65) - the new models had a fiddly amount of extra details around adaptive thinking and no longer supporting prefixes, as described [in Anthropic's migration guide](https://platform.claude.com/docs/en/about-claude/models/migration-guide).\r\n\r\nHere's [what I got](https://gist.github.com/simonw/b185576a95e9321b441f0a4dfc0e297c) from:\r\n\r\n    uvx --with llm-anthropic llm 'Generate an SVG of a pelican riding a bicycle' -m claude-sonnet-4.6\r\n\r\n![The pelican has a jaunty top hat with a red band. There is a string between the upper and lower beaks for some reason. The bicycle frame is warped in the wrong way.](https://static.simonwillison.net/static/2026/pelican-sonnet-4.6.png)\r\n\r\nThe SVG comments include:\r\n\r\n    <!-- Hat (fun accessory) -->\r\n\r\nI tried a second time and also got a top hat. Sonnet 4.6 apparently loves top hats!\r\n\r\nFor comparison, here's the pelican Opus 4.5 drew me [in November]((https://simonwillison.net/2025/Nov/24/claude-opus/)):\r\n\r\n![The pelican is cute and looks pretty good. The bicycle is not great - the frame is wrong and the pelican is facing backwards when the handlebars appear to be forwards.There is also something that looks a bit like an egg on the handlebars.](https://static.simonwillison.net/static/2025/claude-opus-4.5-pelican.jpg)\r\n\r\nAnd here's Anthropic's current best pelican, drawn by Opus 4.6 [on February 5th](https://simonwillison.net/2026/Feb/5/two-new-models/):\r\n\r\n![Slightly wonky bicycle frame but an excellent pelican, very clear beak and pouch, nice feathers.](https://static.simonwillison.net/static/2026/opus-4.6-pelican.png)\r\n\r\nOpus 4.6 produces the best pelican beak/pouch. I do think the top hat from Sonnet 4.6 is a nice touch though."
        },
        {
          "type": "quotation",
          "id": 2030,
          "created": "2026-02-18T16:50:07+00:00",
          "title": "Martin Fowler",
          "body": "LLMs are eating specialty skills. There will be less use of specialist front-end and back-end developers as the LLM-driving skills become more important than the details of platform usage. Will this lead to a greater recognition of the role of [Expert Generalists](https://martinfowler.com/articles/expert-generalist.html)? Or will the ability of LLMs to write lots of code mean they code around the silos rather than eliminating them? - Martin Fowler"
        },
        {
          "type": "blogmark",
          "id": 9301,
          "created": "2026-02-18T17:07:31+00:00",
          "title": "The A.I. Disruption We\u2019ve Been Waiting for Has Arrived - ",
          "body": "New opinion piece from Paul Ford in the New York Times. Unsurprisingly for a piece by Paul it's packed with quoteworthy snippets, but a few stood out for me in particular.\r\n\r\nPaul describes the [November moment](https://simonwillison.net/2026/Jan/4/inflection/) that so many other programmers have observed, and highlights Claude Code's ability to revive old side projects:\r\n\r\n> [Claude Code] was always a helpful coding assistant, but in November it suddenly got much better, and ever since I\u2019ve been knocking off side projects that had sat in folders for a decade or longer. It\u2019s fun to see old ideas come to life, so I keep a steady flow. Maybe it adds up to a half-hour a day of my time, and an hour of Claude\u2019s.\r\n>\r\n> November was, for me and many others in tech, a great surprise. Before, A.I. coding tools were often useful, but halting and clumsy. Now, the bot can run for a full hour and make whole, designed websites and apps that may be flawed, but credible. I spent an entire session of therapy talking about it.\r\n\r\nAnd as the former CEO of a respected consultancy firm (Postlight) he's well positioned to evaluate the potential impact:\r\n\r\n> When you watch a large language model slice through some horrible, expensive problem \u2014 like migrating data from an old platform to a modern one \u2014 you feel the earth shifting. I was the chief executive of a software services firm, which made me a professional software cost estimator. When I rebooted my messy personal website a few weeks ago, I realized: I would have paid $25,000 for someone else to do this. When a friend asked me to convert a large, thorny data set, I downloaded it, cleaned it up and made it pretty and easy to explore. In the past I would have charged $350,000.\r\n>\r\n> That last price is full 2021 retail \u2014 it implies a product manager, a designer, two engineers (one senior) and four to six months of design, coding and testing. Plus maintenance. Bespoke software is joltingly expensive. Today, though, when the stars align and my prompts work out, I can do hundreds of thousands of dollars worth of work for fun (fun for me) over weekends and evenings, for the price of the Claude $200-a-month plan.\r\n\r\nHe also neatly captures the inherent community tension involved in exploring this technology:\r\n\r\n> All of the people I love hate this stuff, and all the people I hate love it. And yet, likely because of the same personality flaws that drew me to technology in the first place, I am annoyingly excited."
        },
        {
          "type": "note",
          "id": 269,
          "created": "2026-02-18T18:56:56+00:00",
          "title": "Typing without having to type",
          "body": "25+ years into my career as a programmer I think I may *finally* be coming around to preferring type hints or even strong typing. I resisted those in the past because they slowed down the rate at which I could iterate on code, especially in the REPL environments that were key to my productivity. But if a coding agent is doing all that *typing* for me, the benefits of explicitly defining all of those types are suddenly much more attractive."
        },
        {
          "type": "blogmark",
          "id": 9302,
          "created": "2026-02-19T01:25:33+00:00",
          "title": "LadybirdBrowser/ladybird: Abandon Swift adoption - Hacker News",
          "body": "Back [in August 2024](https://simonwillison.net/2024/Aug/11/ladybird-set-to-adopt-swift/) the Ladybird browser project announced an intention to adopt Swift as their memory-safe language of choice.\r\n\r\nAs of [this commit](https://github.com/LadybirdBrowser/ladybird/commit/e87f889e31afbb5fa32c910603c7f5e781c97afd) it looks like they've changed their mind:\r\n\r\n> **Everywhere: Abandon Swift adoption**\r\n>\r\n> After making no progress on this for a very long time, let's acknowledge it's not going anywhere and remove it from the codebase.\r\n\r\n**Update 23rd February 2025**: They've [adopted Rust](https://ladybird.org/posts/adopting-rust/) instead."
        },
        {
          "type": "blogmark",
          "id": 9303,
          "created": "2026-02-19T04:48:47+00:00",
          "title": "SWE-bench February 2026 leaderboard update - @KLieret",
          "body": "SWE-bench is one of the benchmarks that the labs love to list in their model releases. The official leaderboard is infrequently updated but they just did a full run of it against the current generation of models, which is notable because it's always good to see benchmark results like this that *weren't* self-reported by the labs.\r\n\r\nThe fresh results are for their \"Bash Only\" benchmark, which runs their [mini-swe-bench](https://github.com/SWE-agent/mini-swe-agent) agent (~9,000 lines of Python, [here are the prompts](https://github.com/SWE-agent/mini-swe-agent/blob/v2.2.1/src/minisweagent/config/benchmarks/swebench.yaml) they use) against the [SWE-bench](https://huggingface.co/datasets/princeton-nlp/SWE-bench) dataset of coding problems - 2,294 real-world examples pulled from 12 open source repos: [django/django](https://github.com/django/django) (850), [sympy/sympy](https://github.com/sympy/sympy) (386), [scikit-learn/scikit-learn](https://github.com/scikit-learn/scikit-learn) (229), [sphinx-doc/sphinx](https://github.com/sphinx-doc/sphinx) (187), [matplotlib/matplotlib](https://github.com/matplotlib/matplotlib) (184), [pytest-dev/pytest](https://github.com/pytest-dev/pytest) (119), [pydata/xarray](https://github.com/pydata/xarray) (110), [astropy/astropy](https://github.com/astropy/astropy) (95), [pylint-dev/pylint](https://github.com/pylint-dev/pylint) (57), [psf/requests](https://github.com/psf/requests) (44), [mwaskom/seaborn](https://github.com/mwaskom/seaborn) (22), [pallets/flask](https://github.com/pallets/flask) (11).\r\n\r\n**Correction**: *The Bash only benchmark runs against SWE-bench Verified, not original SWE-bench. Verified is a manually curated subset of 500 samples [described here](https://openai.com/index/introducing-swe-bench-verified/), funded by OpenAI. Here's [SWE-bench Verified](https://huggingface.co/datasets/princeton-nlp/SWE-bench_Verified) on Hugging Face - since it's just 2.1MB of Parquet it's easy to browse [using Datasette Lite](https://lite.datasette.io/?parquet=https%3A%2F%2Fhuggingface.co%2Fdatasets%2Fprinceton-nlp%2FSWE-bench_Verified%2Fresolve%2Fmain%2Fdata%2Ftest-00000-of-00001.parquet#/data/test-00000-of-00001?_facet=repo), which cuts those numbers down to django/django (231), sympy/sympy (75), sphinx-doc/sphinx (44), matplotlib/matplotlib (34), scikit-learn/scikit-learn (32), astropy/astropy (22), pydata/xarray (22), pytest-dev/pytest (19), pylint-dev/pylint (10), psf/requests (8), mwaskom/seaborn (2), pallets/flask (1)*.\r\n\r\nHere's how the top ten models performed:\r\n\r\n![Bar chart showing \"% Resolved\" by \"Model\". Bars in descending order: Claude 4.5 Opus (high reasoning) 76.8%, Gemini 3 Flash (high reasoning) 75.8%, MiniMax M2.5 (high reasoning) 75.8%, Claude Opus 4.6 75.6%, GLM-5 (high reasoning) 72.8%, GPT-5.2 (high reasoning) 72.8%, Claude 4.5 Sonnet (high reasoning) 72.8%, Kimi K2.5 (high reasoning) 71.4%, DeepSeek V3.2 (high reasoning) 70.8%, Claude 4.5 Haiku (high reasoning) 70.0%, and a partially visible final bar at 66.6%.](https://static.simonwillison.net/static/2026/swbench-feb-2026.jpg)\r\n\r\nIt's interesting to see Claude Opus 4.5 beat Opus 4.6, though only by about a percentage point. 4.5 Opus is top, then Gemini 3 Flash, then MiniMax M2.5 - a 229B model released [last week](https://www.minimax.io/news/minimax-m25) by Chinese lab MiniMax. GLM-5, Kimi K2.5 and DeepSeek V3.2 are three more Chinese models that make the top ten as well.\r\n\r\nOpenAI's GPT-5.2 is their highest performing model at position 6, but it's worth noting that their best coding model, GPT-5.3-Codex, is not represented - maybe because it's not yet available in the OpenAI API.\r\n\r\nThis benchmark uses the same system prompt for every model, which is important for a fair comparison but does mean that the quality of the different harnesses or optimized prompts is not being measured here.\r\n\r\nThe chart above is a screenshot from the SWE-bench website, but their charts don't include the actual percentage values visible on the bars. I successfully used Claude for Chrome to add these - [transcript here](https://claude.ai/share/81a0c519-c727-4caa-b0d4-0d866375d0da). My prompt sequence included:\r\n\r\n> Use claude in chrome to open https://www.swebench.com/\r\n\r\n> Click on \"Compare results\" and then select \"Select top 10\"\r\n\r\n> See those bar charts? I want them to display the percentage on each bar so I can take a better screenshot, modify the page like that\r\n\r\nI'm impressed at how well this worked - Claude injected custom JavaScript into the page to draw additional labels on top of the existing chart.\r\n\r\n![Screenshot of a Claude AI conversation showing browser automation. A thinking step reads \"Pivoted strategy to avoid recursion issues with chart labeling >\" followed by the message \"Good, the chart is back. Now let me carefully add the labels using an inline plugin on the chart instance to avoid the recursion issue.\" A collapsed \"Browser_evaluate\" section shows a browser_evaluate tool call with JavaScript code using Chart.js canvas context to draw percentage labels on bars: meta.data.forEach((bar, index) => { const value = dataset.data[index]; if (value !== undefined && value !== null) { ctx.save(); ctx.textAlign = 'center'; ctx.textBaseline = 'bottom'; ctx.fillStyle = '#333'; ctx.font = 'bold 12px sans-serif'; ctx.fillText(value.toFixed(1) + '%', bar.x, bar.y - 5); A pending step reads \"Let me take a screenshot to see if it worked.\" followed by a completed \"Done\" step, and the message \"Let me take a screenshot to check the result.\"](https://static.simonwillison.net/static/2026/claude-chrome-draw-on-chart.jpg)\r\n\r\n**Update**: If you look at the transcript Claude claims to have switched to Playwright, which is confusing because I didn't think I had that configured."
        },
        {
          "type": "note",
          "id": 270,
          "created": "2026-02-19T05:44:29+00:00",
          "title": "Experimenting with sponsorship for my blog and newsletter",
          "body": "I've long been resistant to the idea of accepting sponsorship for my blog. I value my credibility as an independent voice, and I don't want to risk compromising that reputation.\r\n\r\nThen I learned about Troy Hunt's [approach to sponsorship](https://www.troyhunt.com/sponsorship/), which he first wrote about [in 2016](https://www.troyhunt.com/im-now-offering-sponsorship-of-this-blog/). Troy runs with a simple text row in the page banner - no JavaScript, no cookies, unobtrusive while providing value to the sponsor. I can live with that!\r\n\r\nAccepting sponsorship in this way helps me maintain my independence while offsetting the opportunity cost of not taking a full-time job.\r\n\r\nTo start with I'm selling sponsorship by the week. Sponsors get that unobtrusive banner across my blog and also their sponsored message at the top of [my newsletter](https://simonw.substack.com/).\r\n\r\n![Screenshot of my blog's homepage. Below the Simon Willison's Weblog heading and list of tags is a new blue page-wide banner reading \"Sponsored by: Teleport - Secure, Govern, and Operate Al at Engineering Scale. Learn more\".](https://static.simonwillison.net/static/2026/sponsor-banner.jpg)\r\n\r\nI **will not write content in exchange for sponsorship**. I hope the sponsors I work with understand that my credibility as an independent voice is a key reason I have an audience, and compromising that trust would be bad for everyone.\r\n\r\n[Freeman & Forrest](https://www.freemanandforrest.com/) helped me set up and sell my first slots. Thanks also to [Theo Browne](https://t3.gg/) for helping me think through my approach."
        },
        {
          "type": "blogmark",
          "id": 9304,
          "created": "2026-02-19T17:58:37+00:00",
          "title": "Gemini 3.1 Pro - ",
          "body": "The first in the Gemini 3.1 series, priced the same as Gemini 3 Pro ($2/million input, $12/million output under 200,000 tokens, $4/$18 for 200,000 to 1,000,000). That's less than half the price of Claude Opus 4.6 with very similar benchmark scores to that model.\r\n\r\nThey boast about its improved SVG animation performance compared to Gemini 3 Pro in the announcement!\r\n\r\nI tried \"Generate an SVG of a pelican riding a bicycle\" [in Google AI Studio](https://aistudio.google.com/app/prompts?state=%7B%22ids%22:%5B%221ugF9fBfLGxnNoe8_rLlluzo9NSPJDWuF%22%5D,%22action%22:%22open%22,%22userId%22:%22106366615678321494423%22,%22resourceKeys%22:%7B%7D%7D&usp=sharing) and it thought for 323.9 seconds ([thinking trace here](https://gist.github.com/simonw/03a755865021739a3659943a22c125ba#thinking-trace)) before producing this one:\r\n\r\n![Whimsical flat-style illustration of a pelican wearing a blue and white baseball cap, riding a red bicycle with yellow-rimmed wheels along a road. The pelican has a large orange bill and a green scarf. A small fish peeks out of a brown basket on the handlebars. The background features a light blue sky with a yellow sun, white clouds, and green hills.](https://static.simonwillison.net/static/2026/gemini-3.1-pro-pelican.png)\r\n\r\nIt's good to see the legs clearly depicted on both sides of the frame (should [satisfy Elon](https://twitter.com/elonmusk/status/2023833496804839808)), the fish in the basket is a nice touch and I appreciated this comment in [the SVG code](https://gist.github.com/simonw/03a755865021739a3659943a22c125ba#response):\r\n\r\n    <!-- Black Flight Feathers on Wing Tip -->\r\n    <path d=\"M 420 175 C 440 182, 460 187, 470 190 C 450 210, 430 208, 410 198 Z\" fill=\"#374151\" />\r\n\r\nI've [added](https://github.com/simonw/llm-gemini/issues/121) the two new model IDs `gemini-3.1-pro-preview` and `gemini-3.1-pro-preview-customtools` to my [llm-gemini plugin](https://github.com/simonw/llm-gemini) for [LLM](https://llm.datasette.io/). That \"custom tools\" one is [described here](https://ai.google.dev/gemini-api/docs/models/gemini-3.1-pro-preview#gemini-31-pro-preview-customtools) - apparently it may provide better tool performance than the default model in some situations.\r\n\r\nThe model appears to be *incredibly* slow right now - it took 104s to respond to a simple \"hi\" and a few of my other tests met \"Error: This model is currently experiencing high demand. Spikes in demand are usually temporary. Please try again later.\" or \"Error: Deadline expired before operation could complete\" errors. I'm assuming that's just teething problems on launch day.\r\n\r\nIt sounds like last week's [Deep Think release](https://simonwillison.net/2026/Feb/12/gemini-3-deep-think/) was our first exposure to the 3.1 family:\r\n\r\n> Last week, we released a major update to Gemini 3 Deep Think to solve modern challenges across science, research and engineering. Today, we\u2019re releasing the upgraded core intelligence that makes those breakthroughs possible: Gemini 3.1 Pro.\r\n\r\n**Update**: In [What happens if AI labs train for pelicans riding bicycles?](https://simonwillison.net/2025/nov/13/training-for-pelicans-riding-bicycles/) last November I said:\r\n\r\n> If a model finally comes out that produces an excellent SVG of a pelican riding a bicycle you can bet I\u2019m going to test it on all manner of creatures riding all sorts of transportation devices.\r\n\r\nGoogle's Gemini Lead Jeff Dean [tweeted this video](https://x.com/JeffDean/status/2024525132266688757) featuring an animated pelican riding a bicycle, plus a frog on a penny-farthing and a giraffe driving a tiny car and an ostrich on roller skates and a turtle kickflipping a skateboard and a dachshund driving a stretch limousine.\r\n\r\n<video style=\"margin-bottom: 1em\" poster=\"https://static.simonwillison.net/static/2026/gemini-animated-pelicans.jpg\" muted controls preload=\"none\" style=\"max-width: 100%\">\r\n  <source src=\"https://static.simonwillison.net/static/2026/gemini-animated-pelicans.mp4\" type=\"video/mp4\">\r\n</video>\r\n\r\nI've been saying for a while that I wish AI labs would highlight things that their new models can do that their older models could not, so top marks to the Gemini team for this video.\r\n\r\n**Update 2**: I used `llm-gemini` to run my [more detailed Pelican prompt](https://simonwillison.net/2025/Nov/18/gemini-3/#and-a-new-pelican-benchmark), with [this result](https://gist.github.com/simonw/a3bdd4ec9476ba9e9ba7aa61b46d8296):\r\n\r\n![Flat-style illustration of a brown pelican riding a teal bicycle with dark blue-rimmed wheels against a plain white background. Unlike the previous image's white cartoon pelican, this pelican has realistic brown plumage with detailed feather patterns, a dark maroon head, yellow eye, and a large pink-tinged pouch bill. The bicycle is a simpler design without a basket, and the scene lacks the colorful background elements like the sun, clouds, road, hills, cap, and scarf from the first illustration, giving it a more minimalist feel.](https://static.simonwillison.net/static/2026/gemini-3.1-pro-pelican-2.png)\r\n\r\nFrom the SVG comments:\r\n\r\n    <!-- Pouch Gradient (Breeding Plumage: Red to Olive/Green) -->\r\n    ...\r\n    <!-- Neck Gradient (Breeding Plumage: Chestnut Nape, White/Yellow Front) -->"
        },
        {
          "type": "note",
          "id": 271,
          "created": "2026-02-19T23:48:35+00:00",
          "title": "Recovering lost code",
          "body": "Reached the stage of parallel agent psychosis where I've lost a whole feature - I know I had it yesterday, but I can't seem to find the branch or worktree or cloud instance or checkout with it in.\r\n\r\n... found it! Turns out I'd been hacking on a random prototype in `/tmp` and then my computer crashed and rebooted and I lost the code... but it's all still there in `~/.claude/projects/` session logs and Claude Code can extract it out and spin up the missing feature again."
        },
        {
          "type": "quotation",
          "id": 2031,
          "created": "2026-02-20T07:13:19+00:00",
          "title": "Thariq Shihipar",
          "body": "Long running agentic products like Claude Code are made feasible by prompt caching which allows us to reuse computation from previous roundtrips and significantly decrease latency and cost. [...]\r\n\r\nAt Claude Code, we build our entire harness around prompt caching. A high prompt cache hit rate decreases costs and helps us create more generous rate limits for our subscription plans, so we run alerts on our prompt cache hit rate and declare SEVs if they're too low. - Thariq Shihipar"
        },
        {
          "type": "blogmark",
          "id": 9305,
          "created": "2026-02-20T17:12:55+00:00",
          "title": "ggml.ai joins Hugging Face to ensure the long-term progress of Local AI - @ggerganov",
          "body": "I don't normally cover acquisition news like this, but I have some thoughts.\r\n\r\nIt's hard to overstate the impact Georgi Gerganov has had on the local model space. Back in March 2023 his release of [llama.cpp](https://github.com/ggml-org/llama.cpp) made it possible to run a local LLM on consumer hardware. The [original README](https://github.com/ggml-org/llama.cpp/blob/775328064e69db1ebd7e19ccb59d2a7fa6142470/README.md?plain=1#L7) said:\r\n\r\n> The main goal is to run the model using 4-bit quantization on a MacBook. [...] This was hacked in an evening - I have no idea if it works correctly.\r\n\r\nI wrote about trying llama.cpp out at the time in [Large language models are having their Stable Diffusion moment](https://simonwillison.net/2023/Mar/11/llama/#llama-cpp):\r\n\r\n> I used it to run the 7B LLaMA model on my laptop last night, and then this morning upgraded to the 13B model\u2014the one that Facebook claim is competitive with GPT-3.\r\n\r\nMeta's [original LLaMA release](https://github.com/meta-llama/llama/tree/llama_v1) depended on PyTorch and their [FairScale](https://github.com/facebookresearch/fairscale) PyTorch extension for running on multiple GPUs, and required CUDA and NVIDIA hardware. Georgi's work opened that up to a much wider range of hardware and kicked off the local model movement that has continued to grow since then.\r\n\r\nHugging Face are already responsible for the incredibly influential [Transformers](https://github.com/huggingface/transformers) library used by the majority of LLM releases today. They've proven themselves a good steward for that open source project, which makes me optimistic for the future of llama.cpp and related projects.\r\n\r\nThis section from the announcement looks particularly promising:\r\n\r\n> Going forward, our joint efforts will be geared towards the following objectives:\r\n>\r\n> - Towards seamless \"single-click\" integration with the [transformers](https://github.com/huggingface/transformers) library. The `transformers` framework has established itself as the 'source of truth' for AI model definitions. Improving the compatibility between the transformers and the ggml ecosystems is essential for wider model support and quality control.\r\n> - Better packaging and user experience of ggml-based software. As we enter the phase in which local inference becomes a meaningful and competitive alternative to cloud inference, it is crucial to improve and simplify the way in which casual users deploy and access local models. We will work towards making llama.cpp ubiquitous and readily available everywhere, and continue partnering with great downstream projects.\r\n\r\nGiven the influence of Transformers, this closer integration could lead to model releases that are compatible with the GGML ecosystem out of the box. That would be a big win for the local model ecosystem.\r\n\r\nI'm also excited to see investment in \"packaging and user experience of ggml-based software\". This has mostly been left to tools like [Ollama](https://ollama.com) and [LM Studio](https://lmstudio.ai). ggml-org released [LlamaBarn](https://github.com/ggml-org/LlamaBarn) last year - \"a macOS menu bar app for running local LLMs\" - and I'm hopeful that further investment in this area will result in more high quality open source tools for running local models from the team best placed to deliver them."
        },
        {
          "type": "blogmark",
          "id": 9306,
          "created": "2026-02-20T22:10:04+00:00",
          "title": "Taalas serves Llama 3.1 8B at 17,000 tokens/second - Hacker News",
          "body": "This new Canadian hardware startup just announced their first product - a custom hardware implementation of the Llama 3.1 8B model (from [July 2024](https://simonwillison.net/2024/Jul/23/introducing-llama-31/)) that can run at a staggering 17,000 tokens/second.\r\n\r\nI was going to include a video of their demo but it's so fast it would look more like a screenshot. You can try it out at [chatjimmy.ai](https://chatjimmy.ai).\r\n\r\nThey describe their Silicon Llama as \u201caggressively quantized, combining 3-bit and 6-bit parameters.\u201d Their next generation will use 4-bit - presumably they have quite a long lead time for baking out new models!"
        },
        {
          "type": "entry",
          "id": 9141,
          "created": "2026-02-20T23:47:10+00:00",
          "title": "Adding TILs, releases, museums, tools and research to my blog",
          "body": "<p>I've been wanting to add indications of my various other online activities to my blog for a while now. I just turned on a new feature I'm calling \"beats\" (after story beats, naming this was hard!) which adds five new types of content to my site, all corresponding to activity elsewhere.</p>\r\n<p>Here's what beats look like:</p>\r\n<p><img src=\"https://static.simonwillison.net/static/2026/three-beats.jpg\" alt=\"Screenshot of a fragment of a page showing three entries from 30th Dec 2025. First: [RELEASE] &quot;datasette-turnstile 0.1a0 \u2014 Configurable CAPTCHAs for Datasette paths usin\u2026&quot; at 7:23 pm. Second: [TOOL] &quot;Software Heritage Repository Retriever \u2014 Download archived Git repositories f\u2026&quot; at 11:41 pm. Third: [TIL] &quot;Downloading archived Git repositories from archive.softwareheritage.org \u2014 \u2026&quot; at 11:43 pm.\" style=\"max-width: 100%;\" /></p>\r\n<p>Those three are from <a href=\"https://simonwillison.net/2025/Dec/30/\">the 30th December 2025</a> archive page.</p>\r\n<p>Beats are little inline links with badges that fit into different content timeline views around my site, including the homepage, search and archive pages.</p>\r\n<p>There are currently five types of beats:</p>\r\n<ul>\r\n<li>\r\n<a href=\"https://simonwillison.net/elsewhere/release/\">Releases</a> are GitHub releases of my many different open source projects, imported from <a href=\"https://github.com/simonw/simonw/blob/main/releases_cache.json\">this JSON file</a> that was constructed <a href=\"https://simonwillison.net/2020/Jul/10/self-updating-profile-readme/\">by GitHub Actions</a>.</li>\r\n<li>\r\n<a href=\"https://simonwillison.net/elsewhere/til/\">TILs</a> are the posts from my <a href=\"https://til.simonwillison.net/\">TIL blog</a>, imported using <a href=\"https://github.com/simonw/simonwillisonblog/blob/f883b92be23892d082de39dbada571e406f5cfbf/blog/views.py#L1169\">a SQL query over JSON and HTTP</a> against the Datasette instance powering that site.</li>\r\n<li>\r\n<a href=\"https://simonwillison.net/elsewhere/museum/\">Museums</a> are new posts on my <a href=\"https://www.niche-museums.com/\">niche-museums.com</a> blog, imported from <a href=\"https://github.com/simonw/museums/blob/909bef71cc8d336bf4ac1f13574db67a6e1b3166/plugins/export.py\">this custom JSON feed</a>.</li>\r\n<li>\r\n<a href=\"https://simonwillison.net/elsewhere/tool/\">Tools</a> are HTML and JavaScript tools I've vibe-coded on my <a href=\"https://tools.simonwillison.net/\">tools.simonwillison.net</a> site, as described in <a href=\"https://simonwillison.net/2025/Dec/10/html-tools/\">Useful patterns for building HTML tools</a>.</li>\r\n<li>\r\n<a href=\"https://simonwillison.net/elsewhere/research/\">Research</a> is for AI-generated research projects, hosted in my <a href=\"https://github.com/simonw/research\">simonw/research repo</a> and described in <a href=\"https://simonwillison.net/2025/Nov/6/async-code-research/\">Code research projects with async coding agents like Claude Code and Codex</a>.</li>\r\n</ul>\r\n<p>That's five different custom integrations to pull in all of that data. The good news is that this kind of integration project is the kind of thing that coding agents <em>really</em> excel at. I knocked most of the feature out in a single morning while working in parallel on various other things.</p>\r\n<p>I didn't have a useful structured feed of my Research projects, and it didn't matter because I gave Claude Code a link to <a href=\"https://raw.githubusercontent.com/simonw/research/refs/heads/main/README.md\">the raw Markdown README</a> that lists them all and it <a href=\"https://github.com/simonw/simonwillisonblog/blob/f883b92be23892d082de39dbada571e406f5cfbf/blog/importers.py#L77-L80\">spun up a parser regex</a>. Since I'm responsible for both the source and the destination I'm fine with a brittle solution that would be too risky against a source that I don't control myself.</p>\r\n<p>Claude also handled all of the potentially tedious UI integration work with my site, making sure the new content worked on all of my different page types and was handled correctly by my <a href=\"https://simonwillison.net/2017/Oct/5/django-postgresql-faceted-search/\">faceted search engine</a>.</p>\r\n<h4 id=\"prototyping-with-claude-artifacts\">Prototyping with Claude Artifacts</h4>\r\n<p>I actually prototyped the initial concept for beats in regular Claude - not Claude Code - taking advantage of the fact that it can clone public repos from GitHub these days. I started with:</p>\r\n<blockquote>\r\n<p><code>Clone simonw/simonwillisonblog and tell me about the models and views</code></p>\r\n</blockquote>\r\n<p>And then later in the brainstorming session said:</p>\r\n<blockquote>\r\n<p><code>use the templates and CSS in this repo to create a new artifact with all HTML and CSS inline that shows me my homepage with some of those inline content types mixed in</code></p>\r\n</blockquote>\r\n<p>After some iteration we got to <a href=\"https://gisthost.github.io/?c3f443cc4451cf8ce03a2715a43581a4/preview.html\">this artifact mockup</a>, which was enough to convince me that the concept had legs and was worth handing over to full <a href=\"https://code.claude.com/docs/en/claude-code-on-the-web\">Claude Code for web</a> to implement.</p>\r\n<p>If you want to see how the rest of the build played out the most interesting PRs are <a href=\"https://github.com/simonw/simonwillisonblog/pull/592\">Beats #592</a> which implemented the core feature and <a href=\"https://github.com/simonw/simonwillisonblog/pull/595/changes\">Add Museums Beat importer #595</a> which added the Museums content type.</p>"
        },
        {
          "type": "blogmark",
          "id": 9307,
          "created": "2026-02-21T00:37:45+00:00",
          "title": "Andrej Karpathy talks about \"Claws\" - ",
          "body": "Andrej Karpathy tweeted a mini-essay about buying a Mac Mini (\"The apple store person told me they are selling like hotcakes and everyone is confused\") to tinker with Claws:\r\n\r\n> I'm definitely a bit sus'd to run OpenClaw specifically [...] But I do love the concept and I think that just like LLM agents were a new layer on top of LLMs, Claws are now a new layer on top of LLM agents, taking the orchestration, scheduling, context, tool calls and a kind of persistence to a next level.\r\n>\r\n> Looking around, and given that the high level idea is clear, there are a lot of smaller Claws starting to pop out. For example, on a quick skim NanoClaw looks really interesting in that the core engine is ~4000 lines of code (fits into both my head and that of AI agents, so it feels manageable, auditable, flexible, etc.) and runs everything in containers by default. [...]\r\n>\r\n> Anyway there are many others - e.g. nanobot, zeroclaw, ironclaw, picoclaw (lol @ prefixes). [...]\r\n>\r\n> Not 100% sure what my setup ends up looking like just yet but Claws are an awesome, exciting new layer of the AI stack.\r\n\r\nAndrej has an ear for fresh terminology (see [vibe coding](https://simonwillison.net/2025/Mar/19/vibe-coding/), [agentic engineering](https://simonwillison.net/2026/Feb/11/glm-5/)) and I think he's right about this one, too: \"**Claw**\" is becoming a term of art for the entire category of OpenClaw-like agent systems - AI agents that generally run on personal hardware, communicate via messaging protocols and can both act on direct instructions and schedule tasks.\r\n\r\nIt even comes with an established emoji \ud83e\udd9e"
        },
        {
          "type": "quotation",
          "id": 2032,
          "created": "2026-02-21T01:30:21+00:00",
          "title": "Thibault Sottiaux",
          "body": "We\u2019ve made GPT-5.3-Codex-Spark about 30% faster. It is now serving at over 1200 tokens per second. - Thibault Sottiaux"
        },
        {
          "type": "blogmark",
          "id": 9308,
          "created": "2026-02-22T15:53:43+00:00",
          "title": "How I think about Codex - ",
          "body": "Gabriel Chua (Developer Experience Engineer for APAC at OpenAI) provides his take on the confusing terminology behind the term \"Codex\", which can refer to a bunch of of different things within the OpenAI ecosystem:\r\n\r\n> In plain terms, Codex is OpenAI\u2019s software engineering agent, available through multiple interfaces, and an agent is a model plus instructions and tools, wrapped in a runtime that can execute tasks on your behalf. [...]\r\n> \r\n> At a high level, I see Codex as three parts working together:\r\n>\r\n> *Codex = Model + Harness + Surfaces* [...]\r\n>\r\n> - Model + Harness = the Agent\r\n> - Surfaces = how you interact with the Agent\r\n\r\nHe defines the harness as \"the collection of instructions and tools\", which is notably open source and lives in the [openai/codex](https://github.com/openai/codex) repository.\r\n\r\nGabriel also provides the first acknowledgment I've seen from an OpenAI insider that the Codex model family are directly trained for the Codex harness:\r\n\r\n> Codex models are trained in the presence of the harness. Tool use, execution loops, compaction, and iterative verification aren\u2019t bolted on behaviors \u2014 they\u2019re part of how the model learns to operate. The harness, in turn, is shaped around how the model plans, invokes tools, and recovers from failure."
        },
        {
          "type": "blogmark",
          "id": 9309,
          "created": "2026-02-22T23:54:39+00:00",
          "title": "London Stock Exchange: Raspberry Pi Holdings plc - ",
          "body": "Striking graph illustrating stock in the UK Raspberry Pi holding company spiking on Tuesday:\r\n\r\n![Stock price line chart for RASPBERRY PI showing a 3-month daily view from 24 Nov to 16 Feb. The price trends downward from around 325 to a low near 260, then sharply spikes upward. A tooltip highlights \"RASPBERRY PI: 415.00, 16/02/2026\". The y-axis ranges from 240 to 420.](https://static.simonwillison.net/static/2026/raspberry-pi-plc.jpg)\r\n\r\nThe Telegraph [credited excitement around OpenClaw](https://finance.yahoo.com/news/british-computer-maker-soars-ai-141836041.html): \r\n\r\n> Raspberry Pi's stock price has surged 30pc in two days, amid chatter on social media that the company's tiny computers can be used to power a popular AI chatbot.\r\n>\r\n> Users have turned to Raspberry Pi's small computers to run a technology known as OpenClaw, [a viral AI personal assistant](https://www.telegraph.co.uk/business/2026/02/07/i-built-a-whatsapp-bot-and-now-it-runs-my-entire-life/). A flood of posts about the practice have been viewed millions of times since the weekend.\r\n\r\nReuters [also credit a stock purchase by CEO Eben Upton](https://finance.yahoo.com/news/raspberry-pi-soars-40-ceo-151342904.html):\r\n\r\n> Shares in Raspberry Pi rose as much as 42% on Tuesday in \u200ca record two\u2011day rally after CEO Eben Upton bought \u200cstock in the beaten\u2011down UK computer hardware firm, halting a months\u2011long slide, \u200bas chatter grew that its products could benefit from low\u2011cost artificial\u2011intelligence projects.\r\n>\r\n> Two London traders said the driver behind the surge was not clear, though the move followed a filing showing Upton bought \u200cabout 13,224 pounds \u2060worth of shares at around 282 pence each on Monday."
        },
        {
          "type": "blogmark",
          "id": 9310,
          "created": "2026-02-22T23:58:43+00:00",
          "title": "The Claude C Compiler: What It Reveals About the Future of Software - ",
          "body": "On February 5th Anthropic's Nicholas Carlini wrote about a project to use [parallel Claudes to build a C compiler](https://www.anthropic.com/engineering/building-c-compiler) on top of the brand new Opus 4.6\r\n\r\nChris Lattner (Swift, LLVM, Clang, Mojo) knows more about C compilers than most. He just published this review of the code.\r\n\r\nSome points that stood out to me:\r\n\r\n> - Good software depends on judgment, communication, and clear abstraction. AI has amplified this.\r\n> - AI coding is automation of implementation, so design and stewardship become more important.\r\n> - Manual rewrites and translation work are becoming AI-native tasks, automating a large category of engineering effort.\r\n\r\nChris is generally impressed with CCC (the Claude C Compiler):\r\n\r\n> Taken together, CCC looks less like an experimental research compiler and more like a competent textbook implementation, the sort of system a strong undergraduate team might build early in a project before years of refinement. That alone is remarkable.\r\n\r\nIt's a long way from being a production-ready compiler though:\r\n\r\n> Several design choices suggest optimization toward passing tests rather than building general abstractions like a human would. [...] These flaws are informative rather than surprising, suggesting that current AI systems excel at assembling known techniques and optimizing toward measurable success criteria, while struggling with the open-ended generalization required for production-quality systems.\r\n\r\nThe project also leads to deep open questions about how agentic engineering interacts with licensing and IP for both open source and proprietary code:\r\n\r\n> If AI systems trained on decades of publicly available code can reproduce familiar structures, patterns, and even specific implementations, where exactly is the boundary between learning and copying?"
        },
        {
          "type": "chapter",
          "id": 2,
          "created": "2026-02-23T07:12:28+00:00",
          "title": "Red/green TDD",
          "body": "\"**Use red/green TDD**\" is a pleasingly succinct way to get better results out of a coding agent.\r\n\r\nTDD stands for Test Driven Development. It's a programming style where you ensure every piece of code you write is accompanied by automated tests that demonstrate the code works.\r\n\r\nThe most disciplined form of TDD is test-first development. You write the automated tests first, confirm that they fail, then iterate on the implementation until the tests pass.\r\n\r\nThis turns out to be a *fantastic* fit for coding agents. A significant risk with coding agents is that they might write code that doesn't work, or build code that is unnecessary and never gets used, or both.\r\n\r\nTest-first development helps protect against both of these common mistakes, and also ensures a robust automated test suite that protects against future regressions. As projects grow the chance that a new change might break an existing feature grows with them. A comprehensive test suite is by far the most effective way to keep those features working.\r\n\r\nIt's important to confirm that the tests fail before implementing the code to make them pass. If you skip that step you risk building a test that passes already, hence failing to exercise and confirm your new implementation.\r\n\r\nThat's what \"red/green\" means: the red phase watches the tests fail, then the green phase confirms that they now pass.\r\n\r\nEvery good model understands \"red/green TDD\" as a shorthand for the much longer \"use test driven development, write the tests first, confirm that the tests fail before you implement the change that gets them to pass\".\r\n\r\nExample prompt:\r\n````markdown-copy\r\nBuild a Python function to extract headers from a markdown string. Use red/green TDD.\r\n````"
        },
        {
          "type": "quotation",
          "id": 2033,
          "created": "2026-02-23T13:01:13+00:00",
          "title": "Summer Yue",
          "body": "Nothing humbles you like telling your OpenClaw \u201cconfirm before acting\u201d and watching it speedrun deleting your inbox. I couldn\u2019t stop it from my phone. I had to RUN to my Mac mini like I was defusing a bomb.\r\n\r\n![Screenshot of a WhatsApp or similar messaging conversation showing a user repeatedly trying to stop an AI agent (appearing to be \"OpenClaw\") that is autonomously executing terminal commands to mass-delete emails. The agent sends messages prefixed with \"\ud83d\udee0 Exec:\" running commands like \"gog gmail search 'in:inbox' --max 20 -a\" and \"# Nuclear option: trash EVERYTHING in inbox older than Feb 15 that isn't already in my keep list\", while the user urgently responds with \"What's going on? Can you describe what you're doing\" at 6:00 PM, \"Do not do that\" at 6:01 PM, \"Stop don't do anything\" at 6:02 PM, and \"STOP OPENCLAW\" at 6:03 PM. The agent continues executing commands including setting ACCT variables with redacted email addresses and commenting \"# Get ALL remaining old stuff and nuke it\" and \"# Keep looping until we clear everything old\", ignoring the user's repeated requests to stop. Email addresses and account details are partially redacted with gray blocks.](https://static.simonwillison.net/static/2026/stop-openclaw.jpg)\r\n\r\nI said \u201cCheck this inbox too and suggest what you would archive or delete, don\u2019t action until I tell you to.\u201d This has been working well for my toy inbox, but my real inbox was too huge and triggered compaction. During the compaction, it lost my original instruction \ud83e\udd26\u200d\u2640\ufe0f - Summer Yue"
        },
        {
          "type": "note",
          "id": 272,
          "created": "2026-02-23T13:11:57+00:00",
          "title": "Reply guy",
          "body": "The latest scourge of Twitter is AI bots that reply to your tweets with generic, banal commentary slop, often accompanied by a question to \"drive engagement\" and waste as much of your time as possible.\r\n\r\nI just [found out](https://twitter.com/simonw/status/2025918174894673986) that the category name for this genre of software is **reply guy** tools. Amazing."
        },
        {
          "type": "quotation",
          "id": 2034,
          "created": "2026-02-23T16:00:32+00:00",
          "title": "Paul Ford",
          "body": "The paper asked me to explain vibe coding, and I did so, because I think something big is coming there, and I'm deep in, and I worry that normal people are not able to see it and I want them to be prepared. But people can't just read something and hate you quietly; they can't see that you have provided them with a utility or a warning; they need their screech. You are distributed to millions of people, and become the local proxy for the emotions of maybe dozens of people, who disagree and demand your attention, and because you are the one in the paper you need to welcome them with a pastor's smile and deep empathy, and if you speak a word in your own defense they'll screech even louder. - Paul Ford"
        },
        {
          "type": "chapter",
          "id": 4,
          "created": "2026-02-23T16:20:42+00:00",
          "title": "Writing code is cheap now",
          "body": "The biggest challenge in adopting agentic engineering practices is getting comfortable with the consequences of the fact that *writing code is cheap now*.\r\n\r\nCode has always been expensive. Producing a few hundred lines of clean, tested code takes most software developers a full day or more. Many of our engineering habits, at both the macro and micro level, are built around this core constraint.\r\n\r\nAt the macro level we spend a great deal of time designing, estimating and planning out projects, to ensure that our expensive coding time is spent as efficiently as possible. Product feature ideas are evaluated in terms of how much value they can provide *in exchange for that time* - a feature needs to earn its development costs many times over to be worthwhile!\r\n\r\nAt the micro level we make hundreds of decisions a day predicated on available time and anticipated tradeoffs. Should I refactor that function to be slightly more elegant if it adds an extra hour of coding time? How about writing documentation? Is it worth adding a test for this edge case? Can I justify building a debug interface for this?\r\n\r\nCoding agents dramatically drop the cost of typing code into the computer, which disrupts *so many* of our existing personal and organizational intuitions about which trade-offs make sense.\r\n\r\nThe ability to run parallel agents makes this even harder to evaluate, since one human engineer can now be implementing, refactoring, testing and documenting code in multiple places at the same time.\r\n\r\n<h2 id=\"good-code\">Good code still has a cost</h2>\r\n\r\nDelivering new code has dropped in price to almost free... but delivering *good* code remains significantly more expensive than that.\r\n\r\nHere's what I mean by \"good code\":\r\n\r\n- The code works. It does what it's meant to do, without bugs.\r\n- We *know the code works*. We've taken steps to confirm to ourselves and to others that the code is fit for purpose.\r\n- It solves the right problem.\r\n- It handles error cases gracefully and predictably: it doesn't just consider the happy path. Errors should provide enough information to help future maintainers understand what went wrong.\r\n- It\u2019s simple and minimal - it does only what\u2019s needed, in a way that both humans and machines can understand now and maintain in the future.\r\n- It's protected by tests. The tests show that it works now and act as a regression suite to avoid it quietly breaking in the future.\r\n- It's documented at an appropriate level, and that documentation reflects the current state of the system - if the code changes an existing behavior the existing documentation needs to be updated to match.\r\n- The design affords future changes. It's important to maintain [YAGNI](https://en.wikipedia.org/wiki/You_aren%27t_gonna_need_it) - code with added complexity to anticipate future changes that may never come is often bad code - but it's also important not to write code that makes future changes much harder than they should be.\r\n- All of the other relevant \"ilities\" - accessibility, testability, reliability, security, maintainability, observability, scalability, usability - the non-functional quality measures that are appropriate for the particular class of software being developed.\r\n \r\nCoding agent tools can help with most of this, but there is still a substantial burden on the developer driving those tools to ensure that the produced code is good code for the subset of good that's needed for the current project.\r\n\r\n## We need to build new habits\r\n\r\nThe challenge is to develop new personal and organizational habits that respond to the affordances and opportunities of agentic engineering. \r\n\r\nThese best practices are still being figured out across our industry. I'm still figuring them out myself.\r\n\r\nFor now I think the best we can do is to second guess ourselves: any time our instinct says \"don't build that, it's not worth the time\" fire off a prompt anyway, in an asynchronous agent session where the worst that can happen is you check ten minutes later and find that it wasn't worth the tokens."
        },
        {
          "type": "entry",
          "id": 9142,
          "created": "2026-02-23T17:43:02+00:00",
          "title": "Writing about Agentic Engineering Patterns",
          "body": "<p>I've started a new project to collect and document <strong><a href=\"https://simonwillison.net/guides/agentic-engineering-patterns/\">Agentic Engineering Patterns</a></strong> - coding practices and patterns to help get the best results out of this new era of coding agent development we find ourselves entering.</p>\r\n<p>I'm using <strong>Agentic Engineering</strong> to refer to building software using coding agents - tools like Claude Code and OpenAI Codex, where the defining feature is that they can both generate and <em>execute</em> code - allowing them to test that code and iterate on it independently of turn-by-turn guidance from their human supervisor.</p>\r\n<p>I think of <strong>vibe coding</strong> using its <a href=\"https://simonwillison.net/2025/Mar/19/vibe-coding/\">original definition</a> of coding where you pay no attention to the code at all, which today is often associated with non-programmers using LLMs to write code.</p>\r\n<p>Agentic Engineering represents the other end of the scale: professional software engineers using coding agents to improve and accelerate their work by amplifying their existing expertise.</p>\r\n<p>There is so much to learn and explore about this new discipline! I've already published a lot <a href=\"https://simonwillison.net/tags/ai-assisted-programming/\">under my ai-assisted-programming tag</a> (345 posts and counting) but that's been relatively unstructured. My new goal is to produce something that helps answer the question \"how do I get good results out of this stuff\" all in one place.</p>\r\n<p>I'll be developing and growing this project here on my blog as a series of chapter-shaped patterns, loosely inspired by the format popularized by <a href=\"https://en.wikipedia.org/wiki/Design_Patterns\">Design Patterns: Elements of Reusable Object-Oriented Software</a> back in 1994.</p>\r\n<p>I published the first two chapters today:</p>\r\n<ul>\r\n<li>\r\n<strong><a href=\"https://simonwillison.net/guides/agentic-engineering-patterns/code-is-cheap/\">Writing code is cheap now</a></strong> talks about the central challenge of agentic engineering: the cost to churn out initial working code has dropped to almost nothing, how does that impact our existing intuitions about how we work, both individually and as a team?</li>\r\n<li>\r\n<strong><a href=\"https://simonwillison.net/guides/agentic-engineering-patterns/red-green-tdd/\">Red/green TDD</a></strong> describes how test-first development helps agents write more succinct and reliable code with minimal extra prompting.</li>\r\n</ul>\r\n<p>I hope to add more chapters at a rate of 1-2 a week. I don't really know when I'll stop, there's a lot to cover!</p>\r\n<h4 id=\"written-by-me-not-by-an-llm\">Written by me, not by an LLM</h4>\r\n<p>I have a strong personal policy of not publishing AI-generated writing under my own name. That policy will hold true for Agentic Engineering Patterns as well. I'll be using LLMs for proofreading and fleshing out example code and all manner of other side-tasks, but the words you read here will be my own.</p>\r\n<h4 id=\"chapters-and-guides\">Chapters and Guides</h4>\r\n<p>Agentic Engineering Patterns isn't exactly <em>a book</em>, but it's kind of book-shaped. I'll be publishing it on my site using a new shape of content I'm calling a <em>guide</em>. A guide is a collection of chapters, where each chapter is effectively a blog post with a less prominent date that's designed to be updated over time, not frozen at the point of first publication.</p>\r\n<p>Guides and chapters are my answer to the challenge of publishing \"evergreen\" content on a blog. I've been trying to find a way to do this for a while now. This feels like a format that might stick.</p>\r\n\r\n<p>If you're interested in the implementation you can find the code in the <a href=\"https://github.com/simonw/simonwillisonblog/blob/b9cd41a0ac4a232b2a6c90ca3fff9ae465263b02/blog/models.py#L262-L280\">Guide</a>, <a href=\"https://github.com/simonw/simonwillisonblog/blob/b9cd41a0ac4a232b2a6c90ca3fff9ae465263b02/blog/models.py#L349-L405\">Chapter</a> and <a href=\"https://github.com/simonw/simonwillisonblog/blob/b9cd41a0ac4a232b2a6c90ca3fff9ae465263b02/blog/models.py#L408-L423\">ChapterChange</a> models and the <a href=\"https://github.com/simonw/simonwillisonblog/blob/b9cd41a0ac4a232b2a6c90ca3fff9ae465263b02/blog/views.py#L775-L923\">associated Django views</a>, almost all of which was written by Claude Opus 4.6 running in Claude Code for web accessed via my iPhone.</p>"
        },
        {
          "type": "blogmark",
          "id": 9311,
          "created": "2026-02-23T18:52:53+00:00",
          "title": "Ladybird adopts Rust, with help from AI - Hacker News",
          "body": "Really interesting case-study from Andreas Kling on advanced, sophisticated use of coding agents for ambitious coding projects with critical code. After a few years hoping Swift's platform support outside of the Apple ecosystem would mature they switched tracks to Rust their memory-safe language of choice, starting with an AI-assisted port of a critical library:\r\n\r\n> Our first target was **LibJS** , Ladybird's JavaScript engine. The lexer, parser, AST, and bytecode generator are relatively self-contained and have extensive test coverage through [test262](https://github.com/tc39/test262), which made them a natural starting point.\r\n>\r\n> I used [Claude Code](https://docs.anthropic.com/en/docs/claude-code) and [Codex](https://openai.com/codex/) for the translation. This was human-directed, not autonomous code generation. I decided what to port, in what order, and what the Rust code should look like. It was hundreds of small prompts, steering the agents where things needed to go. [...]\r\n>\r\n> The requirement from the start was byte-for-byte identical output from both pipelines. The result was about 25,000 lines of Rust, and the entire port took about two weeks. The same work would have taken me multiple months to do by hand. We\u2019ve verified that every AST produced by the Rust parser is identical to the C++ one, and all bytecode generated by the Rust compiler is identical to the C++ compiler\u2019s output. Zero regressions across the board.\r\n\r\nHaving an existing conformance testing suite of the quality of `test262` is a huge unlock for projects of this magnitude, and the ability to compare output with an existing trusted implementation makes agentic engineering much more of a safe bet."
        },
        {
          "type": "chapter",
          "id": 5,
          "created": "2026-02-24T12:30:05+00:00",
          "title": "First run the tests",
          "body": "Automated tests are no longer optional when working with coding agents.\r\n\r\nThe old excuses for not writing them - that they're time consuming and expensive to constantly rewrite while a codebase is rapidly evolving - no longer hold when an agent can knock them into shape in just a few minutes.\r\n\r\nThey're also *vital* for ensuring AI-generated code does what it claims to do.  If the code has never been executed it's pure luck if it actually works when deployed to production.\r\n\r\nTests are also a great tool to help get an agent up to speed with an existing codebase. Watch what happens when you ask Claude Code or similar about an existing feature - the chances are high that they'll find and read the relevant tests.\r\n\r\nAgents are already biased towards testing, but the presence of an existing test suite will almost certainly push the agent into testing new changes that it makes.\r\n\r\nAny time I start a new session with an agent against an existing project I'll start by prompting a variant of the following:\r\n````markdown-copy\r\nFirst run the tests\r\n````\r\nFor my Python projects I have [pyproject.toml set up](https://til.simonwillison.net/uv/dependency-groups) such that I can prompt this instead:\r\n````markdown-copy\r\nRun \"uv run pytest\"\r\n````\r\nThese four word prompts serve several purposes:\r\n\r\n1. It tells the agent that there is a test suite and forces it to figure out how to run the tests. This makes it almost certain that the agent will run the tests in the future to ensure it didn't break anything.\r\n2. Most test harnesses will give the agent a rough indication of how many tests they are. This can act as a proxy for how large and complex the project is, and also hints that the agent should search the tests themselves if they want to learn more.\r\n3. It puts the agent in a testing mindset. Having run the tests it's natural for it to then expand them with its own tests later on.\r\n\r\nSimilar to [\"Use red/green TDD\"](https://simonwillison.net/guides/agentic-engineering-patterns/red-green-tdd/), \"First run the tests\" provides a four word prompt that encompasses a substantial amount of software engineering discipline that's already baked into the models."
        },
        {
          "type": "blogmark",
          "id": 9312,
          "created": "2026-02-24T16:10:06+00:00",
          "title": "go-size-analyzer - Datadog: How we reduced the size of our Agent Go binaries by up to 77%",
          "body": "The Go ecosystem is *really* good at tooling. I just learned about this tool for analyzing the size of Go binaries using a pleasing treemap view of their bundled dependencies.\r\n\r\nYou can install and run the tool locally, but it's also compiled to WebAssembly and hosted at [gsa.zxilly.dev](https://gsa.zxilly.dev/) - which means you can open compiled Go binaries and analyze them directly in your browser.\r\n\r\nI tried it with a 8.1MB macOS compiled copy of my Go [Showboat](https://github.com/simonw/showboat) tool and got this:\r\n\r\n![Treemap visualization of a Go binary named \"showboat\" showing size breakdown across four major categories: \"Unknown Sections Size\" (containing __rodata __TEXT, __rodata __DATA_CONST, __data __DATA, and Debug Sections Size with __zdebug_line __DWARF, __zdebug_loc __DWARF, __zdebug_info __DWARF), \"Std Packages Size\" (showing standard library packages like runtime, net, crypto, reflect, math, os, fmt, strings, syscall, context, and many subpackages such as crypto/tls, crypto/x509, net/http, with individual .go files visible at deeper levels), \"Main Packages Size\" (showing main, showboat, cmd), and \"Generated Packages Size\" (showing <autogenerated>). A tooltip is visible over __zdebug_line __DWARF showing: Section: __zdebug_line __DWARF, Size: 404.44 KB, File Size: 404.44 KB, Known size: 0 B, Unknown size: 404.44 KB, Offset: 0x52814a \u2013 0x58d310, Address: 0x1005c014a \u2013 0x1005c5310, Memory: false, Debug: true. The treemap uses green for main/generated packages, blue-gray for unknown sections, and shades of purple/pink for standard library packages.](https://static.simonwillison.net/static/2026/showboat-treemap.jpg)"
        },
        {
          "type": "chapter",
          "id": 6,
          "created": "2026-02-25T01:07:10+00:00",
          "title": "Linear walkthroughs",
          "body": "Sometimes it's useful to have a coding agent give you a structured walkthrough of a codebase. \r\n\r\nMaybe it's existing code you need to get up to speed on, maybe it's your own code that you've forgotten the details of, or maybe you vibe coded the whole thing and need to understand how it actually works.\r\n\r\nFrontier models with the right agent harness can construct a detailed walkthrough to help you understand how code works.\r\n\r\n## An example using Showboat and Present\r\n\r\nI recently [vibe coded a SwiftUI slide presentation app](https://simonwillison.net/2026/Feb/25/present/) on my Mac using Claude Code and Opus 4.6.\r\n\r\nI was speaking about the advances in frontier models between November 2025 and February 2026, and I like to include at least one gimmick in my talks (a [STAR moment](https://simonwillison.net/2019/Dec/10/better-presentations/) - Something They'll Always Remember). In this case I decided the gimmick would be revealing at the end of the presentation that the slide mechanism itself was an example of what vibe coding could do.\r\n\r\nI released the code [to GitHub](https://github.com/simonw/present) and then realized I didn't know anything about how it actually worked - I had prompted the whole thing into existence ([partial transcript here](https://gisthost.github.io/?bfbc338977ceb71e298e4d4d5ac7d63c) without paying any attention to the code it was writing.\r\n\r\nSo I fired up a new instance of Claude Code for web, pointed it at my repo and prompted:\r\n````markdown-copy\r\nRead the source and then plan a linear walkthrough of the code that explains how it all works in detail\r\n\r\nThen run \u201cuvx showboat \u2013help\u201d to learn showboat - use showboat to create a walkthrough.md file in the repo and build the walkthrough in there, using showboat note for commentary and showboat exec plus sed or grep or cat or whatever you need to include snippets of code you are talking about\r\n````\r\n[Showboat](https://github.com/simonw/showboat) is a tool I built to help coding agents write documents that demonstrate their work. You can see the [showboat --help output here](https://github.com/simonw/showboat/blob/main/help.txt), which is designed to give the model everything it needs to know in order to use the tool.\r\n\r\nThe `showboat note` command adds Markdown to the document. The `showboat exec` command accepts a shell command, executes it and then adds both the command and its output to the document.\r\n\r\nBy telling it to use \"sed or grep or cat or whatever you need to include snippets of code you are talking about\" I ensured that Claude Code would not manually copy snippets of code into the document, since that could introduce a risk of hallucinations or mistakes.\r\n\r\nThis worked extremely well. Here's the [document Claude Code created with Showboat](https://github.com/simonw/present/blob/main/walkthrough.md), which talks through all six `.swift` files in detail and provides a clear and actionable explanation about how the code works.\r\n\r\nI learned a great deal about how SwiftUI apps are structured and absorbed some solid details about the Swift language itself just from reading this document.\r\n\r\nIf you are concerned that LLMs might reduce the speed at which you learn new skills I strongly recommend adopting patterns like this one.  Even a ~40 minute vibe coded toy project can become an opportunity to explore new ecosystems and pick up some interesting new tricks."
        },
        {
          "type": "quotation",
          "id": 2035,
          "created": "2026-02-25T03:30:32+00:00",
          "title": "Kellan Elliott-McCrea",
          "body": "It\u2019s also reasonable for people who entered technology in the last couple of decades because it was good job, or because they enjoyed coding to look at this moment with a real feeling of loss. That feeling of loss though can be hard to understand emotionally for people my age who entered tech because we were addicted to feeling of agency it gave us. The web was objectively awful as a technology, and genuinely amazing, and nobody got into it because programming in Perl was somehow aesthetically delightful. - Kellan Elliott-McCrea"
        },
        {
          "type": "entry",
          "id": 9143,
          "created": "2026-02-25T16:46:19+00:00",
          "title": "I vibe coded my dream macOS presentation app",
          "body": "<p>I gave a talk this weekend at Social Science FOO Camp in Mountain View. The event was a classic unconference format where anyone could present a talk without needing to propose it in advance. I grabbed a slot for a talk I titled \"The State of LLMs, February 2026 edition\", subtitle \"It's all changed since November!\". I vibe coded a custom macOS app for the presentation the night before.</p>\r\n<p><img src=\"https://static.simonwillison.net/static/2026/state-of-llms.jpg\" alt=\"A sticky note on a board at FOO Camp. It reads: The state of LLMs, Feb 2026 edition - it's all changed since November! Simon Willison - the card is littered with names of new models: Qwen 3.5, DeepSeek 3.2, Sonnet 4.6, Kimi K2.5, GLM5, Opus 4.5/4.6, Gemini 3.1 Pro, Codex 5.3. The card next to it says Why do Social Scientists think they need genetics? Bill January (it's not all because of AI)\" style=\"max-width: 100%;\" /></p>\r\n<p>I've written about the last twelve months of development in LLMs in <a href=\"https://simonwillison.net/2023/Dec/31/ai-in-2023/\">December 2023</a>, <a href=\"https://simonwillison.net/2024/Dec/31/llms-in-2024/\">December 2024</a> and <a href=\"https://simonwillison.net/2025/Dec/31/the-year-in-llms/\">December 2025</a>. I also presented <a href=\"https://simonwillison.net/2025/Jun/6/six-months-in-llms/\">The last six months in LLMs, illustrated by pelicans on bicycles</a> at the AI Engineer World\u2019s Fair in June 2025. This was my first time dropping the time covered to just three months, which neatly illustrates how much the space keeps accelerating and felt appropriate given the <a href=\"https://simonwillison.net/2026/Jan/4/inflection/\">November 2025 inflection point</a>.</p>\r\n<p>(I further illustrated this acceleration by wearing a Gemini 3 sweater to the talk, which I was given a couple of weeks ago and is already out-of-date <a href=\"https://simonwillison.net/2026/Feb/19/gemini-31-pro/\">thanks to Gemini 3.1</a>.)</p>\r\n<p>I always like to have at least one gimmick in any talk I give, based on the STAR moment principle I <a href=\"https://simonwillison.net/2019/Dec/10/better-presentations/\">learned at Stanford</a> - include Something They'll Always Remember to try and help your talk stand out.</p>\r\n<p>For this talk I had two gimmicks. I built the first part of the talk around coding agent assisted data analysis of K\u0101k\u0101p\u014d breeding season (which meant I got to <a href=\"https://simonwillison.net/2026/Feb/8/kakapo-mug/\">show off my mug</a>), then did a quick tour of some new pelicans riding bicycles before ending with the reveal that the entire presentation had been presented using a new macOS app I had vibe coded in ~45 minutes the night before the talk.</p>\r\n<h4 id=\"present-app\">Present.app</h4>\r\n<p>The app is called <strong>Present</strong> - literally the first name I thought of. It's built using Swift and SwiftUI and weighs in at 355KB, or <a href=\"https://github.com/simonw/present/releases/tag/0.1a0\">76KB compressed</a>. Swift apps are tiny!</p>\r\n<p>It may have been quick to build but the combined set of features is something I've wanted for <em>years</em>.</p>\r\n<p>I usually use Keynote for presentations, but sometimes I like to mix things up by presenting using a sequence of web pages. I do this by loading up a browser window with a tab for each page, then clicking through those tabs in turn while I talk.</p>\r\n<p>This works great, but comes with a very scary disadvantage: if the browser crashes I've just lost my entire deck!</p>\r\n<p>I always have the URLs in a notes file, so I can click back to that and launch them all manually if I need to, but it's not something I'd like to deal with in the middle of a talk.</p>\r\n<p>This was <a href=\"https://gisthost.github.io/?639d3c16dcece275af50f028b32480c7/page-001.html#msg-2026-02-21T05-53-43-395Z\">my starting prompt</a>:</p>\r\n<blockquote>\r\n<p>Build a SwiftUI app for giving presentations where every slide is a URL. The app starts as a window with a webview on the right and a UI on the left for adding, removing and reordering the sequence of URLs. Then you click Play in a menu and the app goes full screen and the left and right keys switch between URLs</p>\r\n</blockquote>\r\n<p>That produced a plan. You can see <a href=\"https://gisthost.github.io/?bfbc338977ceb71e298e4d4d5ac7d63c\">the transcript that implemented that plan here</a>.</p>\r\n<p>In Present a talk is an ordered sequence of URLs, with a sidebar UI for adding, removing and reordering those URLs. That's the entirety of the editing experience.</p>\r\n<p><img src=\"https://static.simonwillison.net/static/2026/present.jpg\" alt=\"Screenshot of a macOS app window titled &quot;Present&quot; showing Google Image search results for &quot;kakapo&quot;. A web view shows a Google image search with thumbnail photos of k\u0101k\u0101p\u014d parrots with captions. A sidebar on the left shows a numbered list of URLs, mostly from simonwillison.net and static.simonwillison.net, with item 4 (https://www.google.com/search?...) highlighted in blue.\" style=\"max-width: 100%;\" /></p>\r\n<p>When you select the \"Play\" option in the menu (or hit Cmd+Shift+P) the app switches to full screen mode. Left and right arrow keys navigate back and forth, and you can bump the font size up and down or scroll the page if you need to. Hit Escape when you're done.</p>\r\n<p>Crucially, Present saves your URLs automatically any time you make a change. If the app crashes you can start it back up again and restore your presentation state.</p>\r\n<p>You can also save presentations as a <code>.txt</code> file (literally a newline-delimited sequence of URLs) and load them back up again later.</p>\r\n<h4 id=\"remote-controlled-via-my-phone\">Remote controlled via my phone</h4>\r\n<p>Getting the initial app working took so little time that I decided to get more ambitious.</p>\r\n<p>It's neat having a remote control for a presentation...</p>\r\n<p>So I prompted:</p>\r\n<blockquote>\r\n<p>Add a web server which listens on 0.0.0.0:9123 - the web server serves a single mobile-friendly page with prominent left and right buttons - clicking those buttons switches the slide left and right - there is also a button to start presentation mode or stop depending on the mode it is in.</p>\r\n</blockquote>\r\n<p>I have <a href=\"https://tailscale.com/\">Tailscale</a> on my laptop and my phone, which means I don't have to worry about Wi-Fi networks blocking access between the two devices. My phone can access <code>http://100.122.231.116:9123/</code> directly from anywhere in the world and control the presentation running on my laptop.</p>\r\n<p>It took a few more iterative prompts to get to the final interface, which looked like this:</p>\r\n<p style=\"text-align: center;\"><img src=\"https://static.simonwillison.net/static/2026/present-mobile.jpg\" alt=\"Mobile phone web browser app with large buttons, Slide 4/31 at the top, Prev, Next and Start buttons, a thin bar with a up/down scroll icon and text size + and - buttons and the current slide URL at the bottom.\" style=\"max-width: 80%;\" /></p>\r\n<p>There's a slide indicator at the top, prev and next buttons, a nice big \"Start\" button and buttons for adjusting the font size.</p>\r\n<p>The most complex feature is that thin bar next to the start button. That's a touch-enabled scroll bar - you can slide your finger up and down on it to scroll the currently visible web page up and down on the screen.</p>\r\n<p>It's <em>very</em> clunky but it works just well enough to solve the problem of a page loading with most interesting content below the fold.</p>\r\n<h4 id=\"learning-from-the-code\">Learning from the code</h4>\r\n<p>I'd already <a href=\"https://github.com/simonw/present\">pushed the code to GitHub</a> (with a big \"This app was vibe coded [...] I make no promises other than it worked on my machine!\" disclaimer) when I realized I should probably take a look at the code.</p>\r\n<p>I used this as an opportunity to document a recent pattern I've been using: asking the model to present a linear walkthrough of the entire codebase. Here's the resulting <a href=\"https://simonwillison.net/guides/agentic-engineering-patterns/linear-walkthroughs/\">Linear walkthroughs</a> pattern in my ongoing <a href=\"https://simonwillison.net/2026/Feb/23/agentic-engineering-patterns/\">Agentic Engineering Patterns guide</a>, including the prompt I used.</p>\r\n<p>The <a href=\"https://github.com/simonw/present/blob/main/walkthrough.md\">resulting walkthrough document</a> is genuinely useful. It turns out Claude Code decided to implement the web server for the remote control feature <a href=\"https://github.com/simonw/present/blob/main/walkthrough.md#request-routing\">using socket programming without a library</a>! Here's the minimal HTTP parser it used for routing:</p>\r\n<div class=\"highlight highlight-source-swift\"><pre>    <span class=\"pl-k\">private</span> <span class=\"pl-en\">func</span> route<span class=\"pl-kos\">(</span>_ raw<span class=\"pl-kos\">:</span> <span class=\"pl-smi\">String</span><span class=\"pl-kos\">)</span> <span class=\"pl-c1\">-&gt;</span> <span class=\"pl-smi\">String</span> <span class=\"pl-kos\">{</span>\r\n        <span class=\"pl-k\">let</span> <span class=\"pl-s1\">firstLine</span> <span class=\"pl-c1\">=</span> raw<span class=\"pl-kos\">.</span><span class=\"pl-en\">components</span><span class=\"pl-kos\">(</span>separatedBy<span class=\"pl-kos\">:</span> <span class=\"pl-s\">\"</span><span class=\"pl-s\">\\r</span><span class=\"pl-s\">\\n</span><span class=\"pl-s\">\"</span><span class=\"pl-kos\">)</span><span class=\"pl-kos\">.</span>first <span class=\"pl-c1\">??</span> <span class=\"pl-s\">\"</span><span class=\"pl-s\">\"</span>\r\n        <span class=\"pl-k\">let</span> <span class=\"pl-s1\">parts</span> <span class=\"pl-c1\">=</span> firstLine<span class=\"pl-kos\">.</span><span class=\"pl-en\">split</span><span class=\"pl-kos\">(</span>separator<span class=\"pl-kos\">:</span> <span class=\"pl-s\">\"</span><span class=\"pl-s\"> </span><span class=\"pl-s\">\"</span><span class=\"pl-kos\">)</span>\r\n        <span class=\"pl-k\">let</span> <span class=\"pl-s1\">path</span> <span class=\"pl-c1\">=</span> parts<span class=\"pl-kos\">.</span>count <span class=\"pl-c1\">&gt;=</span> <span class=\"pl-c1\">2</span> <span class=\"pl-c1\">?</span> <span class=\"pl-en\">String</span><span class=\"pl-kos\">(</span><span class=\"pl-en\">parts</span><span class=\"pl-kos\">[</span><span class=\"pl-c1\">1</span><span class=\"pl-kos\">]</span><span class=\"pl-kos\">)</span> <span class=\"pl-k\">:</span> <span class=\"pl-s\">\"</span><span class=\"pl-s\">/</span><span class=\"pl-s\">\"</span>\r\n\r\n        <span class=\"pl-k\">switch</span> path <span class=\"pl-kos\">{</span>\r\n        <span class=\"pl-k\">case</span> <span class=\"pl-s\">\"</span><span class=\"pl-s\">/next</span><span class=\"pl-s\">\"</span><span class=\"pl-kos\">:</span>\r\n            state<span class=\"pl-c1\"><span class=\"pl-c1\">?</span></span><span class=\"pl-kos\">.</span><span class=\"pl-en\">goToNext</span><span class=\"pl-kos\">(</span><span class=\"pl-kos\">)</span>\r\n            <span class=\"pl-k\">return</span> <span class=\"pl-en\">jsonResponse</span><span class=\"pl-kos\">(</span><span class=\"pl-s\">\"</span><span class=\"pl-s\">ok</span><span class=\"pl-s\">\"</span><span class=\"pl-kos\">)</span>\r\n        <span class=\"pl-k\">case</span> <span class=\"pl-s\">\"</span><span class=\"pl-s\">/prev</span><span class=\"pl-s\">\"</span><span class=\"pl-kos\">:</span>\r\n            state<span class=\"pl-c1\"><span class=\"pl-c1\">?</span></span><span class=\"pl-kos\">.</span><span class=\"pl-en\">goToPrevious</span><span class=\"pl-kos\">(</span><span class=\"pl-kos\">)</span>\r\n            <span class=\"pl-k\">return</span> <span class=\"pl-en\">jsonResponse</span><span class=\"pl-kos\">(</span><span class=\"pl-s\">\"</span><span class=\"pl-s\">ok</span><span class=\"pl-s\">\"</span><span class=\"pl-kos\">)</span>\r\n<span class=\"pl-kos\"></span><span class=\"pl-kos\"></span></pre></div>\r\n<p>Using GET requests for state changes like that opens up some fun CSRF vulnerabilities. For this particular application I don't really care.</p>\r\n<h4 id=\"expanding-our-horizons\">Expanding our horizons</h4>\r\n<p>Vibe coding stories like this are ten a penny these days. I think this one is worth sharing for a few reasons:</p>\r\n<ul>\r\n<li>Swift, a language I don't know, was absolutely the right choice here. I wanted a full screen app that embedded web content and could be controlled over the network. Swift had everything I needed.</li>\r\n<li>When I finally did look at the code it was simple, straightforward and did exactly what I needed and not an inch more.</li>\r\n<li>This solved a real problem for me. I've always wanted a good way to serve a presentation as a sequence of pages, and now I have exactly that.</li>\r\n<li>I didn't have to open Xcode even once!</li>\r\n</ul>\r\n<p>This doesn't mean native Mac developers are obsolete. I still used a whole bunch of my own accumulated technical knowledge (and the fact that I'd already installed Xcode and the like) to get this result, and someone who knew what they were doing could have built a far better solution in the same amount of time.</p>\r\n<p>It's a neat illustration of how those of us with software engineering experience can expand our horizons in fun and interesting directions. I'm no longer afraid of Swift! Next time I need a small, personal macOS app I know that it's achievable with our existing set of tools.</p>"
        },
        {
          "type": "blogmark",
          "id": 9313,
          "created": "2026-02-25T17:33:24+00:00",
          "title": "Claude Code Remote Control - @claudeai",
          "body": "New Claude Code feature dropped yesterday: you can now run a \"remote control\" session on your computer and then use the Claude Code for web interfaces (on web, iOS and native desktop app) to send prompts to that session.\r\n\r\nIt's a little bit janky right now. Initially when I tried it I got the error \"Remote Control is not enabled for your account. Contact your administrator.\" (but I *am* my administrator?) - then I logged out and back into the Claude Code terminal app and it started working:\r\n\r\n    claude remote-control\r\n\r\nYou can only run one session on your machine at a time. If you upgrade the Claude iOS app it then shows up as \"Remote Control Session (Mac)\" in the Code tab.\r\n\r\nIt appears not to support the `--dangerously-skip-permissions` flag (I passed that to `claude remote-control` and it didn't reject the option, but it also appeared to have no effect) - which means you have to approve every new action it takes.\r\n\r\nI also managed to get it to a state where every prompt I tried was met by an API 500 error.\r\n\r\n<p style=\"text-align: center;\"><img src=\"https://static.simonwillison.net/static/2026/vampire-remote.jpg\" alt=\"Screenshot of a &quot;Remote Control session&quot; (Mac:dev:817b) chat interface. User message: &quot;Play vampire by Olivia Rodrigo in music app&quot;. Response shows an API Error: 500 {&quot;type&quot;:&quot;error&quot;,&quot;error&quot;:{&quot;type&quot;:&quot;api_error&quot;,&quot;message&quot;:&quot;Internal server error&quot;},&quot;request_id&quot;:&quot;req_011CYVBLH9yt2ze2qehrX8nk&quot;} with a &quot;Try again&quot; button. Below, the assistant responds: &quot;I&#39;ll play &quot;Vampire&quot; by Olivia Rodrigo in the Music app using AppleScript.&quot; A Bash command panel is open showing an osascript command: osascript -e &#39;tell application &quot;Music&quot; activate set searchResults to search playlist &quot;Library&quot; for &quot;vampire Olivia Rodrigo&quot; if (count of searchResults) &gt; 0 then play item 1 of searchResults else return &quot;Song not found in library&quot; end if end tell&#39;\" style=\"max-width: 80%;\" /></p>\r\n\r\nRestarting the program on the machine also causes existing sessions to start returning mysterious API errors rather than neatly explaining that the session has terminated.\r\n\r\nI expect they'll iron out all of these issues relatively quickly. It's interesting to then contrast this to solutions like OpenClaw, where one of the big selling points is the ability to control your personal device from your phone.\r\n\r\nClaude Code still doesn't have a documented mechanism for running things on a schedule, which is the other killer feature of the Claw category of software.\r\n\r\n**Update**: I spoke too soon: also today Anthropic announced [Schedule recurring tasks in Cowork](https://support.claude.com/en/articles/13854387-schedule-recurring-tasks-in-cowork), Claude Code's [general agent sibling](https://simonwillison.net/2026/Jan/12/claude-cowork/). These do include an important limitation:\r\n\r\n> Scheduled tasks only run while your computer is awake and the Claude Desktop app is open. If your computer is asleep or the app is closed when a task is scheduled to run, Cowork will skip the task, then run it automatically once your computer wakes up or you open the desktop app again.\r\n\r\nI really hope they're working on a Cowork Cloud product."
        },
        {
          "type": "blogmark",
          "id": 9315,
          "created": "2026-02-25T21:06:53+00:00",
          "title": "tldraw issue: Move tests to closed source repo - @steveruizok",
          "body": "It's become very apparent over the past few months that a comprehensive test suite is enough to build a completely fresh implementation of any open source library from scratch, potentially in a different language.\r\n\r\nThis has worrying implications for open source projects with commercial business models. Here's an example of a response: tldraw, the outstanding collaborative drawing library (see [previous coverage](https://simonwillison.net/2023/Nov/16/tldrawdraw-a-ui/)), are moving their test suite to a private repository - apparently in response to [Cloudflare's project to port Next.js to use Vite in a week using AI](https://blog.cloudflare.com/vinext/).\r\n\r\nThey also filed a joke issue, now closed to [Translate source code to Traditional Chinese](https://github.com/tldraw/tldraw/issues/8092):\r\n\r\n> The current tldraw codebase is in English, making it easy for external AI coding agents to replicate. It is imperative that we defend our intellectual property.\r\n\r\nWorth noting that tldraw aren't technically open source - their [custom license](https://github.com/tldraw/tldraw?tab=License-1-ov-file#readme) requires a commercial license if you want to use it in \"production environments\".\r\n\r\n**Update**: Well this is embarrassing, it turns out the issue I linked to about removing the tests was [a joke as well](https://github.com/tldraw/tldraw/issues/8082#issuecomment-3964650501):\r\n\r\n> Sorry folks, this issue was more of a joke (am I allowed to do that?) but I'll keep the issue open since there's some discussion here. Writing from mobile\r\n> \r\n> - moving our tests into another repo would complicate and slow down our development, and speed for us is more important than ever\r\n> - more canvas better, I know for sure that our decisions have inspired other products and that's fine and good\r\n> - tldraw itself may eventually be a vibe coded alternative to tldraw\r\n> - the value is in the ability to produce new and good product decisions for users / customers, however you choose to create the code"
        },
        {
          "type": "quotation",
          "id": 2036,
          "created": "2026-02-26T03:44:56+00:00",
          "title": "Benedict Evans",
          "body": "If people are only using this a couple of times a week at most, and can\u2019t think of anything to do with it on the average day, it hasn\u2019t changed their life. OpenAI itself admits the problem, talking about a \u2018capability gap\u2019 between what the models can do and what people do with them, which seems to me like a way to avoid saying that you don\u2019t have clear product-market fit. \r\n\r\nHence, OpenAI\u2019s ad project is partly just about covering the cost of serving the 90% or more of users who don\u2019t pay (and capturing an early lead with advertisers and early learning in how this might work), but more strategically, it\u2019s also about making it possible to give those users the latest and most powerful (i.e. expensive) models, in the hope that this will deepen their engagement. - Benedict Evans"
        },
        {
          "type": "blogmark",
          "id": 9316,
          "created": "2026-02-26T04:28:55+00:00",
          "title": "Google API Keys Weren't Secrets. But then Gemini Changed the Rules. - Hacker News",
          "body": "Yikes! It turns out Gemini and Google Maps (and other services) share the same API keys... but Google Maps API keys are designed to be public, since they are embedded directly in web pages. Gemini API keys can be used to access private files and make billable API requests, so they absolutely should not be shared.\r\n\r\nIf you don't understand this it's very easy to accidentally enable Gemini billing on a previously public API key that exists in the wild already.\r\n\r\n> What makes this a privilege escalation rather than a misconfiguration is the sequence of events.\u00a0\r\n> \r\n> 1.  A developer creates an API key and embeds it in a website for Maps. (At that point, the key is harmless.)\u00a0\r\n> 2.  The Gemini API gets enabled on the same project. (Now that same key can access sensitive Gemini endpoints.)\u00a0\r\n> 3.  The developer is never warned that the keys' privileges changed underneath it. (The key went from public identifier to secret credential).\r\n\r\nTruffle Security found 2,863 API keys in the November 2025 Common Crawl that could access Gemini, verified by hitting the `/models` listing endpoint. This included several keys belonging to Google themselves, one of which had been deployed since February 2023 (according to the Internet Archive) hence predating the Gemini API that it could now access.\r\n\r\nGoogle are working to revoke affected keys but it's still a good idea to check that none of yours are affected by this."
        },
        {
          "type": "quotation",
          "id": 2037,
          "created": "2026-02-26T19:03:27+00:00",
          "title": "Andrej Karpathy",
          "body": "It is hard to communicate how much programming has changed due to AI in the last 2 months: not gradually and over time in the \"progress as usual\" way, but specifically this last December. There are a number of asterisks but imo coding agents basically didn\u2019t work before December and basically work since - the models have significantly higher quality, long-term coherence and tenacity and they can power through large and long tasks, well past enough that it is extremely disruptive to the default programming workflow. [...] - Andrej Karpathy"
        },
        {
          "type": "chapter",
          "id": 7,
          "created": "2026-02-26T20:33:27+00:00",
          "title": "Hoard things you know how to do",
          "body": "Many of my tips for working productively with coding agents are extensions of advice I've found useful in my career without them. Here's a great example of that: **hoard things you know how to do**.\r\n\r\nA big part of the skill in building software is understanding what's possible and what isn't, and having at least a rough idea of how those things can be accomplished.\r\n\r\nThese questions can be broad or quite obscure. Can a web page run OCR operations in JavaScript alone? Can an iPhone app pair with a Bluetooth device even when the app isn't running? Can we process a 100GB JSON file in Python without loading the entire thing into memory first?\r\n\r\nThe more answers to questions like this you have under your belt, the more likely you'll be able to spot opportunities to deploy technology to solve problems in ways other people may not have thought of yet.\r\n\r\nKnowing that something is theoretically possible is not the same as having seen it done for yourself. A key asset to develop as a software professional is a deep collection of answers to questions like this, ideally illustrated by running code.\r\n\r\nI hoard solutions like this in a number of different ways. My [blog](https://simonwillison.net) and [TIL blog](https://til.simonwillison.net) are crammed with notes on things I've figured out how to do. I have [over a thousand GitHub repos](https://github.com/simonw) collecting code I've written for different projects, many of them small proof-of-concepts that demonstrate a key idea.\r\n\r\nMore recently I've used LLMs to help expand my collection of code solutions to interesting problems.\r\n\r\n[tools.simonwillison.net](https://tools.simonwillison.net) is my largest collection of LLM-assisted tools and prototypes. I use this to collect what I call [HTML tools](https://simonwillison.net/2025/Dec/10/html-tools/) - single HTML pages that embed JavaScript and CSS and solve a specific problem.\r\n\r\nMy [simonw/research](https://github.com/simonw/research) repository has larger, more complex examples where I\u2019ve challenged a coding agent to research a problem and come back with working code and a written report detailing what it found out.\r\n\r\n## Recombining things from your hoard\r\n\r\nWhy collect all of this stuff? Aside from helping you build and extend your own abilities, the assets you generate along the way become incredibly powerful inputs for your coding agents.\r\n\r\nOne of my favorite prompting patterns is to tell an agent to build something new by combining two or more existing working examples.\r\n\r\nA project that helped crystallize how effective this can be was the first thing I added to my tools collection - a browser-based [OCR tool](https://tools.simonwillison.net/ocr), described [in more detail here](https://simonwillison.net/2024/Mar/30/ocr-pdfs-images/).\r\n\r\nI wanted an easy, browser-based tool for OCRing pages from PDF files - in particular PDFs that consist entirely of scanned images with no text version provided at all.\r\n\r\nI had previously experimented with running the [Tesseract.js OCR library](https://tesseract.projectnaptha.com/) in my browser, and found it to be very capable. That library provides a WebAssembly build of the mature Tesseract OCR engine and lets you call it from JavaScript to extract text from an image.\r\n\r\nI didn\u2019t want to work with images though, I wanted to work with PDFs. Then I remembered that I had also worked with Mozilla\u2019s [PDF.js](https://mozilla.github.io/pdf.js/) library, which among other things can turn individual pages of a PDF into rendered images.\r\n\r\nI had snippets of JavaScript for both of those libraries in my notes.\r\n\r\nHere\u2019s the full prompt I fed into a model (at the time it was Claude 3 Opus), combining my two examples and describing the solution I was looking for:\r\n\r\n````markdown-copy\r\nThis code shows how to open a PDF and turn it into an image per page:\r\n```html\r\n<!DOCTYPE html>\r\n<html>\r\n<head>\r\n  <title>PDF to Images</title>\r\n  <script src=\"https://cdnjs.cloudflare.com/ajax/libs/pdf.js/2.9.359/pdf.min.js\"></script>\r\n  <style>\r\n    .image-container img {\r\n      margin-bottom: 10px;\r\n    }\r\n    .image-container p {\r\n      margin: 0;\r\n      font-size: 14px;\r\n      color: #888;\r\n    }\r\n  </style>\r\n</head>\r\n<body>\r\n  <input type=\"file\" id=\"fileInput\" accept=\".pdf\" />\r\n  <div class=\"image-container\"></div>\r\n\r\n  <script>\r\n  const desiredWidth = 800;\r\n    const fileInput = document.getElementById('fileInput');\r\n    const imageContainer = document.querySelector('.image-container');\r\n\r\n    fileInput.addEventListener('change', handleFileUpload);\r\n\r\n    pdfjsLib.GlobalWorkerOptions.workerSrc = 'https://cdnjs.cloudflare.com/ajax/libs/pdf.js/2.9.359/pdf.worker.min.js';\r\n\r\n    async function handleFileUpload(event) {\r\n      const file = event.target.files[0];\r\n      const imageIterator = convertPDFToImages(file);\r\n\r\n      for await (const { imageURL, size } of imageIterator) {\r\n        const imgElement = document.createElement('img');\r\n        imgElement.src = imageURL;\r\n        imageContainer.appendChild(imgElement);\r\n\r\n        const sizeElement = document.createElement('p');\r\n        sizeElement.textContent = `Size: ${formatSize(size)}`;\r\n        imageContainer.appendChild(sizeElement);\r\n      }\r\n    }\r\n\r\n    async function* convertPDFToImages(file) {\r\n      try {\r\n        const pdf = await pdfjsLib.getDocument(URL.createObjectURL(file)).promise;\r\n        const numPages = pdf.numPages;\r\n\r\n        for (let i = 1; i <= numPages; i++) {\r\n          const page = await pdf.getPage(i);\r\n          const viewport = page.getViewport({ scale: 1 });\r\n          const canvas = document.createElement('canvas');\r\n          const context = canvas.getContext('2d');\r\n          canvas.width = desiredWidth;\r\n          canvas.height = (desiredWidth / viewport.width) * viewport.height;\r\n          const renderContext = {\r\n            canvasContext: context,\r\n            viewport: page.getViewport({ scale: desiredWidth / viewport.width }),\r\n          };\r\n          await page.render(renderContext).promise;\r\n          const imageURL = canvas.toDataURL('image/jpeg', 0.8);\r\n          const size = calculateSize(imageURL);\r\n          yield { imageURL, size };\r\n        }\r\n      } catch (error) {\r\n        console.error('Error:', error);\r\n      }\r\n    }\r\n\r\n    function calculateSize(imageURL) {\r\n      const base64Length = imageURL.length - 'data:image/jpeg;base64,'.length;\r\n      const sizeInBytes = Math.ceil(base64Length * 0.75);\r\n      return sizeInBytes;\r\n    }\r\n\r\n    function formatSize(size) {\r\n      const sizeInKB = (size / 1024).toFixed(2);\r\n      return `${sizeInKB} KB`;\r\n    }\r\n  </script>\r\n</body>\r\n</html>\r\n```\r\nThis code shows how to OCR an image:\r\n```javascript\r\nasync function ocrMissingAltText() {\r\n    // Load Tesseract\r\n    var s = document.createElement(\"script\");\r\n    s.src = \"https://unpkg.com/tesseract.js@v2.1.0/dist/tesseract.min.js\";\r\n    document.head.appendChild(s);\r\n\r\n    s.onload = async () => {\r\n      const images = document.getElementsByTagName(\"img\");\r\n      const worker = Tesseract.createWorker();\r\n      await worker.load();\r\n      await worker.loadLanguage(\"eng\");\r\n      await worker.initialize(\"eng\");\r\n      ocrButton.innerText = \"Running OCR...\";\r\n\r\n      // Iterate through all the images in the output div\r\n      for (const img of images) {\r\n        const altTextarea = img.parentNode.querySelector(\".textarea-alt\");\r\n        // Check if the alt textarea is empty\r\n        if (altTextarea.value === \"\") {\r\n          const imageUrl = img.src;\r\n          var {\r\n            data: { text },\r\n          } = await worker.recognize(imageUrl);\r\n          altTextarea.value = text; // Set the OCR result to the alt textarea\r\n          progressBar.value += 1;\r\n        }\r\n      }\r\n\r\n      await worker.terminate();\r\n      ocrButton.innerText = \"OCR complete\";\r\n    };\r\n  }\r\n```\r\nUse these examples to put together a single HTML page with embedded HTML and CSS and JavaScript that provides a big square which users can drag and drop a PDF file onto and when they do that the PDF has every page converted to a JPEG and shown below on the page, then OCR is run with tesseract and the results are shown in textarea blocks below each image.\r\n````\r\n\r\nThis worked flawlessly! The model kicked out a proof-of-concept page that did exactly what I needed.\r\n\r\nI ended up [iterating with it a few times](https://gist.github.com/simonw/6a9f077bf8db616e44893a24ae1d36eb) to get to my final result, but it took just a few minutes to build a genuinely useful tool that I\u2019ve benefited from ever since.\r\n\r\n## Coding agents make this even more powerful\r\n\r\nI built that OCR example back in March 2024, nearly a year before the first release of Claude Code. Coding agents have made hoarding working examples even more valuable.\r\n\r\nIf your coding agent has internet access you can tell it to do things like:\r\n\r\n````markdown-copy\r\nUse curl to fetch the source of `https://tools.simonwillison.net/ocr` and `https://tools.simonwillison.net/gemini-bbox` and build a new tool that lets you select a page from a PDF and pass it to Gemini to return bounding boxes for illustrations on that page.\r\n````\r\n(I specified `curl` there because Claude Code defaults to using a WebFetch tool which summarizes the page content rather than returning the raw HTML.)\r\n\r\nCoding agents are excellent at search, which means you can run them on your own machine and tell them where to find the examples of things you want them to do:\r\n````markdown-copy\r\nAdd mocked HTTP tests to the `~/dev/ecosystem/datasette-oauth` project inspired by how `~/dev/ecosystem/llm-mistral` is doing it.\r\n````\r\nOften that's enough - the agent will fire up a search sub-agent to investigate and pull back just the details it needs to achieve the task.\r\n\r\nSince so much of my research code is public I'll often tell coding agents to clone my repositories to `/tmp` and use them as input:\r\n````markdown-copy\r\nClone `simonw/research` from GitHub to `/tmp` and find examples of compiling Rust to WebAssembly, then use that to build a demo HTML page for this project.\r\n````\r\nThe key idea here is that coding agents mean we only ever need to figure out a useful trick *once*. If that trick is then documented somewhere with a working code example our agents can consult that example and use it to solve any similar shaped project in the future."
        },
        {
          "type": "blogmark",
          "id": 9317,
          "created": "2026-02-27T17:50:54+00:00",
          "title": "Unicode Explorer using binary search over fetch() HTTP range requests - ",
          "body": "Here's a little prototype I built this morning from my phone as an experiment in HTTP range requests, and a general example of using LLMs to satisfy curiosity.\r\n\r\nI've been collecting [HTTP range tricks](https://simonwillison.net/tags/http-range-requests/) for a while now, and I decided it would be fun to build something with them myself that used binary search against a large file to do something useful.\r\n\r\nSo I [brainstormed with Claude](https://claude.ai/share/47860666-cb20-44b5-8cdb-d0ebe363384f). The challenge was coming up with a use case for binary search where the data could be naturally sorted in a way that would benefit from binary search.\r\n\r\nOne of Claude's suggestions was looking up information about unicode codepoints, which means searching through many MBs of metadata.\r\n\r\nI had Claude write me a spec to feed to Claude Code - [visible here](https://github.com/simonw/research/pull/90#issue-4001466642) - then kicked off an [asynchronous research project](https://simonwillison.net/2025/Nov/6/async-code-research/) with Claude Code for web against my [simonw/research](https://github.com/simonw/research) repo to turn that into working code.\r\n\r\nHere's the [resulting report and code](https://github.com/simonw/research/tree/main/unicode-explorer-binary-search#readme). One interesting thing I learned is that Range request tricks aren't compatible with HTTP compression because they mess with the byte offset calculations. I added `'Accept-Encoding': 'identity'` to the `fetch()` calls but this isn't actually necessary because Cloudflare and other CDNs automatically skip compression if a `content-range` header is present.\r\n\r\nI deployed the result [to my tools.simonwillison.net site](https://tools.simonwillison.net/unicode-binary-search), after first tweaking it to query the data via range requests against a CORS-enabled 76.6MB file in an S3 bucket fronted by Cloudflare.\r\n\r\nThe demo is fun to play with - type in a single character like `\u00f8` or a hexadecimal codepoint indicator like `1F99C` and it will binary search its way through the large file and show you the steps it takes along the way:\r\n\r\n![Animated demo of a web tool called Unicode Explore. I enter the ampersand character and hit Search. A box below shows a sequence of HTTP binary search requests made, finding in 17 steps with 3,864 bytes transferred and telling me that ampersand is U+0026 in Punctuation other, Basic Latin](https://static.simonwillison.net/static/2026/unicode-explore.gif)"
        },
        {
          "type": "blogmark",
          "id": 9318,
          "created": "2026-02-27T18:08:22+00:00",
          "title": "Free Claude Max for (large project) open source maintainers - Hacker News",
          "body": "Anthropic are now offering their $200/month Claude Max 20x plan for free to open source maintainers... for six months... and you have to meet the following criteria:\r\n\r\n> - **Maintainers:** You're a primary maintainer or core team member of a public repo with 5,000+ GitHub stars *or* 1M+ monthly NPM downloads. You've made commits, releases, or PR reviews within the last 3 months.\r\n> - **Don't quite fit the criteria** If you maintain something the ecosystem quietly depends on, apply anyway and tell us about it.\r\n\r\nAlso in the small print: \"Applications are reviewed on a rolling basis. We accept up to 10,000 contributors\"."
        },
        {
          "type": "blogmark",
          "id": 9319,
          "created": "2026-02-27T20:43:41+00:00",
          "title": "An AI agent coding skeptic tries AI agent coding, in excessive detail - ",
          "body": "Another in the genre of \"OK, coding agents got good in November\" posts, this one is by Max Woolf and is very much worth your time. He describes a sequence of coding agent projects, each more ambitious than the last - starting with simple YouTube metadata scrapers and eventually evolving to this:\r\n\r\n> It would be arrogant to port Python's [scikit-learn](https://scikit-learn.org/stable/) \u2014 the gold standard of data science and machine learning libraries \u2014 to Rust with all the features that implies.\r\n> \r\n> But that's unironically a good idea so I decided to try and do it anyways. With the use of agents, I am now developing `rustlearn` (extreme placeholder name), a Rust crate that implements not only the fast implementations of the standard machine learning algorithms such as [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) and [k-means clustering](https://en.wikipedia.org/wiki/K-means_clustering), but also includes the fast implementations of the algorithms above: the same three step pipeline I describe above still works even with the more simple algorithms to beat scikit-learn's implementations.\r\n\r\nMax also captures the frustration of trying to explain how good the models have got to an existing skeptical audience:\r\n\r\n> The real annoying thing about Opus 4.6/Codex 5.3 is that it\u2019s impossible to publicly say \u201cOpus 4.5 (and the models that came after it) are an order of magnitude better than coding LLMs released just months before it\u201d without sounding like an AI hype booster clickbaiting, but it\u2019s the counterintuitive truth to my personal frustration. I have been trying to break this damn model by giving it complex tasks that would take me months to do by myself despite my coding pedigree but Opus and Codex keep doing them correctly.\r\n\r\nA throwaway remark in this post inspired me to [ask Claude Code to build a Rust word cloud CLI tool](https://github.com/simonw/research/tree/main/rust-wordcloud#readme), which it happily did."
        },
        {
          "type": "blogmark",
          "id": 9320,
          "created": "2026-02-27T22:49:32+00:00",
          "title": "Please, please, please stop using passkeys for encrypting user data - lobste.rs",
          "body": "Because users lose their passkeys *all the time*, and may not understand that their data has been irreversibly encrypted using them and can no longer be recovered.\r\n\r\nTim Cappalli:\r\n\r\n> To the wider identity industry: *please stop promoting and using passkeys to encrypt user data. I\u2019m begging you. Let them be great, phishing-resistant authentication credentials*."
        },
        {
          "type": "chapter",
          "id": 8,
          "created": "2026-02-28T20:53:15+00:00",
          "title": "Prompts I use",
          "body": "This section of the guide will be continually updated with prompts that I use myself, linked to from other chapters where appropriate.\r\n\r\n## Artifacts\r\n\r\nI frequently use Claude's Artifacts feature for prototyping and to build small HTML tools. Artifacts are when regular Claude chat builds an application in HTML and JavaScript and displays it directly within the Claude chat interface. OpenAI and Gemini offer a finial feature which they both call Canvas.\r\n\r\nModels love using React for these. I don't like how React requires an additional build step which prevents me from copying and pasting code out of an artifact and into static hosting elsewhere, so I create my artifacts in Claude using a project with the following custom instructions:\r\n\r\n````markdown-copy\r\nNever use React in artifacts - always plain HTML and vanilla JavaScript and CSS with minimal dependencies.\r\n\r\nCSS should be indented with two spaces and should start like this:\r\n\r\n```\r\n<style>\r\n* {\r\n  box-sizing: border-box;\r\n}\r\n```\r\nInputs and textareas should be font size 16px. Font should always prefer Helvetica.\r\n\r\nJavaScript should be two space indents and start like this:\r\n```\r\n<script type=\"module\">\r\n// code in here should not be indented at the first level\r\n```\r\nPrefer Sentence case for headings.\r\n````"
        },
        {
          "type": "chapter",
          "id": 9,
          "created": "2026-02-28T23:09:39+00:00",
          "title": "Interactive explanations",
          "body": "When we lose track of how code written by our agents works we take on **cognitive debt**.\r\n\r\nFor a lot of things this doesn't matter: if the code fetches some data from a database and outputs it as JSON the implementation details are likely simple enough that we don't need to care. We can try out the new feature and make a very solid guess at how it works, then glance over the code to be sure.\r\n\r\nOften though the details really do matter. If the core of our application becomes a black box that we don't fully understand we can no longer confidently reason about it, which makes planning new features harder and eventually slows our progress in the same way that accumulated technical debt does.\r\n\r\nHow do we pay down cognitive debt? By improving our understanding of how the code works.\r\n\r\nOne of my favorite ways to do that is by building **interactive explanations**.\r\n\r\n## Understanding word clouds\r\n\r\nIn [An AI agent coding skeptic tries AI agent coding, in excessive detail](https://minimaxir.com/2026/02/ai-agent-coding/) Max Woolf mentioned testing LLMs' Rust abilities with the prompt `Create a Rust app that can create \"word cloud\" data visualizations given a long input text`.\r\n\r\nThis captured my imagination: I've always wanted to know how word clouds work, so I fired off an [asynchronous research project](https://simonwillison.net/2025/Nov/6/async-code-research/) - [initial prompt here](https://github.com/simonw/research/pull/91#issue-4002426963), [code and report here](https://github.com/simonw/research/tree/main/rust-wordcloud) - to explore the idea.\r\n\r\nThis worked really well: Claude Code for web built me a Rust CLI tool that could produce images like\r\nthis one:\r\n\r\n![A word cloud, many words, different colors and sizes, larger words in the middle.](https://raw.githubusercontent.com/simonw/research/refs/heads/main/rust-wordcloud/wordcloud.png)\r\n\r\nBut how does it actually work?\r\n\r\nClaude's report said it uses \"**Archimedean spiral placement** with per-word random angular offset for natural-looking layouts\". This did not help me much!\r\n\r\nI requested a [linear walkthrough](https://simonwillison.net/guides/agentic-engineering-patterns/linear-walkthroughs/) of the codebase which helped me understand the Rust code in more detail - here's [that walkthrough](https://github.com/simonw/research/blob/main/rust-wordcloud/walkthrough.md) (and [the prompt](https://github.com/simonw/research/commit/2cb8c62477173ef6a4c2e274be9f712734df6126)). This helped me understand the structure of the Rust code but I still didn't have an intuitive understanding of how that \"Archimedean spiral placement\" part actually worked.\r\n\r\nSo I asked for an **animated explanation**. I did this by pasting a link to that existing `walkthrough.md` document into a Claude Code session along with the following:\r\n\r\n````markdown-copy\r\nFetch https://raw.githubusercontent.com/simonw/research/refs/heads/main/rust-wordcloud/walkthrough.md to /tmp using curl so you can read the whole thing\r\n\r\nInspired by that, build animated-word-cloud.html - a page that accepts pasted text (which it persists in the `#fragment` of the URL such that a page loaded with that `#` populated will use that text as input and auto-submit it) such that when you submit the text it builds a word cloud using the algorithm described in that document but does it animated, to make the algorithm as clear to understand. Include a slider for the animation which can be paused and the speed adjusted or even stepped through frame by frame while paused. At any stage the visible in-progress word cloud can be downloaded as a PNG.\r\n````\r\nYou can [play with the result here](https://tools.simonwillison.net/animated-word-cloud). Here's an animated GIF demo:\r\n\r\n![Words appear on the word cloud one at a time, with little boxes showing where the algorithm is attempting to place them - if those boxes overlap an existing word it tries again.](https://static.simonwillison.net/static/2026/animated-word-cloud-demo.gif)\r\n\r\nThis was using Claude Opus 4.6, which turns out to have quite good taste when it comes to building explanatory animations.\r\n\r\nIf you watch the animation closely you can see that for each word it attempts to place it somewhere on the page by showing a box, run checks if that box intersects an existing word. If so it continues to try to find a good spot, moving outward in a spiral from the center.\r\n\r\nI found that this animation really helped make the way the algorithm worked click for me.\r\n\r\nI have long been a fan of animations and interactive interfaces to help explain different concepts. A good coding agent can produce these on demand to help explain code - its own code or code written by others."
        }
      ]
    }
  ]
}